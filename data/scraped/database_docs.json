[
  {
    "title": "Part VI. Reference",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nPart VI. Reference\nPrev\nUp\nPostgreSQL 17.5 Documentation\nHome\nNext\nPart VI. Reference\nThe entries in this Reference are meant to provide in reasonable length an authoritative, complete, and formal summary about their respective subjects. More information about the use of\nPostgreSQL\n, in narrative, tutorial, or example form, can be found in other parts of this book. See the cross-references listed on each reference page.\nThe reference entries are also available as traditional\n“\nman\n”\npages.\nTable of Contents\nI. SQL Commands\nABORT\n— abort the current transaction\nALTER AGGREGATE\n— change the definition of an aggregate function\nALTER COLLATION\n— change the definition of a collation\nALTER CONVERSION\n— change the definition of a conversion\nALTER DATABASE\n— change a database\nALTER DEFAULT PRIVILEGES\n— define default access privileges\nALTER DOMAIN\n— change the definition of a domain\nALTER EVENT TRIGGER\n— change the definition of an event trigger\nALTER EXTENSION\n— change the definition of an extension\nALTER FOREIGN DATA WRAPPER\n— change the definition of a foreign-data wrapper\nALTER FOREIGN TABLE\n— change the definition of a foreign table\nALTER FUNCTION\n— change the definition of a function\nALTER GROUP\n— change role name or membership\nALTER INDEX\n— change the definition of an index\nALTER LANGUAGE\n— change the definition of a procedural language\nALTER LARGE OBJECT\n— change the definition of a large object\nALTER MATERIALIZED VIEW\n— change the definition of a materialized view\nALTER OPERATOR\n— change the definition of an operator\nALTER OPERATOR CLASS\n— change the definition of an operator class\nALTER OPERATOR FAMILY\n— change the definition of an operator family\nALTER POLICY\n— change the definition of a row-level security policy\nALTER PROCEDURE\n— change the definition of a procedure\nALTER PUBLICATION\n— change the definition of a publication\nALTER ROLE\n— change a database role\nALTER ROUTINE\n— change the definition of a routine\nALTER RULE\n— change the definition of a rule\nALTER SCHEMA\n— change the definition of a schema\nALTER SEQUENCE\n— change the definition of a sequence generator\nALTER SERVER\n— change the definition of a foreign server\nALTER STATISTICS\n— change the definition of an extended statistics object\nALTER SUBSCRIPTION\n— change the definition of a subscription\nALTER SYSTEM\n— change a server configuration parameter\nALTER TABLE\n— change the definition of a table\nALTER TABLESPACE\n— change the definition of a tablespace\nALTER TEXT SEARCH CONFIGURATION\n— change the definition of a text search configuration\nALTER TEXT SEARCH DICTIONARY\n— change the definition of a text search dictionary\nALTER TEXT SEARCH PARSER\n— change the definition of a text search parser\nALTER TEXT SEARCH TEMPLATE\n— change the definition of a text search template\nALTER TRIGGER\n— change the definition of a trigger\nALTER TYPE\n— change the definition of a type\nALTER USER\n— change a database role\nALTER USER MAPPING\n— change the definition of a user mapping\nALTER VIEW\n— change the definition of a view\nANALYZE\n— collect statistics about a database\nBEGIN\n— start a transaction block\nCALL\n— invoke a procedure\nCHECKPOINT\n— force a write-ahead log checkpoint\nCLOSE\n— close a cursor\nCLUSTER\n— cluster a table according to an index\nCOMMENT\n— define or change the comment of an object\nCOMMIT\n— commit the current transaction\nCOMMIT PREPARED\n— commit a transaction that was earlier prepared for two-phase commit\nCOPY\n— copy data between a file and a table\nCREATE ACCESS METHOD\n— define a new access method\nCREATE AGGREGATE\n— define a new aggregate function\nCREATE CAST\n— define a new cast\nCREATE COLLATION\n— define a new collation\nCREATE CONVERSION\n— define a new encoding conversion\nCREATE DATABASE\n— create a new database\nCREATE DOMAIN\n— define a new domain\nCREATE EVENT TRIGGER\n— define a new event trigger\nCREATE EXTENSION\n— install an extension\nCREATE FOREIGN DATA WRAPPER\n— define a new foreign-data wrapper\nCREATE FOREIGN TABLE\n— define a new foreign table\nCREATE FUNCTION\n— define a new function\nCREATE GROUP\n— define a new database role\nCREATE INDEX\n— define a new index\nCREATE LANGUAGE\n— define a new procedural language\nCREATE MATERIALIZED VIEW\n— define a new materialized view\nCREATE OPERATOR\n— define a new operator\nCREATE OPERATOR CLASS\n— define a new operator class\nCREATE OPERATOR FAMILY\n— define a new operator family\nCREATE POLICY\n— define a new row-level security policy for a table\nCREATE PROCEDURE\n— define a new procedure\nCREATE PUBLICATION\n— define a new publication\nCREATE ROLE\n— define a new database role\nCREATE RULE\n— define a new rewrite rule\nCREATE SCHEMA\n— define a new schema\nCREATE SEQUENCE\n— define a new sequence generator\nCREATE SERVER\n— define a new foreign server\nCREATE STATISTICS\n— define extended statistics\nCREATE SUBSCRIPTION\n— define a new subscription\nCREATE TABLE\n— define a new table\nCREATE TABLE AS\n— define a new table from the results of a query\nCREATE TABLESPACE\n— define a new tablespace\nCREATE TEXT SEARCH CONFIGURATION\n— define a new text search configuration\nCREATE TEXT SEARCH DICTIONARY\n— define a new text search dictionary\nCREATE TEXT SEARCH PARSER\n— define a new text search parser\nCREATE TEXT SEARCH TEMPLATE\n— define a new text search template\nCREATE TRANSFORM\n— define a new transform\nCREATE TRIGGER\n— define a new trigger\nCREATE TYPE\n— define a new data type\nCREATE USER\n— define a new database role\nCREATE USER MAPPING\n— define a new mapping of a user to a foreign server\nCREATE VIEW\n— define a new view\nDEALLOCATE\n— deallocate a prepared statement\nDECLARE\n— define a cursor\nDELETE\n— delete rows of a table\nDISCARD\n— discard session state\nDO\n— execute an anonymous code block\nDROP ACCESS METHOD\n— remove an access method\nDROP AGGREGATE\n— remove an aggregate function\nDROP CAST\n— remove a cast\nDROP COLLATION\n— remove a collation\nDROP CONVERSION\n— remove a conversion\nDROP DATABASE\n— remove a database\nDROP DOMAIN\n— remove a domain\nDROP EVENT TRIGGER\n— remove an event trigger\nDROP EXTENSION\n— remove an extension\nDROP FOREIGN DATA WRAPPER\n— remove a foreign-data wrapper\nDROP FOREIGN TABLE\n— remove a foreign table\nDROP FUNCTION\n— remove a function\nDROP GROUP\n— remove a database role\nDROP INDEX\n— remove an index\nDROP LANGUAGE\n— remove a procedural language\nDROP MATERIALIZED VIEW\n— remove a materialized view\nDROP OPERATOR\n— remove an operator\nDROP OPERATOR CLASS\n— remove an operator class\nDROP OPERATOR FAMILY\n— remove an operator family\nDROP OWNED\n— remove database objects owned by a database role\nDROP POLICY\n— remove a row-level security policy from a table\nDROP PROCEDURE\n— remove a procedure\nDROP PUBLICATION\n— remove a publication\nDROP ROLE\n— remove a database role\nDROP ROUTINE\n— remove a routine\nDROP RULE\n— remove a rewrite rule\nDROP SCHEMA\n— remove a schema\nDROP SEQUENCE\n— remove a sequence\nDROP SERVER\n— remove a foreign server descriptor\nDROP STATISTICS\n— remove extended statistics\nDROP SUBSCRIPTION\n— remove a subscription\nDROP TABLE\n— remove a table\nDROP TABLESPACE\n— remove a tablespace\nDROP TEXT SEARCH CONFIGURATION\n— remove a text search configuration\nDROP TEXT SEARCH DICTIONARY\n— remove a text search dictionary\nDROP TEXT SEARCH PARSER\n— remove a text search parser\nDROP TEXT SEARCH TEMPLATE\n— remove a text search template\nDROP TRANSFORM\n— remove a transform\nDROP TRIGGER\n— remove a trigger\nDROP TYPE\n— remove a data type\nDROP USER\n— remove a database role\nDROP USER MAPPING\n— remove a user mapping for a foreign server\nDROP VIEW\n— remove a view\nEND\n— commit the current transaction\nEXECUTE\n— execute a prepared statement\nEXPLAIN\n— show the execution plan of a statement\nFETCH\n— retrieve rows from a query using a cursor\nGRANT\n— define access privileges\nIMPORT FOREIGN SCHEMA\n— import table definitions from a foreign server\nINSERT\n— create new rows in a table\nLISTEN\n— listen for a notification\nLOAD\n— load a shared library file\nLOCK\n— lock a table\nMERGE\n— conditionally insert, update, or delete rows of a table\nMOVE\n— position a cursor\nNOTIFY\n— generate a notification\nPREPARE\n— prepare a statement for execution\nPREPARE TRANSACTION\n— prepare the current transaction for two-phase commit\nREASSIGN OWNED\n— change the ownership of database objects owned by a database role\nREFRESH MATERIALIZED VIEW\n— replace the contents of a materialized view\nREINDEX\n— rebuild indexes\nRELEASE SAVEPOINT\n— release a previously defined savepoint\nRESET\n— restore the value of a run-time parameter to the default value\nREVOKE\n— remove access privileges\nROLLBACK\n— abort the current transaction\nROLLBACK PREPARED\n— cancel a transaction that was earlier prepared for two-phase commit\nROLLBACK TO SAVEPOINT\n— roll back to a savepoint\nSAVEPOINT\n— define a new savepoint within the current transaction\nSECURITY LABEL\n— define or change a security label applied to an object\nSELECT\n— retrieve rows from a table or view\nSELECT INTO\n— define a new table from the results of a query\nSET\n— change a run-time parameter\nSET CONSTRAINTS\n— set constraint check timing for the current transaction\nSET ROLE\n— set the current user identifier of the current session\nSET SESSION AUTHORIZATION\n— set the session user identifier and the current user identifier of the current session\nSET TRANSACTION\n— set the characteristics of the current transaction\nSHOW\n— show the value of a run-time parameter\nSTART TRANSACTION\n— start a transaction block\nTRUNCATE\n— empty a table or set of tables\nUNLISTEN\n— stop listening for a notification\nUPDATE\n— update rows of a table\nVACUUM\n— garbage-collect and optionally analyze a database\nVALUES\n— compute a set of rows\nII. PostgreSQL Client Applications\nclusterdb\n— cluster a\nPostgreSQL\ndatabase\ncreatedb\n— create a new\nPostgreSQL\ndatabase\ncreateuser\n— define a new\nPostgreSQL\nuser account\ndropdb\n— remove a\nPostgreSQL\ndatabase\ndropuser\n— remove a\nPostgreSQL\nuser account\necpg\n— embedded SQL C preprocessor\npg_amcheck\n— checks for corruption in one or more\nPostgreSQL\ndatabases\npg_basebackup\n— take a base backup of a\nPostgreSQL\ncluster\npgbench\n— run a benchmark test on\nPostgreSQL\npg_combinebackup\n— reconstruct a full backup from an incremental backup and dependent backups\npg_config\n— retrieve information about the installed version of\nPostgreSQL\npg_dump\n— extract a\nPostgreSQL\ndatabase into a script file or other archive file\npg_dumpall\n— extract a\nPostgreSQL\ndatabase cluster into a script file\npg_isready\n— check the connection status of a\nPostgreSQL\nserver\npg_receivewal\n— stream write-ahead logs from a\nPostgreSQL\nserver\npg_recvlogical\n— control\nPostgreSQL\nlogical decoding streams\npg_restore\n— restore a\nPostgreSQL\ndatabase from an archive file created by\npg_dump\npg_verifybackup\n— verify the integrity of a base backup of a\nPostgreSQL\ncluster\npsql\n—\nPostgreSQL\ninteractive terminal\nreindexdb\n— reindex a\nPostgreSQL\ndatabase\nvacuumdb\n— garbage-collect and analyze a\nPostgreSQL\ndatabase\nIII. PostgreSQL Server Applications\ninitdb\n— create a new\nPostgreSQL\ndatabase cluster\npg_archivecleanup\n— clean up\nPostgreSQL\nWAL archive files\npg_checksums\n— enable, disable or check data checksums in a\nPostgreSQL\ndatabase cluster\npg_controldata\n— display control information of a\nPostgreSQL\ndatabase cluster\npg_createsubscriber\n— convert a physical replica into a new logical replica\npg_ctl\n— initialize, start, stop, or control a\nPostgreSQL\nserver\npg_resetwal\n— reset the write-ahead log and other control information of a\nPostgreSQL\ndatabase cluster\npg_rewind\n— synchronize a\nPostgreSQL\ndata directory with another data directory that was forked from it\npg_test_fsync\n— determine fastest\nwal_sync_method\nfor\nPostgreSQL\npg_test_timing\n— measure timing overhead\npg_upgrade\n— upgrade a\nPostgreSQL\nserver instance\npg_waldump\n— display a human-readable rendering of the write-ahead log of a\nPostgreSQL\ndatabase cluster\npg_walsummary\n— print contents of WAL summary files\npostgres\n—\nPostgreSQL\ndatabase server\nPrev\nUp\nNext\n49.2. Archive Module Callbacks\nHome\nSQL Commands\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/reference.html",
    "source": "postgresql",
    "doc_type": "reference",
    "scraped_at": 31788.2673837
  },
  {
    "title": "Part II. The SQL Language",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.1\nPart II. The SQL Language\nPrev\nUp\nPostgreSQL 17.5 Documentation\nHome\nNext\nPart II. The SQL Language\nThis part describes the use of the\nSQL\nlanguage in\nPostgreSQL\n. We start with describing the general syntax of\nSQL\n, then how to create tables, how to populate the database, and how to query it. The middle part lists the available data types and functions for use in\nSQL\ncommands. Lastly, we address several aspects of importance for tuning a database.\nThe information is arranged so that a novice user can follow it from start to end and gain a full understanding of the topics without having to refer forward too many times. The chapters are intended to be self-contained, so that advanced users can read the chapters individually as they choose. The information is presented in narrative form with topical units. Readers looking for a complete description of a particular command are encouraged to review the\nPart VI\n.\nReaders should know how to connect to a\nPostgreSQL\ndatabase and issue\nSQL\ncommands. Readers that are unfamiliar with these issues are encouraged to read\nPart I\nfirst.\nSQL\ncommands are typically entered using the\nPostgreSQL\ninteractive terminal\npsql\n, but other programs that have similar functionality can be used as well.\nTable of Contents\n4. SQL Syntax\n4.1. Lexical Structure\n4.2. Value Expressions\n4.3. Calling Functions\n5. Data Definition\n5.1. Table Basics\n5.2. Default Values\n5.3. Identity Columns\n5.4. Generated Columns\n5.5. Constraints\n5.6. System Columns\n5.7. Modifying Tables\n5.8. Privileges\n5.9. Row Security Policies\n5.10. Schemas\n5.11. Inheritance\n5.12. Table Partitioning\n5.13. Foreign Data\n5.14. Other Database Objects\n5.15. Dependency Tracking\n6. Data Manipulation\n6.1. Inserting Data\n6.2. Updating Data\n6.3. Deleting Data\n6.4. Returning Data from Modified Rows\n7. Queries\n7.1. Overview\n7.2. Table Expressions\n7.3. Select Lists\n7.4. Combining Queries (\nUNION\n,\nINTERSECT\n,\nEXCEPT\n)\n7.5. Sorting Rows (\nORDER BY\n)\n7.6.\nLIMIT\nand\nOFFSET\n7.7.\nVALUES\nLists\n7.8.\nWITH\nQueries (Common Table Expressions)\n8. Data Types\n8.1. Numeric Types\n8.2. Monetary Types\n8.3. Character Types\n8.4. Binary Data Types\n8.5. Date/Time Types\n8.6. Boolean Type\n8.7. Enumerated Types\n8.8. Geometric Types\n8.9. Network Address Types\n8.10. Bit String Types\n8.11. Text Search Types\n8.12.\nUUID\nType\n8.13.\nXML\nType\n8.14.\nJSON\nTypes\n8.15. Arrays\n8.16. Composite Types\n8.17. Range Types\n8.18. Domain Types\n8.19. Object Identifier Types\n8.20.\npg_lsn\nType\n8.21. Pseudo-Types\n9. Functions and Operators\n9.1. Logical Operators\n9.2. Comparison Functions and Operators\n9.3. Mathematical Functions and Operators\n9.4. String Functions and Operators\n9.5. Binary String Functions and Operators\n9.6. Bit String Functions and Operators\n9.7. Pattern Matching\n9.8. Data Type Formatting Functions\n9.9. Date/Time Functions and Operators\n9.10. Enum Support Functions\n9.11. Geometric Functions and Operators\n9.12. Network Address Functions and Operators\n9.13. Text Search Functions and Operators\n9.14. UUID Functions\n9.15. XML Functions\n9.16. JSON Functions and Operators\n9.17. Sequence Manipulation Functions\n9.18. Conditional Expressions\n9.19. Array Functions and Operators\n9.20. Range/Multirange Functions and Operators\n9.21. Aggregate Functions\n9.22. Window Functions\n9.23. Merge Support Functions\n9.24. Subquery Expressions\n9.25. Row and Array Comparisons\n9.26. Set Returning Functions\n9.27. System Information Functions and Operators\n9.28. System Administration Functions\n9.29. Trigger Functions\n9.30. Event Trigger Functions\n9.31. Statistics Information Functions\n10. Type Conversion\n10.1. Overview\n10.2. Operators\n10.3. Functions\n10.4. Value Storage\n10.5.\nUNION\n,\nCASE\n, and Related Constructs\n10.6.\nSELECT\nOutput Columns\n11. Indexes\n11.1. Introduction\n11.2. Index Types\n11.3. Multicolumn Indexes\n11.4. Indexes and\nORDER BY\n11.5. Combining Multiple Indexes\n11.6. Unique Indexes\n11.7. Indexes on Expressions\n11.8. Partial Indexes\n11.9. Index-Only Scans and Covering Indexes\n11.10. Operator Classes and Operator Families\n11.11. Indexes and Collations\n11.12. Examining Index Usage\n12. Full Text Search\n12.1. Introduction\n12.2. Tables and Indexes\n12.3. Controlling Text Search\n12.4. Additional Features\n12.5. Parsers\n12.6. Dictionaries\n12.7. Configuration Example\n12.8. Testing and Debugging Text Search\n12.9. Preferred Index Types for Text Search\n12.10.\npsql\nSupport\n12.11. Limitations\n13. Concurrency Control\n13.1. Introduction\n13.2. Transaction Isolation\n13.3. Explicit Locking\n13.4. Data Consistency Checks at the Application Level\n13.5. Serialization Failure Handling\n13.6. Caveats\n13.7. Locking and Indexes\n14. Performance Tips\n14.1. Using\nEXPLAIN\n14.2. Statistics Used by the Planner\n14.3. Controlling the Planner with Explicit\nJOIN\nClauses\n14.4. Populating a Database\n14.5. Non-Durable Settings\n15. Parallel Query\n15.1. How Parallel Query Works\n15.2. When Can Parallel Query Be Used?\n15.3. Parallel Plans\n15.4. Parallel Safety\nPrev\nUp\nNext\n3.7. Conclusion\nHome\nChapter 4. SQL Syntax\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/sql.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31788.4842692
  },
  {
    "title": "Part IV. Client Interfaces",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\nPart IV. Client Interfaces\nPrev\nUp\nPostgreSQL 17.5 Documentation\nHome\nNext\nPart IV. Client Interfaces\nThis part describes the client programming interfaces distributed with\nPostgreSQL\n. Each of these chapters can be read independently. There are many external programming interfaces for client programs that are distributed separately. They contain their own documentation (\nAppendix H\nlists some of the more popular ones). Readers of this part should be familiar with using\nSQL\nto manipulate and query the database (see\nPart II\n) and of course with the programming language of their choice.\nTable of Contents\n32.\nlibpq\n— C Library\n32.1. Database Connection Control Functions\n32.2. Connection Status Functions\n32.3. Command Execution Functions\n32.4. Asynchronous Command Processing\n32.5. Pipeline Mode\n32.6. Retrieving Query Results in Chunks\n32.7. Canceling Queries in Progress\n32.8. The Fast-Path Interface\n32.9. Asynchronous Notification\n32.10. Functions Associated with the\nCOPY\nCommand\n32.11. Control Functions\n32.12. Miscellaneous Functions\n32.13. Notice Processing\n32.14. Event System\n32.15. Environment Variables\n32.16. The Password File\n32.17. The Connection Service File\n32.18. LDAP Lookup of Connection Parameters\n32.19. SSL Support\n32.20. Behavior in Threaded Programs\n32.21. Building\nlibpq\nPrograms\n32.22. Example Programs\n33. Large Objects\n33.1. Introduction\n33.2. Implementation Features\n33.3. Client Interfaces\n33.4. Server-Side Functions\n33.5. Example Program\n34.\nECPG\n— Embedded\nSQL\nin C\n34.1. The Concept\n34.2. Managing Database Connections\n34.3. Running SQL Commands\n34.4. Using Host Variables\n34.5. Dynamic SQL\n34.6. pgtypes Library\n34.7. Using Descriptor Areas\n34.8. Error Handling\n34.9. Preprocessor Directives\n34.10. Processing Embedded SQL Programs\n34.11. Library Functions\n34.12. Large Objects\n34.13.\nC++\nApplications\n34.14. Embedded SQL Commands\n34.15.\nInformix\nCompatibility Mode\n34.16.\nOracle\nCompatibility Mode\n34.17. Internals\n35. The Information Schema\n35.1. The Schema\n35.2. Data Types\n35.3.\ninformation_schema_catalog_name\n35.4.\nadministrable_role_​authorizations\n35.5.\napplicable_roles\n35.6.\nattributes\n35.7.\ncharacter_sets\n35.8.\ncheck_constraint_routine_usage\n35.9.\ncheck_constraints\n35.10.\ncollations\n35.11.\ncollation_character_set_​applicability\n35.12.\ncolumn_column_usage\n35.13.\ncolumn_domain_usage\n35.14.\ncolumn_options\n35.15.\ncolumn_privileges\n35.16.\ncolumn_udt_usage\n35.17.\ncolumns\n35.18.\nconstraint_column_usage\n35.19.\nconstraint_table_usage\n35.20.\ndata_type_privileges\n35.21.\ndomain_constraints\n35.22.\ndomain_udt_usage\n35.23.\ndomains\n35.24.\nelement_types\n35.25.\nenabled_roles\n35.26.\nforeign_data_wrapper_options\n35.27.\nforeign_data_wrappers\n35.28.\nforeign_server_options\n35.29.\nforeign_servers\n35.30.\nforeign_table_options\n35.31.\nforeign_tables\n35.32.\nkey_column_usage\n35.33.\nparameters\n35.34.\nreferential_constraints\n35.35.\nrole_column_grants\n35.36.\nrole_routine_grants\n35.37.\nrole_table_grants\n35.38.\nrole_udt_grants\n35.39.\nrole_usage_grants\n35.40.\nroutine_column_usage\n35.41.\nroutine_privileges\n35.42.\nroutine_routine_usage\n35.43.\nroutine_sequence_usage\n35.44.\nroutine_table_usage\n35.45.\nroutines\n35.46.\nschemata\n35.47.\nsequences\n35.48.\nsql_features\n35.49.\nsql_implementation_info\n35.50.\nsql_parts\n35.51.\nsql_sizing\n35.52.\ntable_constraints\n35.53.\ntable_privileges\n35.54.\ntables\n35.55.\ntransforms\n35.56.\ntriggered_update_columns\n35.57.\ntriggers\n35.58.\nudt_privileges\n35.59.\nusage_privileges\n35.60.\nuser_defined_types\n35.61.\nuser_mapping_options\n35.62.\nuser_mappings\n35.63.\nview_column_usage\n35.64.\nview_routine_usage\n35.65.\nview_table_usage\n35.66.\nviews\nPrev\nUp\nNext\n31.5. Test Coverage Examination\nHome\nChapter 32.\nlibpq\n— C Library\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/client-interfaces.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31788.5626698
  },
  {
    "title": "PostgreSQL: Documentation: 17: DROP USER",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nDROP USER\nPrev\nUp\nSQL Commands\nHome\nNext\nDROP USER\nDROP USER — remove a database role\nSynopsis\nDROP USER [ IF EXISTS ]\nname\n[, ...]\nDescription\nDROP USER\nis simply an alternate spelling of\nDROP ROLE\n.\nCompatibility\nThe\nDROP USER\nstatement is a\nPostgreSQL\nextension. The SQL standard leaves the definition of users to the implementation.\nSee Also\nDROP ROLE\nPrev\nUp\nNext\nDROP TYPE\nHome\nDROP USER MAPPING\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/sql-dropuser.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31788.8181125
  },
  {
    "title": "Part III. Server Administration",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nPart III. Server Administration\nPrev\nUp\nPostgreSQL 17.5 Documentation\nHome\nNext\nPart III. Server Administration\nThis part covers topics that are of interest to a\nPostgreSQL\nadministrator. This includes installation, configuration of the server, management of users and databases, and maintenance tasks. Anyone running\nPostgreSQL\nserver, even for personal use, but especially in production, should be familiar with these topics.\nThe information attempts to be in the order in which a new user should read it. The chapters are self-contained and can be read individually as desired. The information is presented in a narrative form in topical units. Readers looking for a complete description of a command are encouraged to review the\nPart VI\n.\nThe first few chapters are written so they can be understood without prerequisite knowledge, so new users who need to set up their own server can begin their exploration. The rest of this part is about tuning and management; that material assumes that the reader is familiar with the general use of the\nPostgreSQL\ndatabase system. Readers are encouraged review the\nPart I\nand\nPart II\nparts for additional information.\nTable of Contents\n16. Installation from Binaries\n17. Installation from Source Code\n17.1. Requirements\n17.2. Getting the Source\n17.3. Building and Installation with Autoconf and Make\n17.4. Building and Installation with Meson\n17.5. Post-Installation Setup\n17.6. Supported Platforms\n17.7. Platform-Specific Notes\n18. Server Setup and Operation\n18.1. The\nPostgreSQL\nUser Account\n18.2. Creating a Database Cluster\n18.3. Starting the Database Server\n18.4. Managing Kernel Resources\n18.5. Shutting Down the Server\n18.6. Upgrading a\nPostgreSQL\nCluster\n18.7. Preventing Server Spoofing\n18.8. Encryption Options\n18.9. Secure TCP/IP Connections with SSL\n18.10. Secure TCP/IP Connections with GSSAPI Encryption\n18.11. Secure TCP/IP Connections with\nSSH\nTunnels\n18.12. Registering\nEvent Log\non\nWindows\n19. Server Configuration\n19.1. Setting Parameters\n19.2. File Locations\n19.3. Connections and Authentication\n19.4. Resource Consumption\n19.5. Write Ahead Log\n19.6. Replication\n19.7. Query Planning\n19.8. Error Reporting and Logging\n19.9. Run-time Statistics\n19.10. Automatic Vacuuming\n19.11. Client Connection Defaults\n19.12. Lock Management\n19.13. Version and Platform Compatibility\n19.14. Error Handling\n19.15. Preset Options\n19.16. Customized Options\n19.17. Developer Options\n19.18. Short Options\n20. Client Authentication\n20.1. The\npg_hba.conf\nFile\n20.2. User Name Maps\n20.3. Authentication Methods\n20.4. Trust Authentication\n20.5. Password Authentication\n20.6. GSSAPI Authentication\n20.7. SSPI Authentication\n20.8. Ident Authentication\n20.9. Peer Authentication\n20.10. LDAP Authentication\n20.11. RADIUS Authentication\n20.12. Certificate Authentication\n20.13. PAM Authentication\n20.14. BSD Authentication\n20.15. Authentication Problems\n21. Database Roles\n21.1. Database Roles\n21.2. Role Attributes\n21.3. Role Membership\n21.4. Dropping Roles\n21.5. Predefined Roles\n21.6. Function Security\n22. Managing Databases\n22.1. Overview\n22.2. Creating a Database\n22.3. Template Databases\n22.4. Database Configuration\n22.5. Destroying a Database\n22.6. Tablespaces\n23. Localization\n23.1. Locale Support\n23.2. Collation Support\n23.3. Character Set Support\n24. Routine Database Maintenance Tasks\n24.1. Routine Vacuuming\n24.2. Routine Reindexing\n24.3. Log File Maintenance\n25. Backup and Restore\n25.1.\nSQL\nDump\n25.2. File System Level Backup\n25.3. Continuous Archiving and Point-in-Time Recovery (PITR)\n26. High Availability, Load Balancing, and Replication\n26.1. Comparison of Different Solutions\n26.2. Log-Shipping Standby Servers\n26.3. Failover\n26.4. Hot Standby\n27. Monitoring Database Activity\n27.1. Standard Unix Tools\n27.2. The Cumulative Statistics System\n27.3. Viewing Locks\n27.4. Progress Reporting\n27.5. Dynamic Tracing\n27.6. Monitoring Disk Usage\n28. Reliability and the Write-Ahead Log\n28.1. Reliability\n28.2. Data Checksums\n28.3. Write-Ahead Logging (\nWAL\n)\n28.4. Asynchronous Commit\n28.5.\nWAL\nConfiguration\n28.6. WAL Internals\n29. Logical Replication\n29.1. Publication\n29.2. Subscription\n29.3. Logical Replication Failover\n29.4. Row Filters\n29.5. Column Lists\n29.6. Conflicts\n29.7. Restrictions\n29.8. Architecture\n29.9. Monitoring\n29.10. Security\n29.11. Configuration Settings\n29.12. Quick Setup\n30. Just-in-Time Compilation (\nJIT\n)\n30.1. What Is\nJIT\ncompilation?\n30.2. When to\nJIT\n?\n30.3. Configuration\n30.4. Extensibility\n31. Regression Tests\n31.1. Running the Tests\n31.2. Test Evaluation\n31.3. Variant Comparison Files\n31.4. TAP Tests\n31.5. Test Coverage Examination\nPrev\nUp\nNext\n15.4. Parallel Safety\nHome\nChapter 16. Installation from Binaries\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/admin.html",
    "source": "postgresql",
    "doc_type": "administration",
    "scraped_at": 31788.9655329
  },
  {
    "title": "Part I. Tutorial",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nPart I. Tutorial\nPrev\nUp\nPostgreSQL 17.5 Documentation\nHome\nNext\nPart I. Tutorial\nWelcome to the\nPostgreSQL\nTutorial. The tutorial is intended to give an introduction to\nPostgreSQL\n, relational database concepts, and the SQL language. We assume some general knowledge about how to use computers and no particular Unix or programming experience is required. This tutorial is intended to provide hands-on experience with important aspects of the\nPostgreSQL\nsystem. It makes no attempt to be a comprehensive treatment of the topics it covers.\nAfter you have successfully completed this tutorial you will want to read the\nPart II\nsection to gain a better understanding of the SQL language, or\nPart IV\nfor information about developing applications with\nPostgreSQL\n. Those who provision and manage their own PostgreSQL installation should also read\nPart III\n.\nTable of Contents\n1. Getting Started\n1.1. Installation\n1.2. Architectural Fundamentals\n1.3. Creating a Database\n1.4. Accessing a Database\n2. The\nSQL\nLanguage\n2.1. Introduction\n2.2. Concepts\n2.3. Creating a New Table\n2.4. Populating a Table With Rows\n2.5. Querying a Table\n2.6. Joins Between Tables\n2.7. Aggregate Functions\n2.8. Updates\n2.9. Deletions\n3. Advanced Features\n3.1. Introduction\n3.2. Views\n3.3. Foreign Keys\n3.4. Transactions\n3.5. Window Functions\n3.6. Inheritance\n3.7. Conclusion\nPrev\nUp\nNext\n5. Bug Reporting Guidelines\nHome\nChapter 1. Getting Started\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/tutorial.html",
    "source": "postgresql",
    "doc_type": "tutorial",
    "scraped_at": 31789.0516747
  },
  {
    "title": "PostgreSQL: Documentation: 17: 9.5. Binary String Functions and Operators",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n9.5. Binary String Functions and Operators\nPrev\nUp\nChapter 9. Functions and Operators\nHome\nNext\n9.5. Binary String Functions and Operators\n#\nThis section describes functions and operators for examining and manipulating binary strings, that is values of type\nbytea\n. Many of these are equivalent, in purpose and syntax, to the text-string functions described in the previous section.\nSQL\ndefines some string functions that use key words, rather than commas, to separate arguments. Details are in\nTable 9.11\n.\nPostgreSQL\nalso provides versions of these functions that use the regular function invocation syntax (see\nTable 9.12\n).\nTable 9.11.\nSQL\nBinary String Functions and Operators\nFunction/Operator\nDescription\nExample(s)\nbytea\n||\nbytea\n→\nbytea\nConcatenates the two binary strings.\n'\\x123456'::bytea || '\\x789a00bcde'::bytea\n→\n\\x123456789a00bcde\nbit_length\n(\nbytea\n) →\ninteger\nReturns number of bits in the binary string (8 times the\noctet_length\n).\nbit_length('\\x123456'::bytea)\n→\n24\nbtrim\n(\nbytes\nbytea\n,\nbytesremoved\nbytea\n) →\nbytea\nRemoves the longest string containing only bytes appearing in\nbytesremoved\nfrom the start and end of\nbytes\n.\nbtrim('\\x1234567890'::bytea, '\\x9012'::bytea)\n→\n\\x345678\nltrim\n(\nbytes\nbytea\n,\nbytesremoved\nbytea\n) →\nbytea\nRemoves the longest string containing only bytes appearing in\nbytesremoved\nfrom the start of\nbytes\n.\nltrim('\\x1234567890'::bytea, '\\x9012'::bytea)\n→\n\\x34567890\noctet_length\n(\nbytea\n) →\ninteger\nReturns number of bytes in the binary string.\noctet_length('\\x123456'::bytea)\n→\n3\noverlay\n(\nbytes\nbytea\nPLACING\nnewsubstring\nbytea\nFROM\nstart\ninteger\n[\nFOR\ncount\ninteger\n] ) →\nbytea\nReplaces the substring of\nbytes\nthat starts at the\nstart\n'th byte and extends for\ncount\nbytes with\nnewsubstring\n. If\ncount\nis omitted, it defaults to the length of\nnewsubstring\n.\noverlay('\\x1234567890'::bytea placing '\\002\\003'::bytea from 2 for 3)\n→\n\\x12020390\nposition\n(\nsubstring\nbytea\nIN\nbytes\nbytea\n) →\ninteger\nReturns first starting index of the specified\nsubstring\nwithin\nbytes\n, or zero if it's not present.\nposition('\\x5678'::bytea in '\\x1234567890'::bytea)\n→\n3\nrtrim\n(\nbytes\nbytea\n,\nbytesremoved\nbytea\n) →\nbytea\nRemoves the longest string containing only bytes appearing in\nbytesremoved\nfrom the end of\nbytes\n.\nrtrim('\\x1234567890'::bytea, '\\x9012'::bytea)\n→\n\\x12345678\nsubstring\n(\nbytes\nbytea\n[\nFROM\nstart\ninteger\n] [\nFOR\ncount\ninteger\n] ) →\nbytea\nExtracts the substring of\nbytes\nstarting at the\nstart\n'th byte if that is specified, and stopping after\ncount\nbytes if that is specified. Provide at least one of\nstart\nand\ncount\n.\nsubstring('\\x1234567890'::bytea from 3 for 2)\n→\n\\x5678\ntrim\n( [\nLEADING\n|\nTRAILING\n|\nBOTH\n]\nbytesremoved\nbytea\nFROM\nbytes\nbytea\n) →\nbytea\nRemoves the longest string containing only bytes appearing in\nbytesremoved\nfrom the start, end, or both ends (\nBOTH\nis the default) of\nbytes\n.\ntrim('\\x9012'::bytea from '\\x1234567890'::bytea)\n→\n\\x345678\ntrim\n( [\nLEADING\n|\nTRAILING\n|\nBOTH\n] [\nFROM\n]\nbytes\nbytea\n,\nbytesremoved\nbytea\n) →\nbytea\nThis is a non-standard syntax for\ntrim()\n.\ntrim(both from '\\x1234567890'::bytea, '\\x9012'::bytea)\n→\n\\x345678\nAdditional binary string manipulation functions are available and are listed in\nTable 9.12\n. Some of them are used internally to implement the\nSQL\n-standard string functions listed in\nTable 9.11\n.\nTable 9.12. Other Binary String Functions\nFunction\nDescription\nExample(s)\nbit_count\n(\nbytes\nbytea\n) →\nbigint\nReturns the number of bits set in the binary string (also known as\n“\npopcount\n”\n).\nbit_count('\\x1234567890'::bytea)\n→\n15\nget_bit\n(\nbytes\nbytea\n,\nn\nbigint\n) →\ninteger\nExtracts\nn'th\nbit from binary string.\nget_bit('\\x1234567890'::bytea, 30)\n→\n1\nget_byte\n(\nbytes\nbytea\n,\nn\ninteger\n) →\ninteger\nExtracts\nn'th\nbyte from binary string.\nget_byte('\\x1234567890'::bytea, 4)\n→\n144\nlength\n(\nbytea\n) →\ninteger\nReturns the number of bytes in the binary string.\nlength('\\x1234567890'::bytea)\n→\n5\nlength\n(\nbytes\nbytea\n,\nencoding\nname\n) →\ninteger\nReturns the number of characters in the binary string, assuming that it is text in the given\nencoding\n.\nlength('jose'::bytea, 'UTF8')\n→\n4\nmd5\n(\nbytea\n) →\ntext\nComputes the MD5\nhash\nof the binary string, with the result written in hexadecimal.\nmd5('Th\\000omas'::bytea)\n→\n8ab2d3c9689aaf18​b4958c334c82d8b1\nset_bit\n(\nbytes\nbytea\n,\nn\nbigint\n,\nnewvalue\ninteger\n) →\nbytea\nSets\nn'th\nbit in binary string to\nnewvalue\n.\nset_bit('\\x1234567890'::bytea, 30, 0)\n→\n\\x1234563890\nset_byte\n(\nbytes\nbytea\n,\nn\ninteger\n,\nnewvalue\ninteger\n) →\nbytea\nSets\nn'th\nbyte in binary string to\nnewvalue\n.\nset_byte('\\x1234567890'::bytea, 4, 64)\n→\n\\x1234567840\nsha224\n(\nbytea\n) →\nbytea\nComputes the SHA-224\nhash\nof the binary string.\nsha224('abc'::bytea)\n→\n\\x23097d223405d8228642a477bda2​55b32aadbce4bda0b3f7e36c9da7\nsha256\n(\nbytea\n) →\nbytea\nComputes the SHA-256\nhash\nof the binary string.\nsha256('abc'::bytea)\n→\n\\xba7816bf8f01cfea414140de5dae2223​b00361a396177a9cb410ff61f20015ad\nsha384\n(\nbytea\n) →\nbytea\nComputes the SHA-384\nhash\nof the binary string.\nsha384('abc'::bytea)\n→\n\\xcb00753f45a35e8bb5a03d699ac65007​272c32ab0eded1631a8b605a43ff5bed​8086072ba1e7cc2358baeca134c825a7\nsha512\n(\nbytea\n) →\nbytea\nComputes the SHA-512\nhash\nof the binary string.\nsha512('abc'::bytea)\n→\n\\xddaf35a193617abacc417349ae204131​12e6fa4e89a97ea20a9eeee64b55d39a​2192992a274fc1a836ba3c23a3feebbd​454d4423643ce80e2a9ac94fa54ca49f\nsubstr\n(\nbytes\nbytea\n,\nstart\ninteger\n[\n,\ncount\ninteger\n] ) →\nbytea\nExtracts the substring of\nbytes\nstarting at the\nstart\n'th byte, and extending for\ncount\nbytes if that is specified. (Same as\nsubstring(\nbytes\nfrom\nstart\nfor\ncount\n)\n.)\nsubstr('\\x1234567890'::bytea, 3, 2)\n→\n\\x5678\nFunctions\nget_byte\nand\nset_byte\nnumber the first byte of a binary string as byte 0. Functions\nget_bit\nand\nset_bit\nnumber bits from the right within each byte; for example bit 0 is the least significant bit of the first byte, and bit 15 is the most significant bit of the second byte.\nFor historical reasons, the function\nmd5\nreturns a hex-encoded value of type\ntext\nwhereas the SHA-2 functions return type\nbytea\n. Use the functions\nencode\nand\ndecode\nto convert between the two. For example write\nencode(sha256('abc'), 'hex')\nto get a hex-encoded text representation, or\ndecode(md5('abc'), 'hex')\nto get a\nbytea\nvalue.\nFunctions for converting strings between different character sets (encodings), and for representing arbitrary binary data in textual form, are shown in\nTable 9.13\n. For these functions, an argument or result of type\ntext\nis expressed in the database's default encoding, while arguments or results of type\nbytea\nare in an encoding named by another argument.\nTable 9.13. Text/Binary String Conversion Functions\nFunction\nDescription\nExample(s)\nconvert\n(\nbytes\nbytea\n,\nsrc_encoding\nname\n,\ndest_encoding\nname\n) →\nbytea\nConverts a binary string representing text in encoding\nsrc_encoding\nto a binary string in encoding\ndest_encoding\n(see\nSection 23.3.4\nfor available conversions).\nconvert('text_in_utf8', 'UTF8', 'LATIN1')\n→\n\\x746578745f696e5f75746638\nconvert_from\n(\nbytes\nbytea\n,\nsrc_encoding\nname\n) →\ntext\nConverts a binary string representing text in encoding\nsrc_encoding\nto\ntext\nin the database encoding (see\nSection 23.3.4\nfor available conversions).\nconvert_from('text_in_utf8', 'UTF8')\n→\ntext_in_utf8\nconvert_to\n(\nstring\ntext\n,\ndest_encoding\nname\n) →\nbytea\nConverts a\ntext\nstring (in the database encoding) to a binary string encoded in encoding\ndest_encoding\n(see\nSection 23.3.4\nfor available conversions).\nconvert_to('some_text', 'UTF8')\n→\n\\x736f6d655f74657874\nencode\n(\nbytes\nbytea\n,\nformat\ntext\n) →\ntext\nEncodes binary data into a textual representation; supported\nformat\nvalues are:\nbase64\n,\nescape\n,\nhex\n.\nencode('123\\000\\001', 'base64')\n→\nMTIzAAE=\ndecode\n(\nstring\ntext\n,\nformat\ntext\n) →\nbytea\nDecodes binary data from a textual representation; supported\nformat\nvalues are the same as for\nencode\n.\ndecode('MTIzAAE=', 'base64')\n→\n\\x3132330001\nThe\nencode\nand\ndecode\nfunctions support the following textual formats:\nbase64\n#\nThe\nbase64\nformat is that of\nRFC 2045 Section 6.8\n. As per the\nRFC\n, encoded lines are broken at 76 characters. However instead of the MIME CRLF end-of-line marker, only a newline is used for end-of-line. The\ndecode\nfunction ignores carriage-return, newline, space, and tab characters. Otherwise, an error is raised when\ndecode\nis supplied invalid base64 data — including when trailing padding is incorrect.\nescape\n#\nThe\nescape\nformat converts zero bytes and bytes with the high bit set into octal escape sequences (\n\\\nnnn\n), and it doubles backslashes. Other byte values are represented literally. The\ndecode\nfunction will raise an error if a backslash is not followed by either a second backslash or three octal digits; it accepts other byte values unchanged.\nhex\n#\nThe\nhex\nformat represents each 4 bits of data as one hexadecimal digit,\n0\nthrough\nf\n, writing the higher-order digit of each byte first. The\nencode\nfunction outputs the\na\n-\nf\nhex digits in lower case. Because the smallest unit of data is 8 bits, there are always an even number of characters returned by\nencode\n. The\ndecode\nfunction accepts the\na\n-\nf\ncharacters in either upper or lower case. An error is raised when\ndecode\nis given invalid hex data — including when given an odd number of characters.\nSee also the aggregate function\nstring_agg\nin\nSection 9.21\nand the large object functions in\nSection 33.4\n.\nPrev\nUp\nNext\n9.4. String Functions and Operators\nHome\n9.6. Bit String Functions and Operators\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/functions-binarystring.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31789.1359084
  },
  {
    "title": "PostgreSQL: Documentation: 17: 33.4. Server-Side Functions",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\n33.4. Server-Side Functions\nPrev\nUp\nChapter 33. Large Objects\nHome\nNext\n33.4. Server-Side Functions\n#\nServer-side functions tailored for manipulating large objects from SQL are listed in\nTable 33.1\n.\nTable 33.1. SQL-Oriented Large Object Functions\nFunction\nDescription\nExample(s)\nlo_from_bytea\n(\nloid\noid\n,\ndata\nbytea\n) →\noid\nCreates a large object and stores\ndata\nin it. If\nloid\nis zero then the system will choose a free OID, otherwise that OID is used (with an error if some large object already has that OID). On success, the large object's OID is returned.\nlo_from_bytea(0, '\\xffffff00')\n→\n24528\nlo_put\n(\nloid\noid\n,\noffset\nbigint\n,\ndata\nbytea\n) →\nvoid\nWrites\ndata\nstarting at the given offset within the large object; the large object is enlarged if necessary.\nlo_put(24528, 1, '\\xaa')\n→\nlo_get\n(\nloid\noid\n[\n,\noffset\nbigint\n,\nlength\ninteger\n] ) →\nbytea\nExtracts the large object's contents, or a substring thereof.\nlo_get(24528, 0, 3)\n→\n\\xffaaff\nThere are additional server-side functions corresponding to each of the client-side functions described earlier; indeed, for the most part the client-side functions are simply interfaces to the equivalent server-side functions. The ones just as convenient to call via SQL commands are\nlo_creat\n,\nlo_create\n,\nlo_unlink\n,\nlo_import\n, and\nlo_export\n. Here are examples of their use:\nCREATE TABLE image (\nname            text,\nraster          oid\n);\nSELECT lo_creat(-1);       -- returns OID of new, empty large object\nSELECT lo_create(43213);   -- attempts to create large object with OID 43213\nSELECT lo_unlink(173454);  -- deletes large object with OID 173454\nINSERT INTO image (name, raster)\nVALUES ('beautiful image', lo_import('/etc/motd'));\nINSERT INTO image (name, raster)  -- same as above, but specify OID to use\nVALUES ('beautiful image', lo_import('/etc/motd', 68583));\nSELECT lo_export(image.raster, '/tmp/motd') FROM image\nWHERE name = 'beautiful image';\nThe server-side\nlo_import\nand\nlo_export\nfunctions behave considerably differently from their client-side analogs. These two functions read and write files in the server's file system, using the permissions of the database's owning user. Therefore, by default their use is restricted to superusers. In contrast, the client-side import and export functions read and write files in the client's file system, using the permissions of the client program. The client-side functions do not require any database privileges, except the privilege to read or write the large object in question.\nCaution\nIt is possible to\nGRANT\nuse of the server-side\nlo_import\nand\nlo_export\nfunctions to non-superusers, but careful consideration of the security implications is required. A malicious user of such privileges could easily parlay them into becoming superuser (for example by rewriting server configuration files), or could attack the rest of the server's file system without bothering to obtain database superuser privileges as such.\nAccess to roles having such privilege must therefore be guarded just as carefully as access to superuser roles.\nNonetheless, if use of server-side\nlo_import\nor\nlo_export\nis needed for some routine task, it's safer to use a role with such privileges than one with full superuser privileges, as that helps to reduce the risk of damage from accidental errors.\nThe functionality of\nlo_read\nand\nlo_write\nis also available via server-side calls, but the names of the server-side functions differ from the client side interfaces in that they do not contain underscores. You must call these functions as\nloread\nand\nlowrite\n.\nPrev\nUp\nNext\n33.3. Client Interfaces\nHome\n33.5. Example Program\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/lo-funcs.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31789.2636653
  },
  {
    "title": "VI. Reference",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 8.3\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPostgreSQL\n8.3.23 Documentation\nPrev\nFast Backward\nFast Forward\nNext\nVI. Reference\nThe entries in this Reference are meant to provide in\nreasonable length an authoritative, complete, and formal\nsummary about their respective subjects. More information\nabout the use of\nPostgreSQL\n,\nin narrative, tutorial, or example form, can be found in\nother parts of this book. See the cross-references listed on\neach reference page.\nThe reference entries are also available as traditional\n\"man\"\npages.\nTable of Contents\nI.\nSQL Commands\nABORT\n-- abort the\ncurrent transaction\nALTER\nAGGREGATE\n-- change the definition of an\naggregate function\nALTER\nCONVERSION\n-- change the definition of a\nconversion\nALTER\nDATABASE\n-- change a database\nALTER\nDOMAIN\n--  change the definition of a\ndomain\nALTER\nFUNCTION\n-- change the definition of a\nfunction\nALTER\nGROUP\n-- change role name or\nmembership\nALTER\nINDEX\n-- change the definition of an\nindex\nALTER\nLANGUAGE\n-- change the definition of a\nprocedural language\nALTER\nOPERATOR\n-- change the definition of an\noperator\nALTER OPERATOR\nCLASS\n-- change the definition of an\noperator class\nALTER OPERATOR\nFAMILY\n-- change the definition of an\noperator family\nALTER\nROLE\n-- change a database role\nALTER\nSCHEMA\n-- change the definition of a\nschema\nALTER\nSEQUENCE\n--  change the definition of a\nsequence generator\nALTER\nTABLE\n-- change the definition of a\ntable\nALTER\nTABLESPACE\n-- change the definition of a\ntablespace\nALTER TEXT SEARCH\nCONFIGURATION\n-- change the definition of\na text search configuration\nALTER TEXT\nSEARCH DICTIONARY\n-- change the\ndefinition of a text search dictionary\nALTER TEXT SEARCH\nPARSER\n-- change the definition of a text\nsearch parser\nALTER TEXT\nSEARCH TEMPLATE\n-- change the definition\nof a text search template\nALTER\nTRIGGER\n-- change the definition of a\ntrigger\nALTER\nTYPE\n--  change the definition of a\ntype\nALTER\nUSER\n-- change a database role\nALTER\nVIEW\n-- change the definition of a\nview\nANALYZE\n-- collect\nstatistics about a database\nBEGIN\n-- start a\ntransaction block\nCHECKPOINT\n-- force\na transaction log checkpoint\nCLOSE\n-- close a\ncursor\nCLUSTER\n-- cluster a\ntable according to an index\nCOMMENT\n-- define or\nchange the comment of an object\nCOMMIT\n-- commit the\ncurrent transaction\nCOMMIT\nPREPARED\n-- commit a transaction that was\nearlier prepared for two-phase commit\nCOPY\n-- copy\ndata between a file and a table\nCREATE\nAGGREGATE\n-- define a new aggregate\nfunction\nCREATE\nCAST\n-- define a new cast\nCREATE\nCONSTRAINT TRIGGER\n-- define a new\nconstraint trigger\nCREATE\nCONVERSION\n-- define a new encoding\nconversion\nCREATE\nDATABASE\n-- create a new database\nCREATE\nDOMAIN\n-- define a new domain\nCREATE\nFUNCTION\n-- define a new function\nCREATE\nGROUP\n-- define a new database role\nCREATE\nINDEX\n-- define a new index\nCREATE\nLANGUAGE\n-- define a new procedural\nlanguage\nCREATE\nOPERATOR\n-- define a new operator\nCREATE OPERATOR\nCLASS\n-- define a new operator class\nCREATE OPERATOR\nFAMILY\n-- define a new operator\nfamily\nCREATE\nROLE\n-- define a new database role\nCREATE\nRULE\n-- define a new rewrite rule\nCREATE\nSCHEMA\n-- define a new schema\nCREATE\nSEQUENCE\n-- define a new sequence\ngenerator\nCREATE\nTABLE\n-- define a new table\nCREATE TABLE\nAS\n-- define a new table from the results\nof a query\nCREATE\nTABLESPACE\n-- define a new\ntablespace\nCREATE TEXT\nSEARCH CONFIGURATION\n-- define a new text\nsearch configuration\nCREATE TEXT\nSEARCH DICTIONARY\n-- define a new text\nsearch dictionary\nCREATE TEXT\nSEARCH PARSER\n-- define a new text search\nparser\nCREATE TEXT\nSEARCH TEMPLATE\n-- define a new text\nsearch template\nCREATE\nTRIGGER\n-- define a new trigger\nCREATE\nTYPE\n-- define a new data type\nCREATE\nUSER\n-- define a new database role\nCREATE\nVIEW\n-- define a new view\nDEALLOCATE\n-- deallocate\na prepared statement\nDECLARE\n-- define a\ncursor\nDELETE\n-- delete rows\nof a table\nDISCARD\n-- discard\nsession state\nDROP\nAGGREGATE\n-- remove an aggregate\nfunction\nDROP\nCAST\n-- remove a cast\nDROP\nCONVERSION\n-- remove a conversion\nDROP\nDATABASE\n-- remove a database\nDROP\nDOMAIN\n-- remove a domain\nDROP\nFUNCTION\n-- remove a function\nDROP\nGROUP\n-- remove a database role\nDROP\nINDEX\n-- remove an index\nDROP\nLANGUAGE\n-- remove a procedural\nlanguage\nDROP\nOPERATOR\n-- remove an operator\nDROP OPERATOR\nCLASS\n-- remove an operator class\nDROP OPERATOR\nFAMILY\n-- remove an operator family\nDROP\nOWNED\n-- remove database objects owned by\na database role\nDROP\nROLE\n-- remove a database role\nDROP\nRULE\n-- remove a rewrite rule\nDROP\nSCHEMA\n-- remove a schema\nDROP\nSEQUENCE\n-- remove a sequence\nDROP\nTABLE\n-- remove a table\nDROP\nTABLESPACE\n-- remove a tablespace\nDROP TEXT SEARCH\nCONFIGURATION\n-- remove a text search\nconfiguration\nDROP TEXT\nSEARCH DICTIONARY\n-- remove a text search\ndictionary\nDROP TEXT SEARCH\nPARSER\n-- remove a text search\nparser\nDROP TEXT SEARCH\nTEMPLATE\n-- remove a text search\ntemplate\nDROP\nTRIGGER\n-- remove a trigger\nDROP\nTYPE\n-- remove a data type\nDROP\nUSER\n-- remove a database role\nDROP\nVIEW\n-- remove a view\nEND\n-- commit\nthe current transaction\nEXECUTE\n-- execute a\nprepared statement\nEXPLAIN\n-- show the\nexecution plan of a statement\nFETCH\n-- retrieve rows\nfrom a query using a cursor\nGRANT\n-- define access\nprivileges\nINSERT\n-- create new\nrows in a table\nLISTEN\n-- listen for a\nnotification\nLOAD\n-- load\na shared library file\nLOCK\n-- lock\na table\nMOVE\n-- position a\ncursor\nNOTIFY\n-- generate a\nnotification\nPREPARE\n-- prepare a\nstatement for execution\nPREPARE\nTRANSACTION\n-- prepare the current\ntransaction for two-phase commit\nREASSIGN\nOWNED\n-- change the ownership of database\nobjects owned by a database role\nREINDEX\n-- rebuild\nindexes\nRELEASE\nSAVEPOINT\n-- destroy a previously defined\nsavepoint\nRESET\n-- restore the\nvalue of a run-time parameter to the default value\nREVOKE\n-- remove access\nprivileges\nROLLBACK\n-- abort the\ncurrent transaction\nROLLBACK\nPREPARED\n-- cancel a transaction that was\nearlier prepared for two-phase commit\nROLLBACK TO\nSAVEPOINT\n-- roll back to a\nsavepoint\nSAVEPOINT\n-- define\na new savepoint within the current transaction\nSELECT\n-- retrieve rows\nfrom a table or view\nSELECT\nINTO\n-- define a new table from the\nresults of a query\nSET\n-- change\na run-time parameter\nSET\nCONSTRAINTS\n-- set constraint checking\nmodes for the current transaction\nSET\nROLE\n-- set the current user identifier\nof the current session\nSET\nSESSION AUTHORIZATION\n-- set the session\nuser identifier and the current user identifier of the\ncurrent session\nSET\nTRANSACTION\n-- set the characteristics of\nthe current transaction\nSHOW\n-- show\nthe value of a run-time parameter\nSTART\nTRANSACTION\n-- start a transaction\nblock\nTRUNCATE\n-- empty a\ntable or set of tables\nUNLISTEN\n-- stop\nlistening for a notification\nUPDATE\n-- update rows\nof a table\nVACUUM\n-- garbage-collect\nand optionally analyze a database\nVALUES\n-- compute a set\nof rows\nII.\nPostgreSQL Client\nApplications\nclusterdb\n-- cluster\na\nPostgreSQL\ndatabase\ncreatedb\n-- create a\nnew\nPostgreSQL\ndatabase\ncreatelang\n-- define\na new\nPostgreSQL\nprocedural language\ncreateuser\n-- define\na new\nPostgreSQL\nuser\naccount\ndropdb\n-- remove a\nPostgreSQL\ndatabase\ndroplang\n-- remove a\nPostgreSQL\nprocedural\nlanguage\ndropuser\n-- remove a\nPostgreSQL\nuser\naccount\necpg\n-- embedded SQL\nC preprocessor\npg_config\n-- retrieve\ninformation about the installed version of\nPostgreSQL\npg_dump\n--\nextract a\nPostgreSQL\ndatabase into a script file or other archive file\npg_dumpall\n-- extract\na\nPostgreSQL\ndatabase\ncluster into a script file\npg_restore\n--\nrestore a\nPostgreSQL\ndatabase from an archive file created by\npg_dump\npsql\n--\nPostgreSQL\ninteractive\nterminal\nreindexdb\n-- reindex\na\nPostgreSQL\ndatabase\nvacuumdb\n-- garbage-collect\nand analyze a\nPostgreSQL\ndatabase\nIII.\nPostgreSQL Server\nApplications\ninitdb\n-- create a new\nPostgreSQL\ndatabase\ncluster\nipcclean\n-- remove\nshared memory and semaphores from a failed\nPostgreSQL\nserver\npg_controldata\n-- display\ncontrol information of a\nPostgreSQL\ndatabase cluster\npg_ctl\n-- start,\nstop, or restart a\nPostgreSQL\nserver\npg_resetxlog\n-- reset\nthe write-ahead log and other control information of a\nPostgreSQL\ndatabase\ncluster\npostgres\n--\nPostgreSQL\ndatabase server\npostmaster\n--\nPostgreSQL\ndatabase server\nPrev\nHome\nNext\nExamples\nSQL Commands",
    "url": "https://www.postgresql.org/docs/8.3/reference.html",
    "source": "postgresql",
    "doc_type": "reference",
    "scraped_at": 31789.3963347
  },
  {
    "title": "PostgreSQL: Documentation: 17: 2.2. Concepts",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n2.2. Concepts\nPrev\nUp\nChapter 2. The\nSQL\nLanguage\nHome\nNext\n2.2. Concepts\n#\nPostgreSQL\nis a\nrelational database management system\n(\nRDBMS\n). That means it is a system for managing data stored in\nrelations\n. Relation is essentially a mathematical term for\ntable\n. The notion of storing data in tables is so commonplace today that it might seem inherently obvious, but there are a number of other ways of organizing databases. Files and directories on Unix-like operating systems form an example of a hierarchical database. A more modern development is the object-oriented database.\nEach table is a named collection of\nrows\n. Each row of a given table has the same set of named\ncolumns\n, and each column is of a specific data type. Whereas columns have a fixed order in each row, it is important to remember that SQL does not guarantee the order of the rows within the table in any way (although they can be explicitly sorted for display).\nTables are grouped into databases, and a collection of databases managed by a single\nPostgreSQL\nserver instance constitutes a database\ncluster\n.\nPrev\nUp\nNext\n2.1. Introduction\nHome\n2.3. Creating a New Table\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/tutorial-concepts.html",
    "source": "postgresql",
    "doc_type": "tutorial",
    "scraped_at": 31789.5929336
  },
  {
    "title": "PostgreSQL: Documentation: 17: 14.5. Non-Durable Settings",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n14.5. Non-Durable Settings\nPrev\nUp\nChapter 14. Performance Tips\nHome\nNext\n14.5. Non-Durable Settings\n#\nDurability is a database feature that guarantees the recording of committed transactions even if the server crashes or loses power. However, durability adds significant database overhead, so if your site does not require such a guarantee,\nPostgreSQL\ncan be configured to run much faster. The following are configuration changes you can make to improve performance in such cases. Except as noted below, durability is still guaranteed in case of a crash of the database software; only an abrupt operating system crash creates a risk of data loss or corruption when these settings are used.\nPlace the database cluster's data directory in a memory-backed file system (i.e.,\nRAM\ndisk). This eliminates all database disk I/O, but limits data storage to the amount of available memory (and perhaps swap).\nTurn off\nfsync\n; there is no need to flush data to disk.\nTurn off\nsynchronous_commit\n; there might be no need to force\nWAL\nwrites to disk on every commit. This setting does risk transaction loss (though not data corruption) in case of a crash of the\ndatabase\n.\nTurn off\nfull_page_writes\n; there is no need to guard against partial page writes.\nIncrease\nmax_wal_size\nand\ncheckpoint_timeout\n; this reduces the frequency of checkpoints, but increases the storage requirements of\n/pg_wal\n.\nCreate\nunlogged tables\nto avoid\nWAL\nwrites, though it makes the tables non-crash-safe.\nPrev\nUp\nNext\n14.4. Populating a Database\nHome\nChapter 15. Parallel Query\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/non-durability.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31789.7157642
  },
  {
    "title": "PostgreSQL: Documentation: 17: 32.13. Notice Processing",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n32.13. Notice Processing\nPrev\nUp\nChapter 32.\nlibpq\n— C Library\nHome\nNext\n32.13. Notice Processing\n#\nNotice and warning messages generated by the server are not returned by the query execution functions, since they do not imply failure of the query. Instead they are passed to a notice handling function, and execution continues normally after the handler returns. The default notice handling function prints the message on\nstderr\n, but the application can override this behavior by supplying its own handling function.\nFor historical reasons, there are two levels of notice handling, called the notice receiver and notice processor. The default behavior is for the notice receiver to format the notice and pass a string to the notice processor for printing. However, an application that chooses to provide its own notice receiver will typically ignore the notice processor layer and just do all the work in the notice receiver.\nThe function\nPQsetNoticeReceiver\nsets or examines the current notice receiver for a connection object. Similarly,\nPQsetNoticeProcessor\nsets or examines the current notice processor.\ntypedef void (*PQnoticeReceiver) (void *arg, const PGresult *res);\nPQnoticeReceiver\nPQsetNoticeReceiver(PGconn *conn,\nPQnoticeReceiver proc,\nvoid *arg);\ntypedef void (*PQnoticeProcessor) (void *arg, const char *message);\nPQnoticeProcessor\nPQsetNoticeProcessor(PGconn *conn,\nPQnoticeProcessor proc,\nvoid *arg);\nEach of these functions returns the previous notice receiver or processor function pointer, and sets the new value. If you supply a null function pointer, no action is taken, but the current pointer is returned.\nWhen a notice or warning message is received from the server, or generated internally by\nlibpq\n, the notice receiver function is called. It is passed the message in the form of a\nPGRES_NONFATAL_ERROR\nPGresult\n. (This allows the receiver to extract individual fields using\nPQresultErrorField\n, or obtain a complete preformatted message using\nPQresultErrorMessage\nor\nPQresultVerboseErrorMessage\n.) The same void pointer passed to\nPQsetNoticeReceiver\nis also passed. (This pointer can be used to access application-specific state if needed.)\nThe default notice receiver simply extracts the message (using\nPQresultErrorMessage\n) and passes it to the notice processor.\nThe notice processor is responsible for handling a notice or warning message given in text form. It is passed the string text of the message (including a trailing newline), plus a void pointer that is the same one passed to\nPQsetNoticeProcessor\n. (This pointer can be used to access application-specific state if needed.)\nThe default notice processor is simply:\nstatic void\ndefaultNoticeProcessor(void *arg, const char *message)\n{\nfprintf(stderr, \"%s\", message);\n}\nOnce you have set a notice receiver or processor, you should expect that that function could be called as long as either the\nPGconn\nobject or\nPGresult\nobjects made from it exist. At creation of a\nPGresult\n, the\nPGconn\n's current notice handling pointers are copied into the\nPGresult\nfor possible use by functions like\nPQgetvalue\n.\nPrev\nUp\nNext\n32.12. Miscellaneous Functions\nHome\n32.14. Event System\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/libpq-notice-processing.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31789.7612987
  },
  {
    "title": "PostgreSQL: Documentation: 17: CREATE TABLESPACE",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\nCREATE TABLESPACE\nPrev\nUp\nSQL Commands\nHome\nNext\nCREATE TABLESPACE\nCREATE TABLESPACE — define a new tablespace\nSynopsis\nCREATE TABLESPACE\ntablespace_name\n[ OWNER {\nnew_owner\n| CURRENT_ROLE | CURRENT_USER | SESSION_USER } ]\nLOCATION '\ndirectory\n'\n[ WITH (\ntablespace_option\n=\nvalue\n[, ... ] ) ]\nDescription\nCREATE TABLESPACE\nregisters a new cluster-wide tablespace. The tablespace name must be distinct from the name of any existing tablespace in the database cluster.\nA tablespace allows superusers to define an alternative location on the file system where the data files containing database objects (such as tables and indexes) can reside.\nA user with appropriate privileges can pass\ntablespace_name\nto\nCREATE DATABASE\n,\nCREATE TABLE\n,\nCREATE INDEX\nor\nADD CONSTRAINT\nto have the data files for these objects stored within the specified tablespace.\nWarning\nA tablespace cannot be used independently of the cluster in which it is defined; see\nSection 22.6\n.\nParameters\ntablespace_name\nThe name of a tablespace to be created. The name cannot begin with\npg_\n, as such names are reserved for system tablespaces.\nuser_name\nThe name of the user who will own the tablespace. If omitted, defaults to the user executing the command. Only superusers can create tablespaces, but they can assign ownership of tablespaces to non-superusers.\ndirectory\nThe directory that will be used for the tablespace. The directory must exist (\nCREATE TABLESPACE\nwill not create it), should be empty, and must be owned by the\nPostgreSQL\nsystem user. The directory must be specified by an absolute path name.\ntablespace_option\nA tablespace parameter to be set or reset. Currently, the only available parameters are\nseq_page_cost\n,\nrandom_page_cost\n,\neffective_io_concurrency\nand\nmaintenance_io_concurrency\n. Setting these values for a particular tablespace will override the planner's usual estimate of the cost of reading pages from tables in that tablespace, and the executor's prefetching behavior, as established by the configuration parameters of the same name (see\nseq_page_cost\n,\nrandom_page_cost\n,\neffective_io_concurrency\n,\nmaintenance_io_concurrency\n). This may be useful if one tablespace is located on a disk which is faster or slower than the remainder of the I/O subsystem.\nNotes\nCREATE TABLESPACE\ncannot be executed inside a transaction block.\nExamples\nTo create a tablespace\ndbspace\nat file system location\n/data/dbs\n, first create the directory using operating system facilities and set the correct ownership:\nmkdir /data/dbs\nchown postgres:postgres /data/dbs\nThen issue the tablespace creation command inside\nPostgreSQL\n:\nCREATE TABLESPACE dbspace LOCATION '/data/dbs';\nTo create a tablespace owned by a different database user, use a command like this:\nCREATE TABLESPACE indexspace OWNER genevieve LOCATION '/data/indexes';\nCompatibility\nCREATE TABLESPACE\nis a\nPostgreSQL\nextension.\nSee Also\nCREATE DATABASE\n,\nCREATE TABLE\n,\nCREATE INDEX\n,\nDROP TABLESPACE\n,\nALTER TABLESPACE\nPrev\nUp\nNext\nCREATE TABLE AS\nHome\nCREATE TEXT SEARCH CONFIGURATION\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/sql-createtablespace.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31789.8606172
  },
  {
    "title": "PostgreSQL: Documentation: 17: 19.7. Query Planning",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n19.7. Query Planning\nPrev\nUp\nChapter 19. Server Configuration\nHome\nNext\n19.7. Query Planning\n#\n19.7.1. Planner Method Configuration\n19.7.2. Planner Cost Constants\n19.7.3. Genetic Query Optimizer\n19.7.4. Other Planner Options\n19.7.1. Planner Method Configuration\n#\nThese configuration parameters provide a crude method of influencing the query plans chosen by the query optimizer. If the default plan chosen by the optimizer for a particular query is not optimal, a\ntemporary\nsolution is to use one of these configuration parameters to force the optimizer to choose a different plan. Better ways to improve the quality of the plans chosen by the optimizer include adjusting the planner cost constants (see\nSection 19.7.2\n), running\nANALYZE\nmanually, increasing the value of the\ndefault_statistics_target\nconfiguration parameter, and increasing the amount of statistics collected for specific columns using\nALTER TABLE SET STATISTICS\n.\nenable_async_append\n(\nboolean\n)\n#\nEnables or disables the query planner's use of async-aware append plan types. The default is\non\n.\nenable_bitmapscan\n(\nboolean\n)\n#\nEnables or disables the query planner's use of bitmap-scan plan types. The default is\non\n.\nenable_gathermerge\n(\nboolean\n)\n#\nEnables or disables the query planner's use of gather merge plan types. The default is\non\n.\nenable_group_by_reordering\n(\nboolean\n)\n#\nControls if the query planner will produce a plan which will provide\nGROUP BY\nkeys sorted in the order of keys of a child node of the plan, such as an index scan. When disabled, the query planner will produce a plan with\nGROUP BY\nkeys only sorted to match the\nORDER BY\nclause, if any. When enabled, the planner will try to produce a more efficient plan. The default value is\non\n.\nenable_hashagg\n(\nboolean\n)\n#\nEnables or disables the query planner's use of hashed aggregation plan types. The default is\non\n.\nenable_hashjoin\n(\nboolean\n)\n#\nEnables or disables the query planner's use of hash-join plan types. The default is\non\n.\nenable_incremental_sort\n(\nboolean\n)\n#\nEnables or disables the query planner's use of incremental sort steps. The default is\non\n.\nenable_indexscan\n(\nboolean\n)\n#\nEnables or disables the query planner's use of index-scan and index-only-scan plan types. The default is\non\n. Also see\nenable_indexonlyscan\n.\nenable_indexonlyscan\n(\nboolean\n)\n#\nEnables or disables the query planner's use of index-only-scan plan types (see\nSection 11.9\n). The default is\non\n. The\nenable_indexscan\nsetting must also be enabled to have the query planner consider index-only-scans.\nenable_material\n(\nboolean\n)\n#\nEnables or disables the query planner's use of materialization. It is impossible to suppress materialization entirely, but turning this variable off prevents the planner from inserting materialize nodes except in cases where it is required for correctness. The default is\non\n.\nenable_memoize\n(\nboolean\n)\n#\nEnables or disables the query planner's use of memoize plans for caching results from parameterized scans inside nested-loop joins. This plan type allows scans to the underlying plans to be skipped when the results for the current parameters are already in the cache. Less commonly looked up results may be evicted from the cache when more space is required for new entries. The default is\non\n.\nenable_mergejoin\n(\nboolean\n)\n#\nEnables or disables the query planner's use of merge-join plan types. The default is\non\n.\nenable_nestloop\n(\nboolean\n)\n#\nEnables or disables the query planner's use of nested-loop join plans. It is impossible to suppress nested-loop joins entirely, but turning this variable off discourages the planner from using one if there are other methods available. The default is\non\n.\nenable_parallel_append\n(\nboolean\n)\n#\nEnables or disables the query planner's use of parallel-aware append plan types. The default is\non\n.\nenable_parallel_hash\n(\nboolean\n)\n#\nEnables or disables the query planner's use of hash-join plan types with parallel hash. Has no effect if hash-join plans are not also enabled. The default is\non\n.\nenable_partition_pruning\n(\nboolean\n)\n#\nEnables or disables the query planner's ability to eliminate a partitioned table's partitions from query plans. This also controls the planner's ability to generate query plans which allow the query executor to remove (ignore) partitions during query execution. The default is\non\n. See\nSection 5.12.4\nfor details.\nenable_partitionwise_join\n(\nboolean\n)\n#\nEnables or disables the query planner's use of partitionwise join, which allows a join between partitioned tables to be performed by joining the matching partitions. Partitionwise join currently applies only when the join conditions include all the partition keys, which must be of the same data type and have one-to-one matching sets of child partitions. With this setting enabled, the number of nodes whose memory usage is restricted by\nwork_mem\nappearing in the final plan can increase linearly according to the number of partitions being scanned. This can result in a large increase in overall memory consumption during the execution of the query. Query planning also becomes significantly more expensive in terms of memory and CPU. The default value is\noff\n.\nenable_partitionwise_aggregate\n(\nboolean\n)\n#\nEnables or disables the query planner's use of partitionwise grouping or aggregation, which allows grouping or aggregation on partitioned tables to be performed separately for each partition. If the\nGROUP BY\nclause does not include the partition keys, only partial aggregation can be performed on a per-partition basis, and finalization must be performed later. With this setting enabled, the number of nodes whose memory usage is restricted by\nwork_mem\nappearing in the final plan can increase linearly according to the number of partitions being scanned. This can result in a large increase in overall memory consumption during the execution of the query. Query planning also becomes significantly more expensive in terms of memory and CPU. The default value is\noff\n.\nenable_presorted_aggregate\n(\nboolean\n)\n#\nControls if the query planner will produce a plan which will provide rows which are presorted in the order required for the query's\nORDER BY\n/\nDISTINCT\naggregate functions. When disabled, the query planner will produce a plan which will always require the executor to perform a sort before performing aggregation of each aggregate function containing an\nORDER BY\nor\nDISTINCT\nclause. When enabled, the planner will try to produce a more efficient plan which provides input to the aggregate functions which is presorted in the order they require for aggregation. The default value is\non\n.\nenable_seqscan\n(\nboolean\n)\n#\nEnables or disables the query planner's use of sequential scan plan types. It is impossible to suppress sequential scans entirely, but turning this variable off discourages the planner from using one if there are other methods available. The default is\non\n.\nenable_sort\n(\nboolean\n)\n#\nEnables or disables the query planner's use of explicit sort steps. It is impossible to suppress explicit sorts entirely, but turning this variable off discourages the planner from using one if there are other methods available. The default is\non\n.\nenable_tidscan\n(\nboolean\n)\n#\nEnables or disables the query planner's use of\nTID\nscan plan types. The default is\non\n.\n19.7.2. Planner Cost Constants\n#\nThe\ncost\nvariables described in this section are measured on an arbitrary scale. Only their relative values matter, hence scaling them all up or down by the same factor will result in no change in the planner's choices. By default, these cost variables are based on the cost of sequential page fetches; that is,\nseq_page_cost\nis conventionally set to\n1.0\nand the other cost variables are set with reference to that. But you can use a different scale if you prefer, such as actual execution times in milliseconds on a particular machine.\nNote\nUnfortunately, there is no well-defined method for determining ideal values for the cost variables. They are best treated as averages over the entire mix of queries that a particular installation will receive. This means that changing them on the basis of just a few experiments is very risky.\nseq_page_cost\n(\nfloating point\n)\n#\nSets the planner's estimate of the cost of a disk page fetch that is part of a series of sequential fetches. The default is 1.0. This value can be overridden for tables and indexes in a particular tablespace by setting the tablespace parameter of the same name (see\nALTER TABLESPACE\n).\nrandom_page_cost\n(\nfloating point\n)\n#\nSets the planner's estimate of the cost of a non-sequentially-fetched disk page. The default is 4.0. This value can be overridden for tables and indexes in a particular tablespace by setting the tablespace parameter of the same name (see\nALTER TABLESPACE\n).\nReducing this value relative to\nseq_page_cost\nwill cause the system to prefer index scans; raising it will make index scans look relatively more expensive. You can raise or lower both values together to change the importance of disk I/O costs relative to CPU costs, which are described by the following parameters.\nRandom access to mechanical disk storage is normally much more expensive than four times sequential access. However, a lower default is used (4.0) because the majority of random accesses to disk, such as indexed reads, are assumed to be in cache. The default value can be thought of as modeling random access as 40 times slower than sequential, while expecting 90% of random reads to be cached.\nIf you believe a 90% cache rate is an incorrect assumption for your workload, you can increase random_page_cost to better reflect the true cost of random storage reads. Correspondingly, if your data is likely to be completely in cache, such as when the database is smaller than the total server memory, decreasing random_page_cost can be appropriate. Storage that has a low random read cost relative to sequential, e.g., solid-state drives, might also be better modeled with a lower value for random_page_cost, e.g.,\n1.1\n.\nTip\nAlthough the system will let you set\nrandom_page_cost\nto less than\nseq_page_cost\n, it is not physically sensible to do so. However, setting them equal makes sense if the database is entirely cached in RAM, since in that case there is no penalty for touching pages out of sequence. Also, in a heavily-cached database you should lower both values relative to the CPU parameters, since the cost of fetching a page already in RAM is much smaller than it would normally be.\ncpu_tuple_cost\n(\nfloating point\n)\n#\nSets the planner's estimate of the cost of processing each row during a query. The default is 0.01.\ncpu_index_tuple_cost\n(\nfloating point\n)\n#\nSets the planner's estimate of the cost of processing each index entry during an index scan. The default is 0.005.\ncpu_operator_cost\n(\nfloating point\n)\n#\nSets the planner's estimate of the cost of processing each operator or function executed during a query. The default is 0.0025.\nparallel_setup_cost\n(\nfloating point\n)\n#\nSets the planner's estimate of the cost of launching parallel worker processes. The default is 1000.\nparallel_tuple_cost\n(\nfloating point\n)\n#\nSets the planner's estimate of the cost of transferring one tuple from a parallel worker process to another process. The default is 0.1.\nmin_parallel_table_scan_size\n(\ninteger\n)\n#\nSets the minimum amount of table data that must be scanned in order for a parallel scan to be considered. For a parallel sequential scan, the amount of table data scanned is always equal to the size of the table, but when indexes are used the amount of table data scanned will normally be less. If this value is specified without units, it is taken as blocks, that is\nBLCKSZ\nbytes, typically 8kB. The default is 8 megabytes (\n8MB\n).\nmin_parallel_index_scan_size\n(\ninteger\n)\n#\nSets the minimum amount of index data that must be scanned in order for a parallel scan to be considered. Note that a parallel index scan typically won't touch the entire index; it is the number of pages which the planner believes will actually be touched by the scan which is relevant. This parameter is also used to decide whether a particular index can participate in a parallel vacuum. See\nVACUUM\n. If this value is specified without units, it is taken as blocks, that is\nBLCKSZ\nbytes, typically 8kB. The default is 512 kilobytes (\n512kB\n).\neffective_cache_size\n(\ninteger\n)\n#\nSets the planner's assumption about the effective size of the disk cache that is available to a single query. This is factored into estimates of the cost of using an index; a higher value makes it more likely index scans will be used, a lower value makes it more likely sequential scans will be used. When setting this parameter you should consider both\nPostgreSQL\n's shared buffers and the portion of the kernel's disk cache that will be used for\nPostgreSQL\ndata files, though some data might exist in both places. Also, take into account the expected number of concurrent queries on different tables, since they will have to share the available space. This parameter has no effect on the size of shared memory allocated by\nPostgreSQL\n, nor does it reserve kernel disk cache; it is used only for estimation purposes. The system also does not assume data remains in the disk cache between queries. If this value is specified without units, it is taken as blocks, that is\nBLCKSZ\nbytes, typically 8kB. The default is 4 gigabytes (\n4GB\n). (If\nBLCKSZ\nis not 8kB, the default value scales proportionally to it.)\njit_above_cost\n(\nfloating point\n)\n#\nSets the query cost above which JIT compilation is activated, if enabled (see\nChapter 30\n). Performing\nJIT\ncosts planning time but can accelerate query execution. Setting this to\n-1\ndisables JIT compilation. The default is\n100000\n.\njit_inline_above_cost\n(\nfloating point\n)\n#\nSets the query cost above which JIT compilation attempts to inline functions and operators. Inlining adds planning time, but can improve execution speed. It is not meaningful to set this to less than\njit_above_cost\n. Setting this to\n-1\ndisables inlining. The default is\n500000\n.\njit_optimize_above_cost\n(\nfloating point\n)\n#\nSets the query cost above which JIT compilation applies expensive optimizations. Such optimization adds planning time, but can improve execution speed. It is not meaningful to set this to less than\njit_above_cost\n, and it is unlikely to be beneficial to set it to more than\njit_inline_above_cost\n. Setting this to\n-1\ndisables expensive optimizations. The default is\n500000\n.\n19.7.3. Genetic Query Optimizer\n#\nThe genetic query optimizer (GEQO) is an algorithm that does query planning using heuristic searching. This reduces planning time for complex queries (those joining many relations), at the cost of producing plans that are sometimes inferior to those found by the normal exhaustive-search algorithm. For more information see\nChapter 60\n.\ngeqo\n(\nboolean\n)\n#\nEnables or disables genetic query optimization. This is on by default. It is usually best not to turn it off in production; the\ngeqo_threshold\nvariable provides more granular control of GEQO.\ngeqo_threshold\n(\ninteger\n)\n#\nUse genetic query optimization to plan queries with at least this many\nFROM\nitems involved. (Note that a\nFULL OUTER JOIN\nconstruct counts as only one\nFROM\nitem.) The default is 12. For simpler queries it is usually best to use the regular, exhaustive-search planner, but for queries with many tables the exhaustive search takes too long, often longer than the penalty of executing a suboptimal plan. Thus, a threshold on the size of the query is a convenient way to manage use of GEQO.\ngeqo_effort\n(\ninteger\n)\n#\nControls the trade-off between planning time and query plan quality in GEQO. This variable must be an integer in the range from 1 to 10. The default value is five. Larger values increase the time spent doing query planning, but also increase the likelihood that an efficient query plan will be chosen.\ngeqo_effort\ndoesn't actually do anything directly; it is only used to compute the default values for the other variables that influence GEQO behavior (described below). If you prefer, you can set the other parameters by hand instead.\ngeqo_pool_size\n(\ninteger\n)\n#\nControls the pool size used by GEQO, that is the number of individuals in the genetic population. It must be at least two, and useful values are typically 100 to 1000. If it is set to zero (the default setting) then a suitable value is chosen based on\ngeqo_effort\nand the number of tables in the query.\ngeqo_generations\n(\ninteger\n)\n#\nControls the number of generations used by GEQO, that is the number of iterations of the algorithm. It must be at least one, and useful values are in the same range as the pool size. If it is set to zero (the default setting) then a suitable value is chosen based on\ngeqo_pool_size\n.\ngeqo_selection_bias\n(\nfloating point\n)\n#\nControls the selection bias used by GEQO. The selection bias is the selective pressure within the population. Values can be from 1.50 to 2.00; the latter is the default.\ngeqo_seed\n(\nfloating point\n)\n#\nControls the initial value of the random number generator used by GEQO to select random paths through the join order search space. The value can range from zero (the default) to one. Varying the value changes the set of join paths explored, and may result in a better or worse best path being found.\n19.7.4. Other Planner Options\n#\ndefault_statistics_target\n(\ninteger\n)\n#\nSets the default statistics target for table columns without a column-specific target set via\nALTER TABLE SET STATISTICS\n. Larger values increase the time needed to do\nANALYZE\n, but might improve the quality of the planner's estimates. The default is 100. For more information on the use of statistics by the\nPostgreSQL\nquery planner, refer to\nSection 14.2\n.\nconstraint_exclusion\n(\nenum\n)\n#\nControls the query planner's use of table constraints to optimize queries. The allowed values of\nconstraint_exclusion\nare\non\n(examine constraints for all tables),\noff\n(never examine constraints), and\npartition\n(examine constraints only for inheritance child tables and\nUNION ALL\nsubqueries).\npartition\nis the default setting. It is often used with traditional inheritance trees to improve performance.\nWhen this parameter allows it for a particular table, the planner compares query conditions with the table's\nCHECK\nconstraints, and omits scanning tables for which the conditions contradict the constraints. For example:\nCREATE TABLE parent(key integer, ...);\nCREATE TABLE child1000(check (key between 1000 and 1999)) INHERITS(parent);\nCREATE TABLE child2000(check (key between 2000 and 2999)) INHERITS(parent);\n...\nSELECT * FROM parent WHERE key = 2400;\nWith constraint exclusion enabled, this\nSELECT\nwill not scan\nchild1000\nat all, improving performance.\nCurrently, constraint exclusion is enabled by default only for cases that are often used to implement table partitioning via inheritance trees. Turning it on for all tables imposes extra planning overhead that is quite noticeable on simple queries, and most often will yield no benefit for simple queries. If you have no tables that are partitioned using traditional inheritance, you might prefer to turn it off entirely. (Note that the equivalent feature for partitioned tables is controlled by a separate parameter,\nenable_partition_pruning\n.)\nRefer to\nSection 5.12.5\nfor more information on using constraint exclusion to implement partitioning.\ncursor_tuple_fraction\n(\nfloating point\n)\n#\nSets the planner's estimate of the fraction of a cursor's rows that will be retrieved. The default is 0.1. Smaller values of this setting bias the planner towards using\n“\nfast start\n”\nplans for cursors, which will retrieve the first few rows quickly while perhaps taking a long time to fetch all rows. Larger values put more emphasis on the total estimated time. At the maximum setting of 1.0, cursors are planned exactly like regular queries, considering only the total estimated time and not how soon the first rows might be delivered.\nfrom_collapse_limit\n(\ninteger\n)\n#\nThe planner will merge sub-queries into upper queries if the resulting\nFROM\nlist would have no more than this many items. Smaller values reduce planning time but might yield inferior query plans. The default is eight. For more information see\nSection 14.3\n.\nSetting this value to\ngeqo_threshold\nor more may trigger use of the GEQO planner, resulting in non-optimal plans. See\nSection 19.7.3\n.\njit\n(\nboolean\n)\n#\nDetermines whether\nJIT\ncompilation may be used by\nPostgreSQL\n, if available (see\nChapter 30\n). The default is\non\n.\njoin_collapse_limit\n(\ninteger\n)\n#\nThe planner will rewrite explicit\nJOIN\nconstructs (except\nFULL JOIN\ns) into lists of\nFROM\nitems whenever a list of no more than this many items would result. Smaller values reduce planning time but might yield inferior query plans.\nBy default, this variable is set the same as\nfrom_collapse_limit\n, which is appropriate for most uses. Setting it to 1 prevents any reordering of explicit\nJOIN\ns. Thus, the explicit join order specified in the query will be the actual order in which the relations are joined. Because the query planner does not always choose the optimal join order, advanced users can elect to temporarily set this variable to 1, and then specify the join order they desire explicitly. For more information see\nSection 14.3\n.\nSetting this value to\ngeqo_threshold\nor more may trigger use of the GEQO planner, resulting in non-optimal plans. See\nSection 19.7.3\n.\nplan_cache_mode\n(\nenum\n)\n#\nPrepared statements (either explicitly prepared or implicitly generated, for example by PL/pgSQL) can be executed using custom or generic plans. Custom plans are made afresh for each execution using its specific set of parameter values, while generic plans do not rely on the parameter values and can be re-used across executions. Thus, use of a generic plan saves planning time, but if the ideal plan depends strongly on the parameter values then a generic plan may be inefficient. The choice between these options is normally made automatically, but it can be overridden with\nplan_cache_mode\n. The allowed values are\nauto\n(the default),\nforce_custom_plan\nand\nforce_generic_plan\n. This setting is considered when a cached plan is to be executed, not when it is prepared. For more information see\nPREPARE\n.\nrecursive_worktable_factor\n(\nfloating point\n)\n#\nSets the planner's estimate of the average size of the working table of a\nrecursive query\n, as a multiple of the estimated size of the initial non-recursive term of the query. This helps the planner choose the most appropriate method for joining the working table to the query's other tables. The default value is\n10.0\n. A smaller value such as\n1.0\ncan be helpful when the recursion has low\n“\nfan-out\n”\nfrom one step to the next, as for example in shortest-path queries. Graph analytics queries may benefit from larger-than-default values.\nPrev\nUp\nNext\n19.6. Replication\nHome\n19.8. Error Reporting and Logging\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/runtime-config-query.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31790.0371164
  },
  {
    "title": "PostgreSQL: Documentation: 17: 2.9. Deletions",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n2.9. Deletions\nPrev\nUp\nChapter 2. The\nSQL\nLanguage\nHome\nNext\n2.9. Deletions\n#\nRows can be removed from a table using the\nDELETE\ncommand. Suppose you are no longer interested in the weather of Hayward. Then you can do the following to delete those rows from the table:\nDELETE FROM weather WHERE city = 'Hayward';\nAll weather records belonging to Hayward are removed.\nSELECT * FROM weather;\ncity      | temp_lo | temp_hi | prcp |    date\n---------------+---------+---------+------+------------\nSan Francisco |      46 |      50 | 0.25 | 1994-11-27\nSan Francisco |      41 |      55 |    0 | 1994-11-29\n(2 rows)\nOne should be wary of statements of the form\nDELETE FROM\ntablename\n;\nWithout a qualification,\nDELETE\nwill remove\nall\nrows from the given table, leaving it empty. The system will not request confirmation before doing this!\nPrev\nUp\nNext\n2.8. Updates\nHome\nChapter 3. Advanced Features\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/tutorial-delete.html",
    "source": "postgresql",
    "doc_type": "tutorial",
    "scraped_at": 31790.1464187
  },
  {
    "title": "SQL Commands",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nSQL Commands\nPrev\nUp\nPart VI. Reference\nHome\nNext\nSQL Commands\nThis part contains reference information for the\nSQL\ncommands supported by\nPostgreSQL\n. By\n“\nSQL\n”\nthe language in general is meant; information about the standards conformance and compatibility of each command can be found on the respective reference page.\nTable of Contents\nABORT\n— abort the current transaction\nALTER AGGREGATE\n— change the definition of an aggregate function\nALTER COLLATION\n— change the definition of a collation\nALTER CONVERSION\n— change the definition of a conversion\nALTER DATABASE\n— change a database\nALTER DEFAULT PRIVILEGES\n— define default access privileges\nALTER DOMAIN\n— change the definition of a domain\nALTER EVENT TRIGGER\n— change the definition of an event trigger\nALTER EXTENSION\n— change the definition of an extension\nALTER FOREIGN DATA WRAPPER\n— change the definition of a foreign-data wrapper\nALTER FOREIGN TABLE\n— change the definition of a foreign table\nALTER FUNCTION\n— change the definition of a function\nALTER GROUP\n— change role name or membership\nALTER INDEX\n— change the definition of an index\nALTER LANGUAGE\n— change the definition of a procedural language\nALTER LARGE OBJECT\n— change the definition of a large object\nALTER MATERIALIZED VIEW\n— change the definition of a materialized view\nALTER OPERATOR\n— change the definition of an operator\nALTER OPERATOR CLASS\n— change the definition of an operator class\nALTER OPERATOR FAMILY\n— change the definition of an operator family\nALTER POLICY\n— change the definition of a row-level security policy\nALTER PROCEDURE\n— change the definition of a procedure\nALTER PUBLICATION\n— change the definition of a publication\nALTER ROLE\n— change a database role\nALTER ROUTINE\n— change the definition of a routine\nALTER RULE\n— change the definition of a rule\nALTER SCHEMA\n— change the definition of a schema\nALTER SEQUENCE\n— change the definition of a sequence generator\nALTER SERVER\n— change the definition of a foreign server\nALTER STATISTICS\n— change the definition of an extended statistics object\nALTER SUBSCRIPTION\n— change the definition of a subscription\nALTER SYSTEM\n— change a server configuration parameter\nALTER TABLE\n— change the definition of a table\nALTER TABLESPACE\n— change the definition of a tablespace\nALTER TEXT SEARCH CONFIGURATION\n— change the definition of a text search configuration\nALTER TEXT SEARCH DICTIONARY\n— change the definition of a text search dictionary\nALTER TEXT SEARCH PARSER\n— change the definition of a text search parser\nALTER TEXT SEARCH TEMPLATE\n— change the definition of a text search template\nALTER TRIGGER\n— change the definition of a trigger\nALTER TYPE\n— change the definition of a type\nALTER USER\n— change a database role\nALTER USER MAPPING\n— change the definition of a user mapping\nALTER VIEW\n— change the definition of a view\nANALYZE\n— collect statistics about a database\nBEGIN\n— start a transaction block\nCALL\n— invoke a procedure\nCHECKPOINT\n— force a write-ahead log checkpoint\nCLOSE\n— close a cursor\nCLUSTER\n— cluster a table according to an index\nCOMMENT\n— define or change the comment of an object\nCOMMIT\n— commit the current transaction\nCOMMIT PREPARED\n— commit a transaction that was earlier prepared for two-phase commit\nCOPY\n— copy data between a file and a table\nCREATE ACCESS METHOD\n— define a new access method\nCREATE AGGREGATE\n— define a new aggregate function\nCREATE CAST\n— define a new cast\nCREATE COLLATION\n— define a new collation\nCREATE CONVERSION\n— define a new encoding conversion\nCREATE DATABASE\n— create a new database\nCREATE DOMAIN\n— define a new domain\nCREATE EVENT TRIGGER\n— define a new event trigger\nCREATE EXTENSION\n— install an extension\nCREATE FOREIGN DATA WRAPPER\n— define a new foreign-data wrapper\nCREATE FOREIGN TABLE\n— define a new foreign table\nCREATE FUNCTION\n— define a new function\nCREATE GROUP\n— define a new database role\nCREATE INDEX\n— define a new index\nCREATE LANGUAGE\n— define a new procedural language\nCREATE MATERIALIZED VIEW\n— define a new materialized view\nCREATE OPERATOR\n— define a new operator\nCREATE OPERATOR CLASS\n— define a new operator class\nCREATE OPERATOR FAMILY\n— define a new operator family\nCREATE POLICY\n— define a new row-level security policy for a table\nCREATE PROCEDURE\n— define a new procedure\nCREATE PUBLICATION\n— define a new publication\nCREATE ROLE\n— define a new database role\nCREATE RULE\n— define a new rewrite rule\nCREATE SCHEMA\n— define a new schema\nCREATE SEQUENCE\n— define a new sequence generator\nCREATE SERVER\n— define a new foreign server\nCREATE STATISTICS\n— define extended statistics\nCREATE SUBSCRIPTION\n— define a new subscription\nCREATE TABLE\n— define a new table\nCREATE TABLE AS\n— define a new table from the results of a query\nCREATE TABLESPACE\n— define a new tablespace\nCREATE TEXT SEARCH CONFIGURATION\n— define a new text search configuration\nCREATE TEXT SEARCH DICTIONARY\n— define a new text search dictionary\nCREATE TEXT SEARCH PARSER\n— define a new text search parser\nCREATE TEXT SEARCH TEMPLATE\n— define a new text search template\nCREATE TRANSFORM\n— define a new transform\nCREATE TRIGGER\n— define a new trigger\nCREATE TYPE\n— define a new data type\nCREATE USER\n— define a new database role\nCREATE USER MAPPING\n— define a new mapping of a user to a foreign server\nCREATE VIEW\n— define a new view\nDEALLOCATE\n— deallocate a prepared statement\nDECLARE\n— define a cursor\nDELETE\n— delete rows of a table\nDISCARD\n— discard session state\nDO\n— execute an anonymous code block\nDROP ACCESS METHOD\n— remove an access method\nDROP AGGREGATE\n— remove an aggregate function\nDROP CAST\n— remove a cast\nDROP COLLATION\n— remove a collation\nDROP CONVERSION\n— remove a conversion\nDROP DATABASE\n— remove a database\nDROP DOMAIN\n— remove a domain\nDROP EVENT TRIGGER\n— remove an event trigger\nDROP EXTENSION\n— remove an extension\nDROP FOREIGN DATA WRAPPER\n— remove a foreign-data wrapper\nDROP FOREIGN TABLE\n— remove a foreign table\nDROP FUNCTION\n— remove a function\nDROP GROUP\n— remove a database role\nDROP INDEX\n— remove an index\nDROP LANGUAGE\n— remove a procedural language\nDROP MATERIALIZED VIEW\n— remove a materialized view\nDROP OPERATOR\n— remove an operator\nDROP OPERATOR CLASS\n— remove an operator class\nDROP OPERATOR FAMILY\n— remove an operator family\nDROP OWNED\n— remove database objects owned by a database role\nDROP POLICY\n— remove a row-level security policy from a table\nDROP PROCEDURE\n— remove a procedure\nDROP PUBLICATION\n— remove a publication\nDROP ROLE\n— remove a database role\nDROP ROUTINE\n— remove a routine\nDROP RULE\n— remove a rewrite rule\nDROP SCHEMA\n— remove a schema\nDROP SEQUENCE\n— remove a sequence\nDROP SERVER\n— remove a foreign server descriptor\nDROP STATISTICS\n— remove extended statistics\nDROP SUBSCRIPTION\n— remove a subscription\nDROP TABLE\n— remove a table\nDROP TABLESPACE\n— remove a tablespace\nDROP TEXT SEARCH CONFIGURATION\n— remove a text search configuration\nDROP TEXT SEARCH DICTIONARY\n— remove a text search dictionary\nDROP TEXT SEARCH PARSER\n— remove a text search parser\nDROP TEXT SEARCH TEMPLATE\n— remove a text search template\nDROP TRANSFORM\n— remove a transform\nDROP TRIGGER\n— remove a trigger\nDROP TYPE\n— remove a data type\nDROP USER\n— remove a database role\nDROP USER MAPPING\n— remove a user mapping for a foreign server\nDROP VIEW\n— remove a view\nEND\n— commit the current transaction\nEXECUTE\n— execute a prepared statement\nEXPLAIN\n— show the execution plan of a statement\nFETCH\n— retrieve rows from a query using a cursor\nGRANT\n— define access privileges\nIMPORT FOREIGN SCHEMA\n— import table definitions from a foreign server\nINSERT\n— create new rows in a table\nLISTEN\n— listen for a notification\nLOAD\n— load a shared library file\nLOCK\n— lock a table\nMERGE\n— conditionally insert, update, or delete rows of a table\nMOVE\n— position a cursor\nNOTIFY\n— generate a notification\nPREPARE\n— prepare a statement for execution\nPREPARE TRANSACTION\n— prepare the current transaction for two-phase commit\nREASSIGN OWNED\n— change the ownership of database objects owned by a database role\nREFRESH MATERIALIZED VIEW\n— replace the contents of a materialized view\nREINDEX\n— rebuild indexes\nRELEASE SAVEPOINT\n— release a previously defined savepoint\nRESET\n— restore the value of a run-time parameter to the default value\nREVOKE\n— remove access privileges\nROLLBACK\n— abort the current transaction\nROLLBACK PREPARED\n— cancel a transaction that was earlier prepared for two-phase commit\nROLLBACK TO SAVEPOINT\n— roll back to a savepoint\nSAVEPOINT\n— define a new savepoint within the current transaction\nSECURITY LABEL\n— define or change a security label applied to an object\nSELECT\n— retrieve rows from a table or view\nSELECT INTO\n— define a new table from the results of a query\nSET\n— change a run-time parameter\nSET CONSTRAINTS\n— set constraint check timing for the current transaction\nSET ROLE\n— set the current user identifier of the current session\nSET SESSION AUTHORIZATION\n— set the session user identifier and the current user identifier of the current session\nSET TRANSACTION\n— set the characteristics of the current transaction\nSHOW\n— show the value of a run-time parameter\nSTART TRANSACTION\n— start a transaction block\nTRUNCATE\n— empty a table or set of tables\nUNLISTEN\n— stop listening for a notification\nUPDATE\n— update rows of a table\nVACUUM\n— garbage-collect and optionally analyze a database\nVALUES\n— compute a set of rows\nPrev\nUp\nNext\nPart VI. Reference\nHome\nABORT\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/sql-commands.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31790.3984893
  },
  {
    "title": "PostgreSQL: Documentation: 17: 35.15. column_privileges",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n35.15.\ncolumn_privileges\nPrev\nUp\nChapter 35. The Information Schema\nHome\nNext\n35.15.\ncolumn_privileges\n#\nThe view\ncolumn_privileges\nidentifies all privileges granted on columns to a currently enabled role or by a currently enabled role. There is one row for each combination of column, grantor, and grantee.\nIf a privilege has been granted on an entire table, it will show up in this view as a grant for each column, but only for the privilege types where column granularity is possible:\nSELECT\n,\nINSERT\n,\nUPDATE\n,\nREFERENCES\n.\nTable 35.13.\ncolumn_privileges\nColumns\nColumn Type\nDescription\ngrantor\nsql_identifier\nName of the role that granted the privilege\ngrantee\nsql_identifier\nName of the role that the privilege was granted to\ntable_catalog\nsql_identifier\nName of the database that contains the table that contains the column (always the current database)\ntable_schema\nsql_identifier\nName of the schema that contains the table that contains the column\ntable_name\nsql_identifier\nName of the table that contains the column\ncolumn_name\nsql_identifier\nName of the column\nprivilege_type\ncharacter_data\nType of the privilege:\nSELECT\n,\nINSERT\n,\nUPDATE\n, or\nREFERENCES\nis_grantable\nyes_or_no\nYES\nif the privilege is grantable,\nNO\nif not\nPrev\nUp\nNext\n35.14.\ncolumn_options\nHome\n35.16.\ncolumn_udt_usage\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/infoschema-column-privileges.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31790.5017543
  },
  {
    "title": "PostgreSQL: Documentation: 17: 12.7. Configuration Example",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n12.7. Configuration Example\nPrev\nUp\nChapter 12. Full Text Search\nHome\nNext\n12.7. Configuration Example\n#\nA text search configuration specifies all options necessary to transform a document into a\ntsvector\n: the parser to use to break text into tokens, and the dictionaries to use to transform each token into a lexeme. Every call of\nto_tsvector\nor\nto_tsquery\nneeds a text search configuration to perform its processing. The configuration parameter\ndefault_text_search_config\nspecifies the name of the default configuration, which is the one used by text search functions if an explicit configuration parameter is omitted. It can be set in\npostgresql.conf\n, or set for an individual session using the\nSET\ncommand.\nSeveral predefined text search configurations are available, and you can create custom configurations easily. To facilitate management of text search objects, a set of\nSQL\ncommands is available, and there are several\npsql\ncommands that display information about text search objects (\nSection 12.10\n).\nAs an example we will create a configuration\npg\n, starting by duplicating the built-in\nenglish\nconfiguration:\nCREATE TEXT SEARCH CONFIGURATION public.pg ( COPY = pg_catalog.english );\nWe will use a PostgreSQL-specific synonym list and store it in\n$SHAREDIR/tsearch_data/pg_dict.syn\n. The file contents look like:\npostgres    pg\npgsql       pg\npostgresql  pg\nWe define the synonym dictionary like this:\nCREATE TEXT SEARCH DICTIONARY pg_dict (\nTEMPLATE = synonym,\nSYNONYMS = pg_dict\n);\nNext we register the\nIspell\ndictionary\nenglish_ispell\n, which has its own configuration files:\nCREATE TEXT SEARCH DICTIONARY english_ispell (\nTEMPLATE = ispell,\nDictFile = english,\nAffFile = english,\nStopWords = english\n);\nNow we can set up the mappings for words in configuration\npg\n:\nALTER TEXT SEARCH CONFIGURATION pg\nALTER MAPPING FOR asciiword, asciihword, hword_asciipart,\nword, hword, hword_part\nWITH pg_dict, english_ispell, english_stem;\nWe choose not to index or search some token types that the built-in configuration does handle:\nALTER TEXT SEARCH CONFIGURATION pg\nDROP MAPPING FOR email, url, url_path, sfloat, float;\nNow we can test our configuration:\nSELECT * FROM ts_debug('public.pg', '\nPostgreSQL, the highly scalable, SQL compliant, open source object-relational\ndatabase management system, is now undergoing beta testing of the next\nversion of our software.\n');\nThe next step is to set the session to use the new configuration, which was created in the\npublic\nschema:\n=> \\dF\nList of text search configurations\nSchema  | Name | Description\n---------+------+-------------\npublic  | pg   |\nSET default_text_search_config = 'public.pg';\nSET\nSHOW default_text_search_config;\ndefault_text_search_config\n----------------------------\npublic.pg\nPrev\nUp\nNext\n12.6. Dictionaries\nHome\n12.8. Testing and Debugging Text Search\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/textsearch-configuration.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31790.5459183
  },
  {
    "title": "PostgreSQL: Documentation: 17: 21.3. Role Membership",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n21.3. Role Membership\nPrev\nUp\nChapter 21. Database Roles\nHome\nNext\n21.3. Role Membership\n#\nIt is frequently convenient to group users together to ease management of privileges: that way, privileges can be granted to, or revoked from, a group as a whole. In\nPostgreSQL\nthis is done by creating a role that represents the group, and then granting\nmembership\nin the group role to individual user roles.\nTo set up a group role, first create the role:\nCREATE ROLE\nname\n;\nTypically a role being used as a group would not have the\nLOGIN\nattribute, though you can set it if you wish.\nOnce the group role exists, you can add and remove members using the\nGRANT\nand\nREVOKE\ncommands:\nGRANT\ngroup_role\nTO\nrole1\n, ... ;\nREVOKE\ngroup_role\nFROM\nrole1\n, ... ;\nYou can grant membership to other group roles, too (since there isn't really any distinction between group roles and non-group roles). The database will not let you set up circular membership loops. Also, it is not permitted to grant membership in a role to\nPUBLIC\n.\nThe members of a group role can use the privileges of the role in two ways. First, member roles that have been granted membership with the\nSET\noption can do\nSET ROLE\nto temporarily\n“\nbecome\n”\nthe group role. In this state, the database session has access to the privileges of the group role rather than the original login role, and any database objects created are considered owned by the group role not the login role. Second, member roles that have been granted membership with the\nINHERIT\noption automatically have use of the privileges of those directly or indirectly a member of, though the chain stops at memberships lacking the inherit option. As an example, suppose we have done:\nCREATE ROLE joe LOGIN;\nCREATE ROLE admin;\nCREATE ROLE wheel;\nCREATE ROLE island;\nGRANT admin TO joe WITH INHERIT TRUE;\nGRANT wheel TO admin WITH INHERIT FALSE;\nGRANT island TO joe WITH INHERIT TRUE, SET FALSE;\nImmediately after connecting as role\njoe\n, a database session will have use of privileges granted directly to\njoe\nplus any privileges granted to\nadmin\nand\nisland\n, because\njoe\n“\ninherits\n”\nthose privileges. However, privileges granted to\nwheel\nare not available, because even though\njoe\nis indirectly a member of\nwheel\n, the membership is via\nadmin\nwhich was granted using\nWITH INHERIT FALSE\n. After:\nSET ROLE admin;\nthe session would have use of only those privileges granted to\nadmin\n, and not those granted to\njoe\nor\nisland\n. After:\nSET ROLE wheel;\nthe session would have use of only those privileges granted to\nwheel\n, and not those granted to either\njoe\nor\nadmin\n. The original privilege state can be restored with any of:\nSET ROLE joe;\nSET ROLE NONE;\nRESET ROLE;\nNote\nThe\nSET ROLE\ncommand always allows selecting any role that the original login role is directly or indirectly a member of, provided that there is a chain of membership grants each of which has\nSET TRUE\n(which is the default). Thus, in the above example, it is not necessary to become\nadmin\nbefore becoming\nwheel\n. On the other hand, it is not possible to become\nisland\nat all;\njoe\ncan only access those privileges via inheritance.\nNote\nIn the SQL standard, there is a clear distinction between users and roles, and users do not automatically inherit privileges while roles do. This behavior can be obtained in\nPostgreSQL\nby giving roles being used as SQL roles the\nINHERIT\nattribute, while giving roles being used as SQL users the\nNOINHERIT\nattribute. However,\nPostgreSQL\ndefaults to giving all roles the\nINHERIT\nattribute, for backward compatibility with pre-8.1 releases in which users always had use of permissions granted to groups they were members of.\nThe role attributes\nLOGIN\n,\nSUPERUSER\n,\nCREATEDB\n, and\nCREATEROLE\ncan be thought of as special privileges, but they are never inherited as ordinary privileges on database objects are. You must actually\nSET ROLE\nto a specific role having one of these attributes in order to make use of the attribute. Continuing the above example, we might choose to grant\nCREATEDB\nand\nCREATEROLE\nto the\nadmin\nrole. Then a session connecting as role\njoe\nwould not have these privileges immediately, only after doing\nSET ROLE admin\n.\nTo destroy a group role, use\nDROP ROLE\n:\nDROP ROLE\nname\n;\nAny memberships in the group role are automatically revoked (but the member roles are not otherwise affected).\nPrev\nUp\nNext\n21.2. Role Attributes\nHome\n21.4. Dropping Roles\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/role-membership.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31790.8141834
  },
  {
    "title": "PostgreSQL: Documentation: 12: 34.4. Server-Side Functions",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 12\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\n34.4. Server-Side Functions\nPrev\nUp\nChapter 34. Large Objects\nHome\nNext\n34.4. Server-Side Functions\nServer-side functions tailored for manipulating large objects from SQL are listed in\nTable 34.1\n.\nTable 34.1. SQL-Oriented Large Object Functions\nFunction\nReturn Type\nDescription\nExample\nResult\nlo_from_bytea(\nloid\noid\n,\nstring\nbytea\n)\noid\nCreate a large object and store data there, returning its OID. Pass\n0\nto have the system choose an OID.\nlo_from_bytea(0, '\\xffffff00')\n24528\nlo_put(\nloid\noid\n,\noffset\nbigint\n,\nstr\nbytea\n)\nvoid\nWrite data at the given offset.\nlo_put(24528, 1, '\\xaa')\nlo_get(\nloid\noid\n[\n,\nfrom\nbigint\n,\nfor\nint\n])\nbytea\nExtract contents or a substring thereof.\nlo_get(24528, 0, 3)\n\\xffaaff\nThere are additional server-side functions corresponding to each of the client-side functions described earlier; indeed, for the most part the client-side functions are simply interfaces to the equivalent server-side functions. The ones just as convenient to call via SQL commands are\nlo_creat\n,\nlo_create\n,\nlo_unlink\n,\nlo_import\n, and\nlo_export\n. Here are examples of their use:\nCREATE TABLE image (\nname            text,\nraster          oid\n);\nSELECT lo_creat(-1);       -- returns OID of new, empty large object\nSELECT lo_create(43213);   -- attempts to create large object with OID 43213\nSELECT lo_unlink(173454);  -- deletes large object with OID 173454\nINSERT INTO image (name, raster)\nVALUES ('beautiful image', lo_import('/etc/motd'));\nINSERT INTO image (name, raster)  -- same as above, but specify OID to use\nVALUES ('beautiful image', lo_import('/etc/motd', 68583));\nSELECT lo_export(image.raster, '/tmp/motd') FROM image\nWHERE name = 'beautiful image';\nThe server-side\nlo_import\nand\nlo_export\nfunctions behave considerably differently from their client-side analogs. These two functions read and write files in the server's file system, using the permissions of the database's owning user. Therefore, by default their use is restricted to superusers. In contrast, the client-side import and export functions read and write files in the client's file system, using the permissions of the client program. The client-side functions do not require any database privileges, except the privilege to read or write the large object in question.\nCaution\nIt is possible to\nGRANT\nuse of the server-side\nlo_import\nand\nlo_export\nfunctions to non-superusers, but careful consideration of the security implications is required. A malicious user of such privileges could easily parlay them into becoming superuser (for example by rewriting server configuration files), or could attack the rest of the server's file system without bothering to obtain database superuser privileges as such.\nAccess to roles having such privilege must therefore be guarded just as carefully as access to superuser roles.\nNonetheless, if use of server-side\nlo_import\nor\nlo_export\nis needed for some routine task, it's safer to use a role with such privileges than one with full superuser privileges, as that helps to reduce the risk of damage from accidental errors.\nThe functionality of\nlo_read\nand\nlo_write\nis also available via server-side calls, but the names of the server-side functions differ from the client side interfaces in that they do not contain underscores. You must call these functions as\nloread\nand\nlowrite\n.\nPrev\nUp\nNext\n34.3. Client Interfaces\nHome\n34.5. Example Program",
    "url": "https://www.postgresql.org/docs/12/lo-funcs.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31791.0052479
  },
  {
    "title": "DROP USER",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 9.3\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPostgreSQL 9.3.25 Documentation\nPrev\nUp\nNext\nDROP USER\nName\nDROP USER -- remove a database role\nSynopsis\nDROP USER [ IF EXISTS ]\nname\n[, ...]\nDescription\nDROP USER\nis simply an alternate\nspelling of\nDROP ROLE\n.\nCompatibility\nThe\nDROP USER\nstatement is a\nPostgreSQL\nextension. The SQL\nstandard leaves the definition of users to the implementation.\nSee Also\nDROP ROLE\nPrev\nHome\nNext\nDROP TYPE\nUp\nDROP USER MAPPING",
    "url": "https://www.postgresql.org/docs/9.3/sql-dropuser.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31791.109803
  },
  {
    "title": "9.5. Binary String Functions and Operators",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 9.4\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPostgreSQL 9.4.26 Documentation\nPrev\nUp\nChapter 9. Functions and Operators\nNext\n9.5. Binary String Functions and Operators\nThis section describes functions and operators for examining and manipulating values of type\nbytea\n.\nSQL\ndefines some string functions that use key words, rather than commas, to separate arguments. Details are in\nTable 9-9\n.\nPostgreSQL\nalso provides versions of these functions that use the regular function invocation syntax (see\nTable 9-10\n).\nNote:\nThe sample results shown on this page assume that the server parameter\nbytea_output\nis set to\nescape\n(the traditional PostgreSQL format).\nTable 9-9.\nSQL\nBinary String Functions and Operators\nFunction\nReturn Type\nDescription\nExample\nResult\nstring\n||\nstring\nbytea\nString concatenation\n'\\\\Post'::bytea || '\\047gres\\000'::bytea\n\\\\Post'gres\\000\noctet_length(\nstring\n)\nint\nNumber of bytes in binary string\noctet_length('jo\\000se'::bytea)\n5\noverlay(\nstring\nplacing\nstring\nfrom\nint\n[\nfor\nint\n])\nbytea\nReplace substring\noverlay('Th\\000omas'::bytea placing '\\002\\003'::bytea from 2 for 3)\nT\\\\002\\\\003mas\nposition(\nsubstring\nin\nstring\n)\nint\nLocation of specified substring\nposition('\\000om'::bytea in 'Th\\000omas'::bytea)\n3\nsubstring(\nstring\n[\nfrom\nint\n] [\nfor\nint\n])\nbytea\nExtract substring\nsubstring('Th\\000omas'::bytea from 2 for 3)\nh\\000o\ntrim([\nboth\n]\nbytes\nfrom\nstring\n)\nbytea\nRemove the longest string containing only bytes appearing in\nbytes\nfrom the start and end of\nstring\ntrim('\\000\\001'::bytea from '\\000Tom\\001'::bytea)\nTom\nAdditional binary string manipulation functions are available and are listed in\nTable 9-10\n. Some of them are used internally to implement the\nSQL\n-standard string functions listed in\nTable 9-9\n.\nTable 9-10. Other Binary String Functions\nFunction\nReturn Type\nDescription\nExample\nResult\nbtrim(\nstring\nbytea\n,\nbytes\nbytea\n)\nbytea\nRemove the longest string containing only bytes appearing in\nbytes\nfrom the start and end of\nstring\nbtrim('\\000trim\\001'::bytea, '\\000\\001'::bytea)\ntrim\ndecode(\nstring\ntext\n,\nformat\ntext\n)\nbytea\nDecode binary data from textual representation in\nstring\n. Options for\nformat\nare same as in\nencode\n.\ndecode('123\\000456', 'escape')\n123\\000456\nencode(\ndata\nbytea\n,\nformat\ntext\n)\ntext\nEncode binary data into a textual representation. Supported formats are:\nbase64\n,\nhex\n,\nescape\n.\nescape\nconverts zero bytes and high-bit-set bytes to octal sequences (\n\\\nnnn\n) and doubles backslashes.\nencode('123\\000456'::bytea, 'escape')\n123\\000456\nget_bit(\nstring\n,\noffset\n)\nint\nExtract bit from string\nget_bit('Th\\000omas'::bytea, 45)\n1\nget_byte(\nstring\n,\noffset\n)\nint\nExtract byte from string\nget_byte('Th\\000omas'::bytea, 4)\n109\nlength(\nstring\n)\nint\nLength of binary string\nlength('jo\\000se'::bytea)\n5\nmd5(\nstring\n)\ntext\nCalculates the MD5 hash of\nstring\n, returning the result in hexadecimal\nmd5('Th\\000omas'::bytea)\n8ab2d3c9689aaf18 b4958c334c82d8b1\nset_bit(\nstring\n,\noffset\n,\nnewvalue\n)\nbytea\nSet bit in string\nset_bit('Th\\000omas'::bytea, 45, 0)\nTh\\000omAs\nset_byte(\nstring\n,\noffset\n,\nnewvalue\n)\nbytea\nSet byte in string\nset_byte('Th\\000omas'::bytea, 4, 64)\nTh\\000o@as\nget_byte\nand\nset_byte\nnumber the first byte of a binary string as byte 0.\nget_bit\nand\nset_bit\nnumber bits from the right within each byte; for example bit 0 is the least significant bit of the first byte, and bit 15 is the most significant bit of the second byte.\nSee also the aggregate function\nstring_agg\nin\nSection 9.20\nand the large object functions in\nSection 32.4\n.\nPrev\nHome\nNext\nString Functions and Operators\nUp\nBit String Functions and Operators",
    "url": "https://www.postgresql.org/docs/9.4/functions-binarystring.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31791.188497
  },
  {
    "title": "Part I. Tutorial",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 12\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPart I. Tutorial\nPrev\nUp\nPostgreSQL 12.22 Documentation\nHome\nNext\nPart I. Tutorial\nWelcome to the\nPostgreSQL\nTutorial. The following few chapters are intended to give a simple introduction to\nPostgreSQL\n, relational database concepts, and the SQL language to those who are new to any one of these aspects. We only assume some general knowledge about how to use computers. No particular Unix or programming experience is required. This part is mainly intended to give you some hands-on experience with important aspects of the\nPostgreSQL\nsystem. It makes no attempt to be a complete or thorough treatment of the topics it covers.\nAfter you have worked through this tutorial you might want to move on to reading\nPart II\nto gain a more formal knowledge of the SQL language, or\nPart IV\nfor information about developing applications for\nPostgreSQL\n. Those who set up and manage their own server should also read\nPart III\n.\nTable of Contents\n1. Getting Started\n1.1. Installation\n1.2. Architectural Fundamentals\n1.3. Creating a Database\n1.4. Accessing a Database\n2. The\nSQL\nLanguage\n2.1. Introduction\n2.2. Concepts\n2.3. Creating a New Table\n2.4. Populating a Table With Rows\n2.5. Querying a Table\n2.6. Joins Between Tables\n2.7. Aggregate Functions\n2.8. Updates\n2.9. Deletions\n3. Advanced Features\n3.1. Introduction\n3.2. Views\n3.3. Foreign Keys\n3.4. Transactions\n3.5. Window Functions\n3.6. Inheritance\n3.7. Conclusion\nPrev\nUp\nNext\n5. Bug Reporting Guidelines\nHome\nChapter 1. Getting Started",
    "url": "https://www.postgresql.org/docs/12/tutorial.html",
    "source": "postgresql",
    "doc_type": "tutorial",
    "scraped_at": 31791.2297756
  },
  {
    "title": "PostgreSQL: Documentation: 10: 34.4. Server-side Functions",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 10\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\n34.4. Server-side Functions\nPrev\nUp\nChapter 34. Large Objects\nHome\nNext\n34.4. Server-side Functions\nServer-side functions tailored for manipulating large objects from SQL are listed in\nTable 34.1\n.\nTable 34.1. SQL-oriented Large Object Functions\nFunction\nReturn Type\nDescription\nExample\nResult\nlo_from_bytea(\nloid\noid\n,\nstring\nbytea\n)\noid\nCreate a large object and store data there, returning its OID. Pass\n0\nto have the system choose an OID.\nlo_from_bytea(0, '\\xffffff00')\n24528\nlo_put(\nloid\noid\n,\noffset\nbigint\n,\nstr\nbytea\n)\nvoid\nWrite data at the given offset.\nlo_put(24528, 1, '\\xaa')\nlo_get(\nloid\noid\n[\n,\nfrom\nbigint\n,\nfor\nint\n])\nbytea\nExtract contents or a substring thereof.\nlo_get(24528, 0, 3)\n\\xffaaff\nThere are additional server-side functions corresponding to each of the client-side functions described earlier; indeed, for the most part the client-side functions are simply interfaces to the equivalent server-side functions. The ones just as convenient to call via SQL commands are\nlo_creat\n,\nlo_create\n,\nlo_unlink\n,\nlo_import\n, and\nlo_export\n. Here are examples of their use:\nCREATE TABLE image (\nname            text,\nraster          oid\n);\nSELECT lo_creat(-1);       -- returns OID of new, empty large object\nSELECT lo_create(43213);   -- attempts to create large object with OID 43213\nSELECT lo_unlink(173454);  -- deletes large object with OID 173454\nINSERT INTO image (name, raster)\nVALUES ('beautiful image', lo_import('/etc/motd'));\nINSERT INTO image (name, raster)  -- same as above, but specify OID to use\nVALUES ('beautiful image', lo_import('/etc/motd', 68583));\nSELECT lo_export(image.raster, '/tmp/motd') FROM image\nWHERE name = 'beautiful image';\nThe server-side\nlo_import\nand\nlo_export\nfunctions behave considerably differently from their client-side analogs. These two functions read and write files in the server's file system, using the permissions of the database's owning user. Therefore, their use is restricted to superusers. In contrast, the client-side import and export functions read and write files in the client's file system, using the permissions of the client program. The client-side functions do not require superuser privilege.\nThe functionality of\nlo_read\nand\nlo_write\nis also available via server-side calls, but the names of the server-side functions differ from the client side interfaces in that they do not contain underscores. You must call these functions as\nloread\nand\nlowrite\n.\nPrev\nUp\nNext\n34.3. Client Interfaces\nHome\n34.5. Example Program",
    "url": "https://www.postgresql.org/docs/10/lo-funcs.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31791.5228838
  },
  {
    "title": "Part III. Server Administration",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 13\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nPart III. Server Administration\nPrev\nUp\nPostgreSQL 13.21 Documentation\nHome\nNext\nPart III. Server Administration\nThis part covers topics that are of interest to a\nPostgreSQL\ndatabase administrator. This includes installation of the software, set up and configuration of the server, management of users and databases, and maintenance tasks. Anyone who runs a\nPostgreSQL\nserver, even for personal use, but especially in production, should be familiar with the topics covered in this part.\nThe information in this part is arranged approximately in the order in which a new user should read it. But the chapters are self-contained and can be read individually as desired. The information in this part is presented in a narrative fashion in topical units. Readers looking for a complete description of a particular command should see\nPart VI\n.\nThe first few chapters are written so they can be understood without prerequisite knowledge, so new users who need to set up their own server can begin their exploration with this part. The rest of this part is about tuning and management; that material assumes that the reader is familiar with the general use of the\nPostgreSQL\ndatabase system. Readers are encouraged to look at\nPart I\nand\nPart II\nfor additional information.\nTable of Contents\n16. Installation from Source Code\n16.1. Short Version\n16.2. Requirements\n16.3. Getting the Source\n16.4. Installation Procedure\n16.5. Post-Installation Setup\n16.6. Supported Platforms\n16.7. Platform-Specific Notes\n17. Installation from Source Code on\nWindows\n17.1. Building with\nVisual C++\nor the\nMicrosoft Windows SDK\n18. Server Setup and Operation\n18.1. The\nPostgreSQL\nUser Account\n18.2. Creating a Database Cluster\n18.3. Starting the Database Server\n18.4. Managing Kernel Resources\n18.5. Shutting Down the Server\n18.6. Upgrading a\nPostgreSQL\nCluster\n18.7. Preventing Server Spoofing\n18.8. Encryption Options\n18.9. Secure TCP/IP Connections with SSL\n18.10. Secure TCP/IP Connections with GSSAPI Encryption\n18.11. Secure TCP/IP Connections with\nSSH\nTunnels\n18.12. Registering\nEvent Log\non\nWindows\n19. Server Configuration\n19.1. Setting Parameters\n19.2. File Locations\n19.3. Connections and Authentication\n19.4. Resource Consumption\n19.5. Write Ahead Log\n19.6. Replication\n19.7. Query Planning\n19.8. Error Reporting and Logging\n19.9. Run-time Statistics\n19.10. Automatic Vacuuming\n19.11. Client Connection Defaults\n19.12. Lock Management\n19.13. Version and Platform Compatibility\n19.14. Error Handling\n19.15. Preset Options\n19.16. Customized Options\n19.17. Developer Options\n19.18. Short Options\n20. Client Authentication\n20.1. The\npg_hba.conf\nFile\n20.2. User Name Maps\n20.3. Authentication Methods\n20.4. Trust Authentication\n20.5. Password Authentication\n20.6. GSSAPI Authentication\n20.7. SSPI Authentication\n20.8. Ident Authentication\n20.9. Peer Authentication\n20.10. LDAP Authentication\n20.11. RADIUS Authentication\n20.12. Certificate Authentication\n20.13. PAM Authentication\n20.14. BSD Authentication\n20.15. Authentication Problems\n21. Database Roles\n21.1. Database Roles\n21.2. Role Attributes\n21.3. Role Membership\n21.4. Dropping Roles\n21.5. Default Roles\n21.6. Function Security\n22. Managing Databases\n22.1. Overview\n22.2. Creating a Database\n22.3. Template Databases\n22.4. Database Configuration\n22.5. Destroying a Database\n22.6. Tablespaces\n23. Localization\n23.1. Locale Support\n23.2. Collation Support\n23.3. Character Set Support\n24. Routine Database Maintenance Tasks\n24.1. Routine Vacuuming\n24.2. Routine Reindexing\n24.3. Log File Maintenance\n25. Backup and Restore\n25.1.\nSQL\nDump\n25.2. File System Level Backup\n25.3. Continuous Archiving and Point-in-Time Recovery (PITR)\n26. High Availability, Load Balancing, and Replication\n26.1. Comparison of Different Solutions\n26.2. Log-Shipping Standby Servers\n26.3. Failover\n26.4. Alternative Method for Log Shipping\n26.5. Hot Standby\n27. Monitoring Database Activity\n27.1. Standard Unix Tools\n27.2. The Statistics Collector\n27.3. Viewing Locks\n27.4. Progress Reporting\n27.5. Dynamic Tracing\n28. Monitoring Disk Usage\n28.1. Determining Disk Usage\n28.2. Disk Full Failure\n29. Reliability and the Write-Ahead Log\n29.1. Reliability\n29.2. Write-Ahead Logging (\nWAL\n)\n29.3. Asynchronous Commit\n29.4.\nWAL\nConfiguration\n29.5. WAL Internals\n30. Logical Replication\n30.1. Publication\n30.2. Subscription\n30.3. Conflicts\n30.4. Restrictions\n30.5. Architecture\n30.6. Monitoring\n30.7. Security\n30.8. Configuration Settings\n30.9. Quick Setup\n31. Just-in-Time Compilation (\nJIT\n)\n31.1. What Is\nJIT\ncompilation?\n31.2. When to\nJIT\n?\n31.3. Configuration\n31.4. Extensibility\n32. Regression Tests\n32.1. Running the Tests\n32.2. Test Evaluation\n32.3. Variant Comparison Files\n32.4. TAP Tests\n32.5. Test Coverage Examination\nPrev\nUp\nNext\n15.4. Parallel Safety\nHome\nChapter 16. Installation from Source Code\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/13/admin.html",
    "source": "postgresql",
    "doc_type": "administration",
    "scraped_at": 31791.646363
  },
  {
    "title": "NOTIFY",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 8.3\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPostgreSQL\n8.3.23 Documentation\nPrev\nFast Backward\nFast Forward\nNext\nNOTIFY\nName\nNOTIFY -- generate a notification\nSynopsis\nNOTIFY\nname\nDescription\nThe\nNOTIFY\ncommand sends a\nnotification event to each client application that has previously\nexecuted\nLISTEN\nname\nfor the specified notification\nname in the current database.\nNOTIFY\nprovides a simple form of\nsignal or interprocess communication mechanism for a collection\nof processes accessing the same\nPostgreSQL\ndatabase. Higher-level mechanisms\ncan be built by using tables in the database to pass additional\ndata (beyond a mere notification name) from notifier to\nlistener(s).\nThe information passed to the client for a notification event\nincludes the notification name and the notifying session's server\nprocess\nPID\n. It is up to the\ndatabase designer to define the notification names that will be\nused in a given database and what each one means.\nCommonly, the notification name is the same as the name of\nsome table in the database, and the notify event essentially\nmeans,\n\"I changed this table, take a look at\nit to see what's new\"\n. But no such association is enforced\nby the\nNOTIFY\nand\nLISTEN\ncommands. For example, a database designer\ncould use several different notification names to signal\ndifferent sorts of changes to a single table.\nWhen\nNOTIFY\nis used to signal the\noccurrence of changes to a particular table, a useful programming\ntechnique is to put the\nNOTIFY\nin a rule\nthat is triggered by table updates. In this way, notification\nhappens automatically when the table is changed, and the\napplication programmer cannot accidentally forget to do it.\nNOTIFY\ninteracts with SQL\ntransactions in some important ways. Firstly, if a\nNOTIFY\nis executed inside a transaction, the\nnotify events are not delivered until and unless the transaction\nis committed. This is appropriate, since if the transaction is\naborted, all the commands within it have had no effect, including\nNOTIFY\n. But it can be disconcerting if\none is expecting the notification events to be delivered\nimmediately. Secondly, if a listening session receives a\nnotification signal while it is within a transaction, the\nnotification event will not be delivered to its connected client\nuntil just after the transaction is completed (either committed\nor aborted). Again, the reasoning is that if a notification were\ndelivered within a transaction that was later aborted, one would\nwant the notification to be undone somehow — but the server\ncannot\n\"take back\"\na notification once\nit has sent it to the client. So notification events are only\ndelivered between transactions. The upshot of this is that\napplications using\nNOTIFY\nfor real-time\nsignaling should try to keep their transactions short.\nNOTIFY\nbehaves like Unix signals in\none important respect: if the same notification name is signaled\nmultiple times in quick succession, recipients might get only one\nnotification event for several executions of\nNOTIFY\n. So it is a bad idea to depend on the\nnumber of notifications received. Instead, use\nNOTIFY\nto wake up applications that need to pay\nattention to something, and use a database object (such as a\nsequence) to keep track of what happened or how many times it\nhappened.\nIt is common for a client that executes\nNOTIFY\nto be listening on the same notification\nname itself. In that case it will get back a notification event,\njust like all the other listening sessions. Depending on the\napplication logic, this could result in useless work, for\nexample, reading a database table to find the same updates that\nthat session just wrote out. It is possible to avoid such extra\nwork by noticing whether the notifying session's server process\nPID\n(supplied in the\nnotification event message) is the same as one's own session's\nPID\n(available from\nlibpq\n). When they are the same,\nthe notification event is one's own work bouncing back, and can\nbe ignored. (Despite what was said in the preceding paragraph,\nthis is a safe technique.\nPostgreSQL\nkeeps self-notifications separate\nfrom notifications arriving from other sessions, so you cannot\nmiss an outside notification by ignoring your own\nnotifications.)\nParameters\nname\nName of the notification to be signaled (any\nidentifier).\nExamples\nConfigure and execute a listen/notify sequence from\npsql\n:\nLISTEN virtual;\nNOTIFY virtual;\nAsynchronous notification \"virtual\" received from server process with PID 8448.\nCompatibility\nThere is no\nNOTIFY\nstatement in the\nSQL standard.\nSee Also\nLISTEN\n,\nUNLISTEN\nPrev\nHome\nNext\nMOVE\nUp\nPREPARE",
    "url": "https://www.postgresql.org/docs/8.3/sql-notify.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31791.744945
  },
  {
    "title": "14.5. Non-Durable Settings",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 9.6\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPostgreSQL 9.6.24 Documentation\nPrev\nUp\nChapter 14. Performance Tips\nNext\n14.5. Non-Durable Settings\nDurability is a database feature that guarantees the recording of committed transactions even if the server crashes or loses power. However, durability adds significant database overhead, so if your site does not require such a guarantee,\nPostgreSQL\ncan be configured to run much faster. The following are configuration changes you can make to improve performance in such cases. Except as noted below, durability is still guaranteed in case of a crash of the database software; only abrupt operating system stoppage creates a risk of data loss or corruption when these settings are used.\nPlace the database cluster's data directory in a memory-backed file system (i.e.,\nRAM\ndisk). This eliminates all database disk I/O, but limits data storage to the amount of available memory (and perhaps swap).\nTurn off\nfsync\n; there is no need to flush data to disk.\nTurn off\nsynchronous_commit\n; there might be no need to force\nWAL\nwrites to disk on every commit. This setting does risk transaction loss (though not data corruption) in case of a crash of the\ndatabase\n.\nTurn off\nfull_page_writes\n; there is no need to guard against partial page writes.\nIncrease\nmax_wal_size\nand\ncheckpoint_timeout\n; this reduces the frequency of checkpoints, but increases the storage requirements of\n/pg_xlog\n.\nCreate\nunlogged tables\nto avoid\nWAL\nwrites, though it makes the tables non-crash-safe.\nPrev\nHome\nNext\nPopulating a Database\nUp\nParallel Query",
    "url": "https://www.postgresql.org/docs/9.6/non-durability.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31791.7968603
  },
  {
    "title": "PostgreSQL: Documentation: 13: 2.2. Concepts",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 13\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n2.2. Concepts\nPrev\nUp\nChapter 2. The\nSQL\nLanguage\nHome\nNext\n2.2. Concepts\nPostgreSQL\nis a\nrelational database management system\n(\nRDBMS\n). That means it is a system for managing data stored in\nrelations\n. Relation is essentially a mathematical term for\ntable\n. The notion of storing data in tables is so commonplace today that it might seem inherently obvious, but there are a number of other ways of organizing databases. Files and directories on Unix-like operating systems form an example of a hierarchical database. A more modern development is the object-oriented database.\nEach table is a named collection of\nrows\n. Each row of a given table has the same set of named\ncolumns\n, and each column is of a specific data type. Whereas columns have a fixed order in each row, it is important to remember that SQL does not guarantee the order of the rows within the table in any way (although they can be explicitly sorted for display).\nTables are grouped into databases, and a collection of databases managed by a single\nPostgreSQL\nserver instance constitutes a database\ncluster\n.\nPrev\nUp\nNext\n2.1. Introduction\nHome\n2.3. Creating a New Table\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/13/tutorial-concepts.html",
    "source": "postgresql",
    "doc_type": "tutorial",
    "scraped_at": 31791.8382775
  },
  {
    "title": "PostgreSQL: Documentation: devel: 32.13. Notice Processing",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL devel\n(2025-07-29 07:44:57 - git commit\nc2c2c7e2256\n)\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\n32.13. Notice Processing\nPrev\nUp\nChapter 32.\nlibpq\n— C Library\nHome\nNext\n32.13. Notice Processing\n#\nNotice and warning messages generated by the server are not returned by the query execution functions, since they do not imply failure of the query. Instead they are passed to a notice handling function, and execution continues normally after the handler returns. The default notice handling function prints the message on\nstderr\n, but the application can override this behavior by supplying its own handling function.\nFor historical reasons, there are two levels of notice handling, called the notice receiver and notice processor. The default behavior is for the notice receiver to format the notice and pass a string to the notice processor for printing. However, an application that chooses to provide its own notice receiver will typically ignore the notice processor layer and just do all the work in the notice receiver.\nThe function\nPQsetNoticeReceiver\nsets or examines the current notice receiver for a connection object. Similarly,\nPQsetNoticeProcessor\nsets or examines the current notice processor.\ntypedef void (*PQnoticeReceiver) (void *arg, const PGresult *res);\nPQnoticeReceiver\nPQsetNoticeReceiver(PGconn *conn,\nPQnoticeReceiver proc,\nvoid *arg);\ntypedef void (*PQnoticeProcessor) (void *arg, const char *message);\nPQnoticeProcessor\nPQsetNoticeProcessor(PGconn *conn,\nPQnoticeProcessor proc,\nvoid *arg);\nEach of these functions returns the previous notice receiver or processor function pointer, and sets the new value. If you supply a null function pointer, no action is taken, but the current pointer is returned.\nWhen a notice or warning message is received from the server, or generated internally by\nlibpq\n, the notice receiver function is called. It is passed the message in the form of a\nPGRES_NONFATAL_ERROR\nPGresult\n. (This allows the receiver to extract individual fields using\nPQresultErrorField\n, or obtain a complete preformatted message using\nPQresultErrorMessage\nor\nPQresultVerboseErrorMessage\n.) The same void pointer passed to\nPQsetNoticeReceiver\nis also passed. (This pointer can be used to access application-specific state if needed.)\nThe default notice receiver simply extracts the message (using\nPQresultErrorMessage\n) and passes it to the notice processor.\nThe notice processor is responsible for handling a notice or warning message given in text form. It is passed the string text of the message (including a trailing newline), plus a void pointer that is the same one passed to\nPQsetNoticeProcessor\n. (This pointer can be used to access application-specific state if needed.)\nThe default notice processor is simply:\nstatic void\ndefaultNoticeProcessor(void *arg, const char *message)\n{\nfprintf(stderr, \"%s\", message);\n}\nOnce you have set a notice receiver or processor, you should expect that that function could be called as long as either the\nPGconn\nobject or\nPGresult\nobjects made from it exist. At creation of a\nPGresult\n, the\nPGconn\n's current notice handling pointers are copied into the\nPGresult\nfor possible use by functions like\nPQgetvalue\n.\nPrev\nUp\nNext\n32.12. Miscellaneous Functions\nHome\n32.14. Event System",
    "url": "https://www.postgresql.org/docs/devel/libpq-notice-processing.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31792.0933599
  },
  {
    "title": "PostgreSQL: Documentation: 12: 14.5. Non-Durable Settings",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 12\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\n14.5. Non-Durable Settings\nPrev\nUp\nChapter 14. Performance Tips\nHome\nNext\n14.5. Non-Durable Settings\nDurability is a database feature that guarantees the recording of committed transactions even if the server crashes or loses power. However, durability adds significant database overhead, so if your site does not require such a guarantee,\nPostgreSQL\ncan be configured to run much faster. The following are configuration changes you can make to improve performance in such cases. Except as noted below, durability is still guaranteed in case of a crash of the database software; only abrupt operating system stoppage creates a risk of data loss or corruption when these settings are used.\nPlace the database cluster's data directory in a memory-backed file system (i.e.,\nRAM\ndisk). This eliminates all database disk I/O, but limits data storage to the amount of available memory (and perhaps swap).\nTurn off\nfsync\n; there is no need to flush data to disk.\nTurn off\nsynchronous_commit\n; there might be no need to force\nWAL\nwrites to disk on every commit. This setting does risk transaction loss (though not data corruption) in case of a crash of the\ndatabase\n.\nTurn off\nfull_page_writes\n; there is no need to guard against partial page writes.\nIncrease\nmax_wal_size\nand\ncheckpoint_timeout\n; this reduces the frequency of checkpoints, but increases the storage requirements of\n/pg_wal\n.\nCreate\nunlogged tables\nto avoid\nWAL\nwrites, though it makes the tables non-crash-safe.\nPrev\nUp\nNext\n14.4. Populating a Database\nHome\nChapter 15. Parallel Query",
    "url": "https://www.postgresql.org/docs/12/non-durability.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31792.3191101
  },
  {
    "title": "VI. Reference",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 9.3\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n/\n7.1\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPostgreSQL 9.3.25 Documentation\nPrev\nHome\nNext\nVI. Reference\nThe entries in this Reference are meant to provide in reasonable\nlength an authoritative, complete, and formal summary about their\nrespective subjects. More information about the use of\nPostgreSQL\n, in narrative, tutorial, or example\nform, can be found in other parts of this book. See the\ncross-references listed on each reference page.\nThe reference entries are also available as traditional\n\"man\"\npages.\nTable of Contents\nI.\nSQL Commands\nABORT\n-- abort the\ncurrent transaction\nALTER\nAGGREGATE\n-- change the definition of an aggregate\nfunction\nALTER\nCOLLATION\n-- change the definition of a\ncollation\nALTER\nCONVERSION\n-- change the definition of a\nconversion\nALTER\nDATABASE\n-- change a database\nALTER DEFAULT\nPRIVILEGES\n-- define default access privileges\nALTER DOMAIN\n--\nchange the definition of a domain\nALTER EVENT\nTRIGGER\n-- change the definition of an event\ntrigger\nALTER\nEXTENSION\n--  change the definition of an\nextension\nALTER FOREIGN DATA\nWRAPPER\n-- change the definition of a foreign-data\nwrapper\nALTER FOREIGN\nTABLE\n-- change the definition of a foreign\ntable\nALTER\nFUNCTION\n-- change the definition of a function\nALTER\nGROUP\n-- change role name or membership\nALTER\nINDEX\n-- change the definition of an index\nALTER\nLANGUAGE\n-- change the definition of a procedural\nlanguage\nALTER LARGE\nOBJECT\n-- change the definition of a large\nobject\nALTER MATERIALIZED\nVIEW\n-- change the definition of a materialized\nview\nALTER\nOPERATOR\n-- change the definition of an operator\nALTER OPERATOR\nCLASS\n-- change the definition of an operator\nclass\nALTER OPERATOR\nFAMILY\n-- change the definition of an operator\nfamily\nALTER ROLE\n-- change\na database role\nALTER RULE\n-- change\nthe definition of a rule\nALTER\nSCHEMA\n-- change the definition of a schema\nALTER\nSEQUENCE\n--  change the definition of a sequence\ngenerator\nALTER\nSERVER\n-- change the definition of a foreign\nserver\nALTER\nTABLE\n-- change the definition of a table\nALTER\nTABLESPACE\n-- change the definition of a\ntablespace\nALTER TEXT SEARCH\nCONFIGURATION\n-- change the definition of a text\nsearch configuration\nALTER TEXT SEARCH\nDICTIONARY\n-- change the definition of a text search\ndictionary\nALTER TEXT SEARCH\nPARSER\n-- change the definition of a text search\nparser\nALTER TEXT SEARCH\nTEMPLATE\n-- change the definition of a text search\ntemplate\nALTER\nTRIGGER\n-- change the definition of a trigger\nALTER TYPE\n--\nchange the definition of a type\nALTER USER\n-- change\na database role\nALTER USER\nMAPPING\n-- change the definition of a user\nmapping\nALTER VIEW\n-- change\nthe definition of a view\nANALYZE\n-- collect\nstatistics about a database\nBEGIN\n-- start a\ntransaction block\nCHECKPOINT\n-- force\na transaction log checkpoint\nCLOSE\n-- close a\ncursor\nCLUSTER\n-- cluster a\ntable according to an index\nCOMMENT\n-- define or\nchange the comment of an object\nCOMMIT\n-- commit the\ncurrent transaction\nCOMMIT\nPREPARED\n-- commit a transaction that was earlier\nprepared for two-phase commit\nCOPY\n-- copy data between\na file and a table\nCREATE\nAGGREGATE\n-- define a new aggregate function\nCREATE\nCAST\n-- define a new cast\nCREATE\nCOLLATION\n-- define a new collation\nCREATE\nCONVERSION\n-- define a new encoding conversion\nCREATE\nDATABASE\n-- create a new database\nCREATE\nDOMAIN\n-- define a new domain\nCREATE EVENT\nTRIGGER\n-- define a new event trigger\nCREATE\nEXTENSION\n-- install an extension\nCREATE FOREIGN DATA\nWRAPPER\n-- define a new foreign-data wrapper\nCREATE FOREIGN\nTABLE\n-- define a new foreign table\nCREATE\nFUNCTION\n-- define a new function\nCREATE\nGROUP\n-- define a new database role\nCREATE\nINDEX\n-- define a new index\nCREATE\nLANGUAGE\n-- define a new procedural language\nCREATE MATERIALIZED\nVIEW\n-- define a new materialized view\nCREATE\nOPERATOR\n-- define a new operator\nCREATE OPERATOR\nCLASS\n-- define a new operator class\nCREATE OPERATOR\nFAMILY\n-- define a new operator family\nCREATE\nROLE\n-- define a new database role\nCREATE\nRULE\n-- define a new rewrite rule\nCREATE\nSCHEMA\n-- define a new schema\nCREATE\nSEQUENCE\n-- define a new sequence generator\nCREATE\nSERVER\n-- define a new foreign server\nCREATE\nTABLE\n-- define a new table\nCREATE TABLE\nAS\n-- define a new table from the results of a\nquery\nCREATE\nTABLESPACE\n-- define a new tablespace\nCREATE TEXT SEARCH\nCONFIGURATION\n-- define a new text search\nconfiguration\nCREATE TEXT SEARCH\nDICTIONARY\n-- define a new text search\ndictionary\nCREATE TEXT SEARCH\nPARSER\n-- define a new text search parser\nCREATE TEXT SEARCH\nTEMPLATE\n-- define a new text search template\nCREATE\nTRIGGER\n-- define a new trigger\nCREATE\nTYPE\n-- define a new data type\nCREATE\nUSER\n-- define a new database role\nCREATE USER\nMAPPING\n-- define a new mapping of a user to a\nforeign server\nCREATE\nVIEW\n-- define a new view\nDEALLOCATE\n-- deallocate a\nprepared statement\nDECLARE\n-- define a\ncursor\nDELETE\n-- delete rows\nof a table\nDISCARD\n-- discard\nsession state\nDO\n-- execute an anonymous\ncode block\nDROP\nAGGREGATE\n-- remove an aggregate function\nDROP CAST\n-- remove a\ncast\nDROP\nCOLLATION\n-- remove a collation\nDROP\nCONVERSION\n-- remove a conversion\nDROP\nDATABASE\n-- remove a database\nDROP\nDOMAIN\n-- remove a domain\nDROP EVENT\nTRIGGER\n-- remove an event trigger\nDROP\nEXTENSION\n-- remove an extension\nDROP FOREIGN DATA\nWRAPPER\n-- remove a foreign-data wrapper\nDROP FOREIGN\nTABLE\n-- remove a foreign table\nDROP\nFUNCTION\n-- remove a function\nDROP GROUP\n-- remove\na database role\nDROP INDEX\n-- remove\nan index\nDROP\nLANGUAGE\n-- remove a procedural language\nDROP MATERIALIZED\nVIEW\n-- remove a materialized view\nDROP\nOPERATOR\n-- remove an operator\nDROP OPERATOR\nCLASS\n-- remove an operator class\nDROP OPERATOR\nFAMILY\n-- remove an operator family\nDROP\nOWNED\n-- remove database objects owned by a database\nrole\nDROP ROLE\n-- remove a\ndatabase role\nDROP RULE\n-- remove a\nrewrite rule\nDROP\nSCHEMA\n-- remove a schema\nDROP\nSEQUENCE\n-- remove a sequence\nDROP\nSERVER\n-- remove a foreign server descriptor\nDROP TABLE\n-- remove\na table\nDROP\nTABLESPACE\n-- remove a tablespace\nDROP TEXT SEARCH\nCONFIGURATION\n-- remove a text search\nconfiguration\nDROP TEXT SEARCH\nDICTIONARY\n-- remove a text search dictionary\nDROP TEXT SEARCH\nPARSER\n-- remove a text search parser\nDROP TEXT SEARCH\nTEMPLATE\n-- remove a text search template\nDROP\nTRIGGER\n-- remove a trigger\nDROP TYPE\n-- remove a\ndata type\nDROP USER\n-- remove a\ndatabase role\nDROP USER\nMAPPING\n-- remove a user mapping for a foreign\nserver\nDROP VIEW\n-- remove a\nview\nEND\n-- commit the current\ntransaction\nEXECUTE\n-- execute a\nprepared statement\nEXPLAIN\n-- show the\nexecution plan of a statement\nFETCH\n-- retrieve rows\nfrom a query using a cursor\nGRANT\n-- define access\nprivileges\nINSERT\n-- create new\nrows in a table\nLISTEN\n-- listen for a\nnotification\nLOAD\n-- load a shared\nlibrary file\nLOCK\n-- lock a table\nMOVE\n-- position a\ncursor\nNOTIFY\n-- generate a\nnotification\nPREPARE\n-- prepare a\nstatement for execution\nPREPARE\nTRANSACTION\n-- prepare the current transaction for\ntwo-phase commit\nREASSIGN\nOWNED\n-- change the ownership of database objects\nowned by a database role\nREFRESH MATERIALIZED\nVIEW\n-- replace the contents of a materialized\nview\nREINDEX\n-- rebuild\nindexes\nRELEASE\nSAVEPOINT\n-- destroy a previously defined\nsavepoint\nRESET\n-- restore the\nvalue of a run-time parameter to the default value\nREVOKE\n-- remove access\nprivileges\nROLLBACK\n-- abort the\ncurrent transaction\nROLLBACK\nPREPARED\n-- cancel a transaction that was earlier\nprepared for two-phase commit\nROLLBACK TO\nSAVEPOINT\n-- roll back to a savepoint\nSAVEPOINT\n-- define\na new savepoint within the current transaction\nSECURITY\nLABEL\n-- define or change a security label applied to\nan object\nSELECT\n-- retrieve rows\nfrom a table or view\nSELECT\nINTO\n-- define a new table from the results of a\nquery\nSET\n-- change a run-time\nparameter\nSET\nCONSTRAINTS\n-- set constraint check timing for the\ncurrent transaction\nSET ROLE\n-- set the\ncurrent user identifier of the current session\nSET SESSION\nAUTHORIZATION\n-- set the session user identifier and\nthe current user identifier of the current session\nSET\nTRANSACTION\n-- set the characteristics of the current\ntransaction\nSHOW\n-- show the value of\na run-time parameter\nSTART\nTRANSACTION\n-- start a transaction block\nTRUNCATE\n-- empty a\ntable or set of tables\nUNLISTEN\n-- stop\nlistening for a notification\nUPDATE\n-- update rows\nof a table\nVACUUM\n-- garbage-collect and\noptionally analyze a database\nVALUES\n-- compute a set\nof rows\nII.\nPostgreSQL Client\nApplications\nclusterdb\n-- cluster a\nPostgreSQL\ndatabase\ncreatedb\n-- create a new\nPostgreSQL\ndatabase\ncreatelang\n-- install a\nPostgreSQL\nprocedural\nlanguage\ncreateuser\n-- define a new\nPostgreSQL\nuser account\ndropdb\n-- remove a\nPostgreSQL\ndatabase\ndroplang\n-- remove a\nPostgreSQL\nprocedural\nlanguage\ndropuser\n-- remove a\nPostgreSQL\nuser account\necpg\n-- embedded SQL C\npreprocessor\npg_basebackup\n-- take a base\nbackup of a\nPostgreSQL\ncluster\npg_config\n-- retrieve\ninformation about the installed version of\nPostgreSQL\npg_dump\n--  extract a\nPostgreSQL\ndatabase into a script\nfile or other archive file\npg_dumpall\n-- extract a\nPostgreSQL\ndatabase cluster into a\nscript file\npg_isready\n-- check the\nconnection status of a\nPostgreSQL\nserver\npg_receivexlog\n-- streams\ntransaction logs from a\nPostgreSQL\ncluster\npg_restore\n--\nrestore a\nPostgreSQL\ndatabase from\nan archive file created by\npg_dump\npsql\n--\nPostgreSQL\ninteractive terminal\nreindexdb\n-- reindex a\nPostgreSQL\ndatabase\nvacuumdb\n-- garbage-collect and\nanalyze a\nPostgreSQL\ndatabase\nIII.\nPostgreSQL Server\nApplications\ninitdb\n-- create a new\nPostgreSQL\ndatabase cluster\npg_controldata\n-- display\ncontrol information of a\nPostgreSQL\ndatabase cluster\npg_ctl\n-- initialize, start,\nstop, or control a\nPostgreSQL\nserver\npg_resetxlog\n-- reset the\nwrite-ahead log and other control information of a\nPostgreSQL\ndatabase cluster\npostgres\n--\nPostgreSQL\ndatabase server\npostmaster\n--\nPostgreSQL\ndatabase server\nPrev\nHome\nNext\nBackground Worker\nProcesses\nSQL Commands",
    "url": "https://www.postgresql.org/docs/9.3/reference.html",
    "source": "postgresql",
    "doc_type": "reference",
    "scraped_at": 31792.4344844
  },
  {
    "title": "18.6. Query Planning",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 8.3\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPostgreSQL\n8.3.23 Documentation\nPrev\nFast Backward\nChapter 18.\nServer Configuration\nFast Forward\nNext\n18.6. Query Planning\n18.6.1. Planner Method\nConfiguration\nThese configuration parameters provide a crude method of\ninfluencing the query plans chosen by the query optimizer. If\nthe default plan chosen by the optimizer for a particular query\nis not optimal, a temporary solution can be found by using one\nof these configuration parameters to force the optimizer to\nchoose a different plan. Turning one of these settings off\npermanently is seldom a good idea, however. Better ways to\nimprove the quality of the plans chosen by the optimizer\ninclude adjusting the\nPlanner\nCost Constants\n, running\nANALYZE\nmore frequently, increasing the\nvalue of the\ndefault_statistics_target\nconfiguration parameter, and increasing the amount of\nstatistics collected for specific columns using\nALTER TABLE SET STATISTICS\n.\nenable_bitmapscan\n(\nboolean\n)\nEnables or disables the query planner's use of\nbitmap-scan plan types. The default is\non\n.\nenable_hashagg\n(\nboolean\n)\nEnables or disables the query planner's use of hashed\naggregation plan types. The default is\non\n.\nenable_hashjoin\n(\nboolean\n)\nEnables or disables the query planner's use of\nhash-join plan types. The default is\non\n.\nenable_indexscan\n(\nboolean\n)\nEnables or disables the query planner's use of\nindex-scan plan types. The default is\non\n.\nenable_mergejoin\n(\nboolean\n)\nEnables or disables the query planner's use of\nmerge-join plan types. The default is\non\n.\nenable_nestloop\n(\nboolean\n)\nEnables or disables the query planner's use of\nnested-loop join plans. It's not possible to suppress\nnested-loop joins entirely, but turning this variable off\ndiscourages the planner from using one if there are other\nmethods available. The default is\non\n.\nenable_seqscan\n(\nboolean\n)\nEnables or disables the query planner's use of\nsequential scan plan types. It's not possible to suppress\nsequential scans entirely, but turning this variable off\ndiscourages the planner from using one if there are other\nmethods available. The default is\non\n.\nenable_sort\n(\nboolean\n)\nEnables or disables the query planner's use of\nexplicit sort steps. It's not possible to suppress\nexplicit sorts entirely, but turning this variable off\ndiscourages the planner from using one if there are other\nmethods available. The default is\non\n.\nenable_tidscan\n(\nboolean\n)\nEnables or disables the query planner's use of\nTID\nscan plan types.\nThe default is\non\n.\n18.6.2. Planner Cost\nConstants\nThe\ncost\nvariables described in\nthis section are measured on an arbitrary scale. Only their\nrelative values matter, hence scaling them all up or down by\nthe same factor will result in no change in the planner's\nchoices. Traditionally, these variables have been referenced to\nsequential page fetches as the unit of cost; that is,\nseq_page_cost\nis conventionally set to\n1.0\nand the other cost variables are\nset with reference to that. But you can use a different scale\nif you prefer, such as actual execution times in milliseconds\non a particular machine.\nNote:\nUnfortunately, there is no well-defined\nmethod for determining ideal values for the cost variables.\nThey are best treated as averages over the entire mix of\nqueries that a particular installation will get. This means\nthat changing them on the basis of just a few experiments\nis very risky.\nseq_page_cost\n(\nfloating\npoint\n)\nSets the planner's estimate of the cost of a disk page\nfetch that is part of a series of sequential fetches. The\ndefault is 1.0.\nrandom_page_cost\n(\nfloating\npoint\n)\nSets the planner's estimate of the cost of a\nnon-sequentially-fetched disk page. The default is 4.0.\nReducing this value relative to\nseq_page_cost\nwill cause the system to\nprefer index scans; raising it will make index scans look\nrelatively more expensive. You can raise or lower both\nvalues together to change the importance of disk I/O\ncosts relative to CPU costs, which are described by the\nfollowing parameters.\nTip:\nAlthough the system will let you set\nrandom_page_cost\nto less\nthan\nseq_page_cost\n, it is\nnot physically sensible to do so. However, setting\nthem equal makes sense if the database is entirely\ncached in RAM, since in that case there is no penalty\nfor touching pages out of sequence. Also, in a\nheavily-cached database you should lower both values\nrelative to the CPU parameters, since the cost of\nfetching a page already in RAM is much smaller than\nit would normally be.\ncpu_tuple_cost\n(\nfloating\npoint\n)\nSets the planner's estimate of the cost of processing\neach row during a query. The default is 0.01.\ncpu_index_tuple_cost\n(\nfloating point\n)\nSets the planner's estimate of the cost of processing\neach index entry during an index scan. The default is\n0.005.\ncpu_operator_cost\n(\nfloating\npoint\n)\nSets the planner's estimate of the cost of processing\neach operator or function executed during a query. The\ndefault is 0.0025.\neffective_cache_size\n(\ninteger\n)\nSets the planner's assumption about the effective size\nof the disk cache that is available to a single query.\nThis is factored into estimates of the cost of using an\nindex; a higher value makes it more likely index scans\nwill be used, a lower value makes it more likely\nsequential scans will be used. When setting this\nparameter you should consider both\nPostgreSQL\n's shared buffers and the\nportion of the kernel's disk cache that will be used for\nPostgreSQL\ndata files.\nAlso, take into account the expected number of concurrent\nqueries on different tables, since they will have to\nshare the available space. This parameter has no effect\non the size of shared memory allocated by\nPostgreSQL\n, nor does it reserve\nkernel disk cache; it is used only for estimation\npurposes. The default is 128 megabytes (\n128MB\n).\n18.6.3. Genetic Query\nOptimizer\ngeqo\n(\nboolean\n)\nEnables or disables genetic query optimization, which\nis an algorithm that attempts to do query planning\nwithout exhaustive searching. This is on by default. The\ngeqo_threshold\nvariable provides\na more granular way to disable GEQO for certain classes\nof queries.\ngeqo_threshold\n(\ninteger\n)\nUse genetic query optimization to plan queries with at\nleast this many\nFROM\nitems\ninvolved. (Note that a\nFULL OUTER\nJOIN\nconstruct counts as only one\nFROM\nitem.) The default is 12. For simpler\nqueries it is usually best to use the deterministic,\nexhaustive planner, but for queries with many tables the\ndeterministic planner takes too long.\ngeqo_effort\n(\ninteger\n)\nControls the trade off between planning time and query\nplan efficiency in GEQO. This variable must be an integer\nin the range from 1 to 10. The default value is five.\nLarger values increase the time spent doing query\nplanning, but also increase the likelihood that an\nefficient query plan will be chosen.\ngeqo_effort\ndoesn't actually\ndo anything directly; it is only used to compute the\ndefault values for the other variables that influence\nGEQO behavior (described below). If you prefer, you can\nset the other parameters by hand instead.\ngeqo_pool_size\n(\ninteger\n)\nControls the pool size used by GEQO. The pool size is\nthe number of individuals in the genetic population. It\nmust be at least two, and useful values are typically 100\nto 1000. If it is set to zero (the default setting) then\na suitable default is chosen based on\ngeqo_effort\nand the number of tables in\nthe query.\ngeqo_generations\n(\ninteger\n)\nControls the number of generations used by GEQO.\nGenerations specifies the number of iterations of the\nalgorithm. It must be at least one, and useful values are\nin the same range as the pool size. If it is set to zero\n(the default setting) then a suitable default is chosen\nbased on\ngeqo_pool_size\n.\ngeqo_selection_bias\n(\nfloating point\n)\nControls the selection bias used by GEQO. The\nselection bias is the selective pressure within the\npopulation. Values can be from 1.50 to 2.00; the latter\nis the default.\n18.6.4. Other Planner\nOptions\ndefault_statistics_target\n(\ninteger\n)\nSets the default statistics target for table columns\nthat have not had a column-specific target set via\nALTER TABLE SET STATISTICS\n.\nLarger values increase the time needed to do\nANALYZE\n, but might improve the quality of\nthe planner's estimates. The default is 10. For more\ninformation on the use of statistics by the\nPostgreSQL\nquery planner, refer to\nSection 14.2\n.\nconstraint_exclusion\n(\nboolean\n)\nEnables or disables the query planner's use of table\nconstraints to optimize queries. The default is\noff\n.\nWhen this parameter is\non\n,\nthe planner compares query conditions with table\nCHECK\nconstraints, and omits\nscanning tables for which the conditions contradict the\nconstraints. For example:\nCREATE TABLE parent(key integer, ...);\nCREATE TABLE child1000(check (key between 1000 and 1999)) INHERITS(parent);\nCREATE TABLE child2000(check (key between 2000 and 2999)) INHERITS(parent);\n...\nSELECT * FROM parent WHERE key = 2400;\nWith constraint exclusion enabled, this\nSELECT\nwill not scan\nchild1000\nat all. This can improve\nperformance when inheritance is used to build partitioned\ntables.\nCurrently,\nconstraint_exclusion\nis disabled by\ndefault because the constraint checks are relatively\nexpensive, and in many circumstances will yield no\nsavings. It is recommended to turn this on only if you\nare actually using partitioned tables designed to take\nadvantage of the feature.\nRefer to\nSection\n5.9\nfor more information on using constraint\nexclusion and partitioning.\nfrom_collapse_limit\n(\ninteger\n)\nThe planner will merge sub-queries into upper queries\nif the resulting\nFROM\nlist would\nhave no more than this many items. Smaller values reduce\nplanning time but might yield inferior query plans. The\ndefault is eight. It is usually wise to keep this less\nthan\ngeqo_threshold\n.\nFor more information see\nSection 14.3\n.\njoin_collapse_limit\n(\ninteger\n)\nThe planner will rewrite explicit\nJOIN\nconstructs (except\nFULL JOIN\ns) into lists of\nFROM\nitems whenever a list of no more than\nthis many items would result. Smaller values reduce\nplanning time but might yield inferior query plans.\nBy default, this variable is set the same as\nfrom_collapse_limit\n, which is\nappropriate for most uses. Setting it to 1 prevents any\nreordering of explicit\nJOIN\ns.\nThus, the explicit join order specified in the query will\nbe the actual order in which the relations are joined.\nThe query planner does not always choose the optimal join\norder; advanced users can elect to temporarily set this\nvariable to 1, and then specify the join order they\ndesire explicitly. For more information see\nSection 14.3\n.\nPrev\nHome\nNext\nWrite Ahead\nLog\nUp\nError Reporting\nand Logging",
    "url": "https://www.postgresql.org/docs/8.3/runtime-config-query.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31792.538672
  },
  {
    "title": "2.2. Concepts",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 8.1\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPostgreSQL\n8.1.23 Documentation\nPrev\nFast Backward\nChapter 2. The\nSQL\nLanguage\nFast Forward\nNext\n2.2. Concepts\nPostgreSQL\nis a\nrelational database management system\n(\nRDBMS\n). That means it is a\nsystem for managing data stored in\nrelations\n. Relation is essentially a mathematical\nterm for\ntable\n. The notion of storing\ndata in tables is so commonplace today that it might seem\ninherently obvious, but there are a number of other ways of\norganizing databases. Files and directories on Unix-like\noperating systems form an example of a hierarchical database. A\nmore modern development is the object-oriented database.\nEach table is a named collection of\nrows\n. Each row of a given table has the same set\nof named\ncolumns\n, and each column is of\na specific data type. Whereas columns have a fixed order in each\nrow, it is important to remember that SQL does not guarantee the\norder of the rows within the table in any way (although they can\nbe explicitly sorted for display).\nTables are grouped into databases, and a collection\nof databases managed by a single\nPostgreSQL\nserver instance constitutes a\ndatabase\ncluster\n.\nPrev\nHome\nNext\nThe\nSQL\nLanguage\nUp\nCreating a New\nTable",
    "url": "https://www.postgresql.org/docs/8.1/tutorial-concepts.html",
    "source": "postgresql",
    "doc_type": "tutorial",
    "scraped_at": 31792.5995733
  },
  {
    "title": "PostgreSQL: Documentation: 12: 33.12. Notice Processing",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 12\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\n33.12. Notice Processing\nPrev\nUp\nChapter 33.\nlibpq\n- C Library\nHome\nNext\n33.12. Notice Processing\nNotice and warning messages generated by the server are not returned by the query execution functions, since they do not imply failure of the query. Instead they are passed to a notice handling function, and execution continues normally after the handler returns. The default notice handling function prints the message on\nstderr\n, but the application can override this behavior by supplying its own handling function.\nFor historical reasons, there are two levels of notice handling, called the notice receiver and notice processor. The default behavior is for the notice receiver to format the notice and pass a string to the notice processor for printing. However, an application that chooses to provide its own notice receiver will typically ignore the notice processor layer and just do all the work in the notice receiver.\nThe function\nPQsetNoticeReceiver\nsets or examines the current notice receiver for a connection object. Similarly,\nPQsetNoticeProcessor\nsets or examines the current notice processor.\ntypedef void (*PQnoticeReceiver) (void *arg, const PGresult *res);\nPQnoticeReceiver\nPQsetNoticeReceiver(PGconn *conn,\nPQnoticeReceiver proc,\nvoid *arg);\ntypedef void (*PQnoticeProcessor) (void *arg, const char *message);\nPQnoticeProcessor\nPQsetNoticeProcessor(PGconn *conn,\nPQnoticeProcessor proc,\nvoid *arg);\nEach of these functions returns the previous notice receiver or processor function pointer, and sets the new value. If you supply a null function pointer, no action is taken, but the current pointer is returned.\nWhen a notice or warning message is received from the server, or generated internally by\nlibpq\n, the notice receiver function is called. It is passed the message in the form of a\nPGRES_NONFATAL_ERROR\nPGresult\n. (This allows the receiver to extract individual fields using\nPQresultErrorField\n, or obtain a complete preformatted message using\nPQresultErrorMessage\nor\nPQresultVerboseErrorMessage\n.) The same void pointer passed to\nPQsetNoticeReceiver\nis also passed. (This pointer can be used to access application-specific state if needed.)\nThe default notice receiver simply extracts the message (using\nPQresultErrorMessage\n) and passes it to the notice processor.\nThe notice processor is responsible for handling a notice or warning message given in text form. It is passed the string text of the message (including a trailing newline), plus a void pointer that is the same one passed to\nPQsetNoticeProcessor\n. (This pointer can be used to access application-specific state if needed.)\nThe default notice processor is simply:\nstatic void\ndefaultNoticeProcessor(void *arg, const char *message)\n{\nfprintf(stderr, \"%s\", message);\n}\nOnce you have set a notice receiver or processor, you should expect that that function could be called as long as either the\nPGconn\nobject or\nPGresult\nobjects made from it exist. At creation of a\nPGresult\n, the\nPGconn\n's current notice handling pointers are copied into the\nPGresult\nfor possible use by functions like\nPQgetvalue\n.\nPrev\nUp\nNext\n33.11. Miscellaneous Functions\nHome\n33.13. Event System",
    "url": "https://www.postgresql.org/docs/12/libpq-notice-processing.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31792.7061386
  },
  {
    "title": "12.7. Configuration Example",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 9.2\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPostgreSQL 9.2.24 Documentation\nPrev\nUp\nChapter 12.\nFull Text Search\nNext\n12.7. Configuration Example\nA text search configuration specifies all options necessary to\ntransform a document into a\ntsvector\n: the\nparser to use to break text into tokens, and the dictionaries to\nuse to transform each token into a lexeme. Every call of\nto_tsvector\nor\nto_tsquery\nneeds a text search configuration to\nperform its processing. The configuration parameter\ndefault_text_search_config\nspecifies the name of the default configuration, which is the one\nused by text search functions if an explicit configuration\nparameter is omitted. It can be set in\npostgresql.conf\n, or set for an individual session\nusing the\nSET\ncommand.\nSeveral predefined text search configurations are available,\nand you can create custom configurations easily. To facilitate\nmanagement of text search objects, a set of\nSQL\ncommands is available, and there are\nseveral\npsql\ncommands that\ndisplay information about text search objects (\nSection 12.10\n).\nAs an example we will create a configuration\npg\n, starting by duplicating the built-in\nenglish\nconfiguration:\nCREATE TEXT SEARCH CONFIGURATION public.pg ( COPY = pg_catalog.english );\nWe will use a PostgreSQL-specific synonym list and store it in\n$SHAREDIR/tsearch_data/pg_dict.syn\n. The\nfile contents look like:\npostgres    pg\npgsql       pg\npostgresql  pg\nWe define the synonym dictionary like this:\nCREATE TEXT SEARCH DICTIONARY pg_dict (\nTEMPLATE = synonym,\nSYNONYMS = pg_dict\n);\nNext we register the\nIspell\ndictionary\nenglish_ispell\n, which has its\nown configuration files:\nCREATE TEXT SEARCH DICTIONARY english_ispell (\nTEMPLATE = ispell,\nDictFile = english,\nAffFile = english,\nStopWords = english\n);\nNow we can set up the mappings for words in configuration\npg\n:\nALTER TEXT SEARCH CONFIGURATION pg\nALTER MAPPING FOR asciiword, asciihword, hword_asciipart,\nword, hword, hword_part\nWITH pg_dict, english_ispell, english_stem;\nWe choose not to index or search some token types that the\nbuilt-in configuration does handle:\nALTER TEXT SEARCH CONFIGURATION pg\nDROP MAPPING FOR email, url, url_path, sfloat, float;\nNow we can test our configuration:\nSELECT * FROM ts_debug('public.pg', '\nPostgreSQL, the highly scalable, SQL compliant, open source object-relational\ndatabase management system, is now undergoing beta testing of the next\nversion of our software.\n');\nThe next step is to set the session to use the new\nconfiguration, which was created in the\npublic\nschema:\n=> \\dF\nList of text search configurations\nSchema  | Name | Description\n---------+------+-------------\npublic  | pg   |\nSET default_text_search_config = 'public.pg';\nSET\nSHOW default_text_search_config;\ndefault_text_search_config\n----------------------------\npublic.pg\nPrev\nHome\nNext\nDictionaries\nUp\nTesting and\nDebugging Text Search",
    "url": "https://www.postgresql.org/docs/9.2/textsearch-configuration.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31793.0166167
  },
  {
    "title": "PostgreSQL: Documentation: 17: ANALYZE",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\nANALYZE\nPrev\nUp\nSQL Commands\nHome\nNext\nANALYZE\nANALYZE — collect statistics about a database\nSynopsis\nANALYZE [ (\noption\n[, ...] ) ] [\ntable_and_columns\n[, ...] ]\nwhere\noption\ncan be one of:\nVERBOSE [\nboolean\n]\nSKIP_LOCKED [\nboolean\n]\nBUFFER_USAGE_LIMIT\nsize\nand\ntable_and_columns\nis:\ntable_name\n[ (\ncolumn_name\n[, ...] ) ]\nDescription\nANALYZE\ncollects statistics about the contents of tables in the database, and stores the results in the\npg_statistic\nsystem catalog. Subsequently, the query planner uses these statistics to help determine the most efficient execution plans for queries.\nWithout a\ntable_and_columns\nlist,\nANALYZE\nprocesses every table and materialized view in the current database that the current user has permission to analyze. With a list,\nANALYZE\nprocesses only those table(s). It is further possible to give a list of column names for a table, in which case only the statistics for those columns are collected.\nParameters\nVERBOSE\nEnables display of progress messages.\nSKIP_LOCKED\nSpecifies that\nANALYZE\nshould not wait for any conflicting locks to be released when beginning work on a relation: if a relation cannot be locked immediately without waiting, the relation is skipped. Note that even with this option,\nANALYZE\nmay still block when opening the relation's indexes or when acquiring sample rows from partitions, table inheritance children, and some types of foreign tables. Also, while\nANALYZE\nordinarily processes all partitions of specified partitioned tables, this option will cause\nANALYZE\nto skip all partitions if there is a conflicting lock on the partitioned table.\nBUFFER_USAGE_LIMIT\nSpecifies the\nBuffer Access Strategy\nring buffer size for\nANALYZE\n. This size is used to calculate the number of shared buffers which will be reused as part of this strategy.\n0\ndisables use of a\nBuffer Access Strategy\n. When this option is not specified,\nANALYZE\nuses the value from\nvacuum_buffer_usage_limit\n. Higher settings can allow\nANALYZE\nto run more quickly, but having too large a setting may cause too many other useful pages to be evicted from shared buffers. The minimum value is\n128 kB\nand the maximum value is\n16 GB\n.\nboolean\nSpecifies whether the selected option should be turned on or off. You can write\nTRUE\n,\nON\n, or\n1\nto enable the option, and\nFALSE\n,\nOFF\n, or\n0\nto disable it. The\nboolean\nvalue can also be omitted, in which case\nTRUE\nis assumed.\nsize\nSpecifies an amount of memory in kilobytes. Sizes may also be specified as a string containing the numerical size followed by any one of the following memory units:\nB\n(bytes),\nkB\n(kilobytes),\nMB\n(megabytes),\nGB\n(gigabytes), or\nTB\n(terabytes).\ntable_name\nThe name (possibly schema-qualified) of a specific table to analyze. If omitted, all regular tables, partitioned tables, and materialized views in the current database are analyzed (but not foreign tables). If the specified table is a partitioned table, both the inheritance statistics of the partitioned table as a whole and statistics of the individual partitions are updated.\ncolumn_name\nThe name of a specific column to analyze. Defaults to all columns.\nOutputs\nWhen\nVERBOSE\nis specified,\nANALYZE\nemits progress messages to indicate which table is currently being processed. Various statistics about the tables are printed as well.\nNotes\nTo analyze a table, one must ordinarily have the\nMAINTAIN\nprivilege on the table. However, database owners are allowed to analyze all tables in their databases, except shared catalogs.\nANALYZE\nwill skip over any tables that the calling user does not have permission to analyze.\nForeign tables are analyzed only when explicitly selected. Not all foreign data wrappers support\nANALYZE\n. If the table's wrapper does not support\nANALYZE\n, the command prints a warning and does nothing.\nIn the default\nPostgreSQL\nconfiguration, the autovacuum daemon (see\nSection 24.1.6\n) takes care of automatic analyzing of tables when they are first loaded with data, and as they change throughout regular operation. When autovacuum is disabled, it is a good idea to run\nANALYZE\nperiodically, or just after making major changes in the contents of a table. Accurate statistics will help the planner to choose the most appropriate query plan, and thereby improve the speed of query processing. A common strategy for read-mostly databases is to run\nVACUUM\nand\nANALYZE\nonce a day during a low-usage time of day. (This will not be sufficient if there is heavy update activity.)\nWhile\nANALYZE\nis running, the\nsearch_path\nis temporarily changed to\npg_catalog, pg_temp\n.\nANALYZE\nrequires only a read lock on the target table, so it can run in parallel with other non-DDL activity on the table.\nThe statistics collected by\nANALYZE\nusually include a list of some of the most common values in each column and a histogram showing the approximate data distribution in each column. One or both of these can be omitted if\nANALYZE\ndeems them uninteresting (for example, in a unique-key column, there are no common values) or if the column data type does not support the appropriate operators. There is more information about the statistics in\nChapter 24\n.\nFor large tables,\nANALYZE\ntakes a random sample of the table contents, rather than examining every row. This allows even very large tables to be analyzed in a small amount of time. Note, however, that the statistics are only approximate, and will change slightly each time\nANALYZE\nis run, even if the actual table contents did not change. This might result in small changes in the planner's estimated costs shown by\nEXPLAIN\n. In rare situations, this non-determinism will cause the planner's choices of query plans to change after\nANALYZE\nis run. To avoid this, raise the amount of statistics collected by\nANALYZE\n, as described below.\nThe extent of analysis can be controlled by adjusting the\ndefault_statistics_target\nconfiguration variable, or on a column-by-column basis by setting the per-column statistics target with\nALTER TABLE ... ALTER COLUMN ... SET STATISTICS\n. The target value sets the maximum number of entries in the most-common-value list and the maximum number of bins in the histogram. The default target value is 100, but this can be adjusted up or down to trade off accuracy of planner estimates against the time taken for\nANALYZE\nand the amount of space occupied in\npg_statistic\n. In particular, setting the statistics target to zero disables collection of statistics for that column. It might be useful to do that for columns that are never used as part of the\nWHERE\n,\nGROUP BY\n, or\nORDER BY\nclauses of queries, since the planner will have no use for statistics on such columns.\nThe largest statistics target among the columns being analyzed determines the number of table rows sampled to prepare the statistics. Increasing the target causes a proportional increase in the time and space needed to do\nANALYZE\n.\nOne of the values estimated by\nANALYZE\nis the number of distinct values that appear in each column. Because only a subset of the rows are examined, this estimate can sometimes be quite inaccurate, even with the largest possible statistics target. If this inaccuracy leads to bad query plans, a more accurate value can be determined manually and then installed with\nALTER TABLE ... ALTER COLUMN ... SET (n_distinct = ...)\n.\nIf the table being analyzed has inheritance children,\nANALYZE\ngathers two sets of statistics: one on the rows of the parent table only, and a second including rows of both the parent table and all of its children. This second set of statistics is needed when planning queries that process the inheritance tree as a whole. The child tables themselves are not individually analyzed in this case. The autovacuum daemon, however, will only consider inserts or updates on the parent table itself when deciding whether to trigger an automatic analyze for that table. If that table is rarely inserted into or updated, the inheritance statistics will not be up to date unless you run\nANALYZE\nmanually.\nFor partitioned tables,\nANALYZE\ngathers statistics by sampling rows from all partitions; in addition, it will recurse into each partition and update its statistics. Each leaf partition is analyzed only once, even with multi-level partitioning. No statistics are collected for only the parent table (without data from its partitions), because with partitioning it's guaranteed to be empty.\nThe autovacuum daemon does not process partitioned tables, nor does it process inheritance parents if only the children are ever modified. It is usually necessary to periodically run a manual\nANALYZE\nto keep the statistics of the table hierarchy up to date.\nIf any child tables or partitions are foreign tables whose foreign data wrappers do not support\nANALYZE\n, those tables are ignored while gathering inheritance statistics.\nIf the table being analyzed is completely empty,\nANALYZE\nwill not record new statistics for that table. Any existing statistics will be retained.\nEach backend running\nANALYZE\nwill report its progress in the\npg_stat_progress_analyze\nview. See\nSection 27.4.1\nfor details.\nCompatibility\nThere is no\nANALYZE\nstatement in the SQL standard.\nThe following syntax was used before\nPostgreSQL\nversion 11 and is still supported:\nANALYZE [ VERBOSE ] [\ntable_and_columns\n[, ...] ]\nSee Also\nVACUUM\n,\nvacuumdb\n,\nSection 19.4.4\n,\nSection 24.1.6\n,\nSection 27.4.1\nPrev\nUp\nNext\nALTER VIEW\nHome\nBEGIN\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/sql-analyze.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31793.1114184
  },
  {
    "title": "PostgreSQL: Documentation: 18: 2.9. Deletions",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 18\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\n2.9. Deletions\nPrev\nUp\nChapter 2. The\nSQL\nLanguage\nHome\nNext\n2.9. Deletions\n#\nRows can be removed from a table using the\nDELETE\ncommand. Suppose you are no longer interested in the weather of Hayward. Then you can do the following to delete those rows from the table:\nDELETE FROM weather WHERE city = 'Hayward';\nAll weather records belonging to Hayward are removed.\nSELECT * FROM weather;\ncity      | temp_lo | temp_hi | prcp |    date\n---------------+---------+---------+------+------------\nSan Francisco |      46 |      50 | 0.25 | 1994-11-27\nSan Francisco |      41 |      55 |    0 | 1994-11-29\n(2 rows)\nOne should be wary of statements of the form\nDELETE FROM\ntablename\n;\nWithout a qualification,\nDELETE\nwill remove\nall\nrows from the given table, leaving it empty. The system will not request confirmation before doing this!\nPrev\nUp\nNext\n2.8. Updates\nHome\nChapter 3. Advanced Features",
    "url": "https://www.postgresql.org/docs/18/tutorial-delete.html",
    "source": "postgresql",
    "doc_type": "tutorial",
    "scraped_at": 31793.4191978
  },
  {
    "title": "20.3. Role Membership",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 9.3\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\nThis documentation is for an unsupported version of PostgreSQL.\nYou may want to view the same page for the\ncurrent\nversion, or one of the other supported versions listed above instead.\nPostgreSQL 9.3.25 Documentation\nPrev\nUp\nChapter 20. Database\nRoles\nNext\n20.3. Role Membership\nIt is frequently convenient to group users together to ease\nmanagement of privileges: that way, privileges can be granted to,\nor revoked from, a group as a whole. In\nPostgreSQL\nthis is done by creating a role\nthat represents the group, and then granting\nmembership\nin the group role to individual user\nroles.\nTo set up a group role, first create the role:\nCREATE ROLE\nname\n;\nTypically a role being used as a group would not have the\nLOGIN\nattribute, though you can set it if\nyou wish.\nOnce the group role exists, you can add and remove members using\nthe\nGRANT\nand\nREVOKE\ncommands:\nGRANT\ngroup_role\nTO\nrole1\n, ... ;\nREVOKE\ngroup_role\nFROM\nrole1\n, ... ;\nYou can grant membership to other group roles, too (since there\nisn't really any distinction between group roles and non-group\nroles). The database will not let you set up circular membership\nloops. Also, it is not permitted to grant membership in a role to\nPUBLIC\n.\nThe members of a group role can use the privileges of the role\nin two ways. First, every member of a group can explicitly do\nSET ROLE\nto temporarily\n\"become\"\nthe group role. In this state,\nthe database session has access to the privileges of the group role\nrather than the original login role, and any database objects\ncreated are considered owned by the group role not the login role.\nSecond, member roles that have the\nINHERIT\nattribute automatically have use of the privileges of roles of\nwhich they are members, including any privileges inherited by those\nroles. As an example, suppose we have done:\nCREATE ROLE joe LOGIN INHERIT;\nCREATE ROLE admin NOINHERIT;\nCREATE ROLE wheel NOINHERIT;\nGRANT admin TO joe;\nGRANT wheel TO admin;\nImmediately after connecting as role\njoe\n, a database session will have use of privileges\ngranted directly to\njoe\nplus any\nprivileges granted to\nadmin\n, because\njoe\n\"inherits\"\nadmin\n's privileges. However, privileges\ngranted to\nwheel\nare not available,\nbecause even though\njoe\nis indirectly a\nmember of\nwheel\n, the membership is via\nadmin\nwhich has the\nNOINHERIT\nattribute. After:\nSET ROLE admin;\nthe session would have use of only those privileges granted to\nadmin\n, and not those granted to\njoe\n. After:\nSET ROLE wheel;\nthe session would have use of only those privileges granted to\nwheel\n, and not those granted to either\njoe\nor\nadmin\n. The\noriginal privilege state can be restored with any of:\nSET ROLE joe;\nSET ROLE NONE;\nRESET ROLE;\nNote:\nThe\nSET ROLE\ncommand\nalways allows selecting any role that the original login role is\ndirectly or indirectly a member of. Thus, in the above example, it\nis not necessary to become\nadmin\nbefore\nbecoming\nwheel\n.\nNote:\nIn the SQL standard, there is a clear distinction\nbetween users and roles, and users do not automatically inherit\nprivileges while roles do. This behavior can be obtained in\nPostgreSQL\nby giving roles being\nused as SQL roles the\nINHERIT\nattribute,\nwhile giving roles being used as SQL users the\nNOINHERIT\nattribute. However,\nPostgreSQL\ndefaults to giving all roles the\nINHERIT\nattribute, for backward\ncompatibility with pre-8.1 releases in which users always had use\nof permissions granted to groups they were members of.\nThe role attributes\nLOGIN\n,\nSUPERUSER\n,\nCREATEDB\n, and\nCREATEROLE\ncan be thought of as special\nprivileges, but they are never inherited as ordinary privileges on\ndatabase objects are. You must actually\nSET\nROLE\nto a specific role having one of these attributes in\norder to make use of the attribute. Continuing the above example,\nwe might choose to grant\nCREATEDB\nand\nCREATEROLE\nto the\nadmin\nrole. Then a session connecting as role\njoe\nwould not have these privileges\nimmediately, only after doing\nSET ROLE\nadmin\n.\nTo destroy a group role, use\nDROP\nROLE\n:\nDROP ROLE\nname\n;\nAny memberships in the group role are automatically revoked (but\nthe member roles are not otherwise affected).\nPrev\nHome\nNext\nRole Attributes\nUp\nDropping Roles",
    "url": "https://www.postgresql.org/docs/9.3/role-membership.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31793.7672059
  },
  {
    "title": "PostgreSQL: Documentation: 17: 2.9. Deletions",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n/\n9.0\n/\n8.4\n/\n8.3\n/\n8.2\n/\n8.1\n/\n8.0\n/\n7.4\n/\n7.3\n/\n7.2\n2.9. Deletions\nPrev\nUp\nChapter 2. The\nSQL\nLanguage\nHome\nNext\n2.9. Deletions\n#\nRows can be removed from a table using the\nDELETE\ncommand. Suppose you are no longer interested in the weather of Hayward. Then you can do the following to delete those rows from the table:\nDELETE FROM weather WHERE city = 'Hayward';\nAll weather records belonging to Hayward are removed.\nSELECT * FROM weather;\ncity      | temp_lo | temp_hi | prcp |    date\n---------------+---------+---------+------+------------\nSan Francisco |      46 |      50 | 0.25 | 1994-11-27\nSan Francisco |      41 |      55 |    0 | 1994-11-29\n(2 rows)\nOne should be wary of statements of the form\nDELETE FROM\ntablename\n;\nWithout a qualification,\nDELETE\nwill remove\nall\nrows from the given table, leaving it empty. The system will not request confirmation before doing this!\nPrev\nUp\nNext\n2.8. Updates\nHome\nChapter 3. Advanced Features\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/17/tutorial-delete.html",
    "source": "postgresql",
    "doc_type": "tutorial",
    "scraped_at": 31794.0390956
  },
  {
    "title": "PostgreSQL: Documentation: 17: 21.4. Dropping Roles",
    "content": "Jan 17, 2025:\n|\nPostgreSQL 18 Beta 2 Released!\nDocumentation\n→\nPostgreSQL 17\nSupported Versions:\nCurrent\n(\n17\n)\n/\n16\n/\n15\n/\n14\n/\n13\nDevelopment Versions:\n18\n/\ndevel\nUnsupported versions:\n12\n/\n11\n/\n10\n/\n9.6\n/\n9.5\n/\n9.4\n/\n9.3\n/\n9.2\n/\n9.1\n21.4. Dropping Roles\nPrev\nUp\nChapter 21. Database Roles\nHome\nNext\n21.4. Dropping Roles\n#\nBecause roles can own database objects and can hold privileges to access other objects, dropping a role is often not just a matter of a quick\nDROP ROLE\n. Any objects owned by the role must first be dropped or reassigned to other owners; and any permissions granted to the role must be revoked.\nOwnership of objects can be transferred one at a time using\nALTER\ncommands, for example:\nALTER TABLE bobs_table OWNER TO alice;\nAlternatively, the\nREASSIGN OWNED\ncommand can be used to reassign ownership of all objects owned by the role-to-be-dropped to a single other role. Because\nREASSIGN OWNED\ncannot access objects in other databases, it is necessary to run it in each database that contains objects owned by the role. (Note that the first such\nREASSIGN OWNED\nwill change the ownership of any shared-across-databases objects, that is databases or tablespaces, that are owned by the role-to-be-dropped.)\nOnce any valuable objects have been transferred to new owners, any remaining objects owned by the role-to-be-dropped can be dropped with the\nDROP OWNED\ncommand. Again, this command cannot access objects in other databases, so it is necessary to run it in each database that contains objects owned by the role. Also,\nDROP OWNED\nwill not drop entire databases or tablespaces, so it is necessary to do that manually if the role owns any databases or tablespaces that have not been transferred to new owners.\nDROP OWNED\nalso takes care of removing any privileges granted to the target role for objects that do not belong to it. Because\nREASSIGN OWNED\ndoes not touch such objects, it's typically necessary to run both\nREASSIGN OWNED\nand\nDROP OWNED\n(in that order!) to fully remove the dependencies of a role to be dropped.\nIn short then, the most general recipe for removing a role that has been used to own objects is:\nREASSIGN OWNED BY doomed_role TO successor_role;\nDROP OWNED BY doomed_role;\n-- repeat the above commands in each database of the cluster\nDROP ROLE doomed_role;\nWhen not all owned objects are to be transferred to the same successor owner, it's best to handle the exceptions manually and then perform the above steps to mop up.\nIf\nDROP ROLE\nis attempted while dependent objects still remain, it will issue messages identifying which objects need to be reassigned or dropped.\nPrev\nUp\nNext\n21.3. Role Membership\nHome\n21.5. Predefined Roles\nSubmit correction\nIf you see anything in the documentation that is not correct, does not match\nyour experience with the particular feature or requires further clarification,\nplease use\nthis form\nto report a documentation issue.",
    "url": "https://www.postgresql.org/docs/current/role-removal.html",
    "source": "postgresql",
    "doc_type": "sql_reference",
    "scraped_at": 31794.4219453
  },
  {
    "title": "What is MongoDB Compass?",
    "content": "What is MongoDB Compass?\nMongoDB Compass is a powerful GUI for querying, aggregating, and analyzing your MongoDB data in a visual environment.\nCompass is free to use and source available, and can be run on macOS, Windows, and Linux.\nDownload Compass\nView installation instructions\nWhat You Can Do\nVisually Explore Your Data\nExplore some of the tasks Compass can help you accomplish, such as importing and managing data from an easy-to-navigate interface.\nImport your data\nQuery your data\nCreate aggregation pipelines\nRun commands in the shell\n1\nConnect to your deployment\nConnect to a MongoDB deployment hosted on MongoDB Atlas, or a deployment hosted locally on your own machine.\nTo learn more, see Connect to MongoDB\n2\nImport your data\nImport data from CSV or JSON files into your MongoDB database.\nTo learn more, see Import and Export Data\n1\nInsert documents into your collections\nPaste documents into the JSON view, or manually insert documents using a field-by-field editor.\nTo learn more, see Insert Documents\n2\nQuery your data\nWrite ad-hoc queries or generate queries with the help of AI\nto filter your data. Explore trends and commonalities in\nyour collections.\nTo learn more, see:\nQuery Your Data\nQuery with Natural Language\n1\nInsert documents into your collections\nInsert documents into your collections in two ways, JSON Mode and a Field-by-Field Editor.\nTo learn more, see Insert Documents\n2\nCreate aggregation pipelines\nWrite aggregation pipelines that allow documents in a collection or view to pass through multiple stages where they are processed into a set of aggregated results.\nTo learn more, see Aggregation Pipeline Builder\n1\nConnect to your deployment\nConnect to a MongoDB deployment hosted on MongoDB Atlas, or a deployment hosted locally on your own machine.\nTo learn more, see Connect to MongoDB\n2\nWork with your data in the MongoDB Shell\nUse the embedded MongoDB Shell in Compass to control your data in an interactive JavaScript environment.\nTo learn more, see Embedded MongoDB Shell\nRelated Products and Resources\nGo Further with Compass\nExpand your knowledge of MongoDB by using Compass with other MongoDB products.\nUse Compass to connect to your Atlas cluster\nRead Atlas docs\nLearn MongoDB Basics with MongoDB University\nLearn MongoDB Basics\nAccess more in-depth examples of querying data\nRead Server docs\nRate this page",
    "url": "https://www.mongodb.com/docs/compass/",
    "source": "mongodb",
    "doc_type": "compass",
    "scraped_at": 31795.2584844
  },
  {
    "title": "What is MongoDB?",
    "content": "What is MongoDB?\nMongoDB is a document database designed for ease of application\ndevelopment and scaling.\nYou can run MongoDB in the following environments:\nMongoDB Atlas\n: The fully\nmanaged service for MongoDB deployments in the cloud\nMongoDB Enterprise\n: The\nsubscription-based, self-managed version of MongoDB\nMongoDB Community\n: The\nsource-available, free-to-use, and self-managed version of MongoDB\nGet started with MongoDB Atlas\nWhat You Can Do\nWork with your data in MongoDB\nStore and query your data\nTransform data with Aggregations\nSecure access to your data\nDeploy and scale your database\n1\nDeploy MongoDB\nCreate a cluster in the MongoDB Atlas UI or the Atlas CLI\nquickly and easily. To learn more, see\nCreate a Cluster\nin the MongoDB Atlas documentation\nand\nGet Started with Atlas\nin the Atlas CLI\ndocumentation.\nFor self-hosted deployments,\nsee\nReplication\nin the MongoDB manual\nto create a replica\nset.\n2\nConnect to your deployment\nAccess deployments in the\nMongoDB Atlas UI or connect with\ndrivers\nor the\nMongoDB Shell (mongosh)\nin the MongoDB\nmanual.\nTo learn more, see\nFind Your Connection String\nin the MongoDB manual.\n3\nInsert, query, update, or delete documents\nPerform CRUD operations in the MongoDB Atlas UI or by using the\nMongoDB Query API - with or without transactions.\nTo learn more, see\nCreate, View, Update, and Delete Documents\nin the MongoDB Atlas documentation and\nMongoDB CRUD Operations\nin the MongoDB manual.\n4\nModel your data\nDesign your data schema to support frequent access patterns.\nYou can update or enforce your schema at any point.\nTo learn more, see\nData Modeling Introduction\nin the MongoDB manual.\n➜ atlas setup\n?\nDo\nyou want to setup your\nAtlas\ndatabase\nwith\ndefault\nsettings\n?\n(\nY\n/\nn)\n➜ Y\nWe\nare deploying\nCluster9876543\n...\nPlease\nstore your database authentication access details\nin\na secure location.\nDatabase\nUser\nUsername\n:\nCluster9876543\nDatabase\nUser\nPassword\n:\nabcdef12345\nCreating\nyour cluster\n...\n[\nIts\nsafe to\n'Ctrl + C'\n]\n1\nImport your data\nImport data from a CSV or JSON file with database tools.\nTo learn more, see\nMigrate or Import Data\nin the MongoDB Atlas\ndocumentation and\nmongoimport\nin the database tools documentation.\n2\nAggregate your data\nUse aggregation pipelines to process your data in multiple\nstages and return the computed results. You can\npreview the results at each pipeline stage when you\nrun aggregation pipelines in MongoDB Atlas.\nTo learn more, see\nRun Aggregation Pipelines\nin the MongoDB Atlas documentation\nand\nAggregation Operations\nin the MongoDB manual.\ntest\n>\ndb.\norders\n.\ninsertMany\n(\n[\n{\n\"item\"\n:\n\"almonds\"\n,\n\"price\"\n:\n12\n,\n\"quantity\"\n:\n2\n}\n,\n{\n\"item\"\n:\n\"pecans\"\n,\n\"price\"\n:\n20\n,\n\"quantity\"\n:\n1\n}\n,\n])\ntest\n>\ndb.\ninventory\n.\ninsertMany\n(\n[\n{\n\"sku\"\n:\n\"almonds\"\n,\n\"description\"\n:\n\"product 1\"\n,\n\"instock\"\n:\n120\n}\n,\n{\n\"sku\"\n:\n\"cashews\"\n,\n\"description\"\n:\n\"product 3\"\n,\n\"instock\"\n:\n60\n}\n,\n{\n\"sku\"\n:\n\"pecans\"\n,\n\"description\"\n:\n\"product 4\"\n,\n\"instock\"\n:\n70\n}\n])\ntest\n>\ndb.\norders\n.\naggregate\n(\n[\n{\n$match\n:\n{\nprice\n:\n{\n$lt\n:\n15\n} } }\n,\n{\n$lookup\n:\n{\nfrom\n:\n\"inventory\"\n,\nlocalField\n:\n\"item\"\n,\nforeignField\n:\n\"sku\"\n,\nas\n:\n\"inventory_docs\"\n} }\n,\n{\n$sort\n:\n{\nprice\n:\n1\n} }\n,\n])\n1\nAuthenticate a client\nVerify the identity of a user, replica set member, or\nsharded cluster member with authentication.\nTo learn more, see\nAtlas UI Authenication\nin the MongoDB Atlas documentation\nand\nAuthentication\nin the MongoDB\nmanual.\n2\nControl access to your database\nEnable Role-Based Access Controls to manage user privileges\non your entire database cluster or individual collections.\nTo learn more, see\nAtlas UI Authorization\nin the MongoDB Atlas documentation\nand\nRole-Based Access Controls\nin the MongoDB manual.\n3\nEncrypt your most sensitive data\nClient-Side Field Level Encryption protects data while it is\nin-use by the database. Fields are encrypted before they\nleave your application, protecting them over the network, in\nmemory and at rest.\nTo learn more, see\nClient-Side Field Level Encryption\nin the MongoDB manual.\n1\nCreate a cluster\nCreate a free cluster, an auto-scaling cluster, or a\nserverless instance in the MongoDB Atlas UI. To learn\nmore, see\nChoose a Cluster Type\nin the MongoDB Atlas\ndocumentation.\nFor self-hosted deployments, provide redundancy and\nresilience for your database by deploying a replica set. To\nlearn more, see\nReplication\nin the\nMongoDB manual.\n2\nScale out your database\nUse sharding to horizontally scale your database or to\nensure location-based separation of data.\nTo learn more, see\nShard a Collection\nin the MongoDB Atlas\ndocumentation and\nSharding\nin the MongoDB manual.\nRelated Products & Resources\nGo Further with MongoDB\nExplore libraries and tools for MongoDB.\nUse MongoDB in your application's language\nLearn about Drivers\nVisually explore your data with MongoDB Compass\nView Compass Docs\nManage and monitor your deployments\nView Ops Manager\nRate this page",
    "url": "https://www.mongodb.com/docs/manual/",
    "source": "mongodb",
    "doc_type": "manual",
    "scraped_at": 31795.5576794
  },
  {
    "title": "MongoDB Python Drivers",
    "content": "Docs Home\n/\nDrivers\nMongoDB Python Drivers\nCopy page\nIntroduction\nWelcome to the documentation site for the official MongoDB Python\ndrivers. Add one of the following drivers to your application to work with\nMongoDB in Python.\nPyMongo\nYou can add PyMongo to your application to work with MongoDB in Python. See\nthe\nPyMongo documentation\nto learn how to install and begin using the driver.\nPyMongoArrow\nis a PyMongo extension for loading MongoDB query result sets as Apache Arrow tables.\nImportant\nMotor Deprecation\nAs of May 14, 2025, Motor is deprecated in favor of the GA release of the PyMongo Async\nAPI in the PyMongo library. We\nstrongly recommend migrating to the PyMongo Async API while Motor is still supported.\nFor more information about migrating, see the\nMigrate to PyMongo Async\nguide in the PyMongo documentation.\nTools and Projects\nThe\nFull Stack FastAPI App Generator\nsimplifies setup of web applications that use the FastAPI, React, and MongoDB\n(FARM) stack. You can learn more about this tool on the\nMongoDB blog.\nImportant\nThe MongoDB Full Stack FastAPI App Generator is an experimental project\nand is not yet supported by MongoDB.\nTake the Free Online Course Taught by MongoDB\nUsing MongoDB with Python\nLearn the essentials of Python application development with MongoDB.\nBack\nLibraries, Frameworks, & Tools\nNext\nMotor (Async Driver)",
    "url": "https://www.mongodb.com/docs/drivers/python/",
    "source": "mongodb",
    "doc_type": "driver",
    "scraped_at": 31795.8067988
  },
  {
    "title": "What is MongoDB Atlas?",
    "content": "What is\nMongoDB Atlas\n?\nMongoDB Atlas\nis a multi-cloud database service by the same people\nthat build MongoDB.\nAtlas\nsimplifies deploying and managing your\ndatabases while offering the versatility you need to build resilient\nand performant global applications on the cloud providers of your choice.\nGet Started\nWhat You Can Do\nBuild with MongoDB on AWS, Azure, and Google Cloud\nMongoDB Atlas\nmakes it easy to deploy and manage databases\non-demand when and where you need them.\nDeploy a Database\nSecure your Database\nConnect to your Database\nOptimize your Database\n1\nChoose a cluster type.\nPlay around with a free cluster, launch a serverless instance, or define a dedicated cluster\nconfiguration for your application.\nTo choose a deployment type, see\nDatabase Deployment Types\n.\n2\nChoose a Cloud Provider and Region\nDeploy your database to the same cloud provider and region as\nyour applications to reduce latency and standardize security\ncontrols.\nTo choose a cloud provider and region, see\nCloud Providers and Regions\n.\n3\nCustomize your cluster.\nEnable multi-cloud and multi-region data distribution to\nexpand global coverage, increase fault tolerance, and meet\ndata compliance requirements.\nTo customize your cluster, see\nConfigure High Availability and Workload Isolation\n.\n1\nAdd IP Access List Entries\nDefine an IP access list for your cluster.\nAlways-on authentication ensures only approved client\nconnections can access your database.\nTo add IP Access List entries, see\nConfigure IP Access List Entries\n.\n2\nManage Database Users\nDefine how your team members and applications authenticate\nto your database and what data they can access.\nTo manage database users, see\nConfigure Database Users\n.\n3\n(Optional) Configure Private Network Access\nConfigure peering connections or private endpoints for your\napplications to connect to your databases for additional\nsecurity controls.\nTo configure peering connections, see\nSet Up a Network Peering Connection\n.\nTo configure private endpoints, see\nConfigure Private Endpoints\n.\n1\nChoose a Connection Type\nConnect to your database using the MongoDB Shell, one of\nMongoDB's native language drivers, MongoDB Compass, or the\nMongoDB Connector for BI.\nTo connect to your database, see\nConnect to a Cluster\n.\n2\nInteract with your Data\nUse your chosen connection type to view your data, import\ndocuments, and run queries.\nTo insert data, see\nInsert and View a Document\n.\n1\nCreate Custom Alerts\nCustomize default alerts or create new ones to receive\nnotifications based on events and metrics that you define.\nTo create custom alerts, see\nConfigure and Resolve Alerts\n.\n2\nReview Index and Schema Suggestions\nSee on-demand index and schema suggestions for your\ncollections to improve your application's query performance.\nTo review suggestions, see\nAnalyze Slow Queries\nand\nImprove Your Schema\n.\n3\nAutomate Archival of Cold Data\nDefine archival rules to move infrequently accessed data\ninto fully managed cloud object storage while retaining full\nqueryability of that data.\nTo archive your data, see\nOnline Archive Overview\n.\nRelated Products & Resources\nGo Further with\nAtlas\nExplore other data services available with\nAtlas\n.\nBuild full-text search on top of your data.\nLearn about Atlas Search\nIntegrate\nAtlas\ninto your workflows.\nExplore integrations\nRate this page",
    "url": "https://www.mongodb.com/docs/atlas/",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31796.3122738
  },
  {
    "title": "Start with Guides",
    "content": "Start with Guides\nDiscover step-by-step guides to help you complete essential tasks to\nget started with MongoDB.\nGet Started\nChapter\nView\nGallery\nView\nChapters\nAtlas\nCRUD\nAtlas Search\nChapters\nAtlas\nCRUD\nAtlas Search\nStill Learning MongoDB?\nExplore these resources to learn some fundamental MongoDB concepts.\nTake M001 MongoDB Basics →\nChapter\n1\nAtlas\nGet up and running with MongoDB Atlas.\nLearn how to create a cluster, load a sample dataset, and\nget your connection information.\nSign Up for a MongoDB Account\n5 mins\nCreate a Cluster\n3 mins\nAdd a Database User\n2 mins\nConfigure a Network Connection\n2 mins\nLoad Sample Data\n5 mins\nGet Connection String\n3 mins\nChapter\n2\nCRUD\nLearn how to perform these core operations with\nyour preferred language and associated MongoDB Driver.\nAdd a MongoDB Driver\n5 mins\nRead Data in MongoDB\n10 mins\nRead Data from MongoDB With Queries\n15 mins\nRead Data using Operators and Compound Queries\n20 mins\nInsert Data into MongoDB\n15 mins\nUpdate Data in MongoDB\n10 mins\nDelete Data from MongoDB\n10 mins\nChapter\n3\nAtlas Search\nLearn how to create Atlas Search indexes\nand run queries to perform relevance-based search on your\ndata sets.\nBuild a Dynamic Index\n5 mins\nBuild an Index with Static Field Mappings\n5 mins\nQuerying with the Compound Operator\n5 mins\nQuerying with Facets\n10 mins\nRate this page",
    "url": "https://www.mongodb.com/docs/guides/",
    "source": "mongodb",
    "doc_type": "general",
    "scraped_at": 31796.7158854
  },
  {
    "title": "MongoDB C++ Driver",
    "content": "Docs Home\n/\nLanguages\n/\nC++\nMongoDB C++ Driver\nCopy page\nOverview\nWelcome to the documentation site for the official MongoDB C++ Driver.\nGet Started\nLearn how to install the driver, establish a connection to MongoDB, and begin\nworking with data in the\nGet Started with the C++ Driver\ntutorial.\nConnect to MongoDB\nLearn how to create and configure a connection to a MongoDB deployment\nin the\nConnect to MongoDB\nsection.\nRead Data\nLearn how you can retrieve data from MongoDB in the\nRead Data\nsection.\nWrite Data to MongoDB\nLearn how you can write data to MongoDB in the\nWrite Data to MongoDB\nsection.\nDatabases and Collections\nLearn how to use the C++ driver to work with MongoDB databases and collections in the\nDatabases and Collections\nsection.\nOptimize Queries with Indexes\nLearn how to work with common types of indexes in the\nOptimize Queries with Indexes\nsection.\nTransform Your Data with Aggregation\nLearn how to use the C++ driver to perform aggregation operations in the\nTransform Your Data with Aggregation\nsection.\nSecure Your Data\nLearn about ways you can authenticate your application and encrypt your data in\nthe\nSecure Your Data\nsection.\nSpecialized Data Formats\nLearn how to work with specialized data formats and custom types in the\nSpecialized Data Formats\nsection.\nAdvanced Installation Options\nLearn about advanced configuration and installation options\nin the\nAdvanced Configuration and Installation Options\nsection.\nInclude and Link the Driver in Your Program\nLearn how to include and link the driver in your program\nin the\nInclude and Link the Driver in Your Program\nsection.\nWhat's New\nFor a list of new features and changes in each version, see the\nWhat's New\nsection.\nUpgrade Driver Versions\nLearn what changes you might need to make to your application to upgrade driver versions\nin the\nUpgrade Driver Versions\nsection.\nIssues & Help\nLearn how to report bugs, contribute to the driver, and find help in the\nIssues & Help\nsection.\nCompatibility\nFor compatibility tables that show the recommended C++ driver\nversion to use for specific C++ and MongoDB Server versions, see the\nCompatibility\nsection.\nAPI Documentation\nFor detailed information about types and methods in the C++ driver, see\nthe\nC++ driver API documentation\n.\nC++17 Polyfill Configuration\nImportant\nWe recommend using the C++ standard library whenever possible by setting the\nCMAKE_CXX_STANDARD\nconfiguration option to\n17\nor newer.\nThe MongoDB C++ Driver uses C++17 features\nstd::optional<T>\nand\nstd::string_view\n.\nIf you configure the driver with a pre-C++17 standard, the bsoncxx library provides\npolyfill implementations for these C++17 features. The driver uses the bsoncxx polyfill\nimplementations when the\nCMAKE_CXX_STANDARD\nconfiguration option is set to a number\nless than\n17\n. By default, this option is set to\n11\n.\nWarning\nThe choice of polyfill library has a direct impact on the public API and ABI for the mongocxx\nlibrary. Changing the polyfill can lead to both source-breaking changes during compilation\nand binary-breaking changes during linking or execution. To limit reliance on polyfill-specific\nbehavior, avoid using\nstdx::string_view\nand\nstdx::optional<T>\nwith non-bsoncxx and non-mongocxx\nlibrary interfaces.\nDriver Status by Family and Version\nStability indicates whether this driver is recommended for production use.\nCurrently, no drivers guarantee API or ABI stability.\nFor documentation about previous releases, see the\nlegacy documentation\n.\nFamily/version\nStability\nDevelopment\nPurpose\n(repo master branch)\nUnstable\nActive development\nNew feature development\nmongocxx 4.1.x\nStable\nBug fixes only\nCurrent stable C++ driver release\nmongocxx 4.0.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.11.x\nStable\nBug fixes until November 2025\nPrevious stable C++ driver release\nmongocxx 3.10.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.9.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.8.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.7.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.6.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.5.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.4.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.3.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.2.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.1.x\nStable\nNone\nPrevious stable C++ driver release\nmongocxx 3.0.x\nStable\nNone\nPrevious stable C++ driver release  |\nCurrent Driver\nThe mongocxx is a ground-up rewrite of a C++ driver for MongoDB based on\nlibmongoc\n.  It requires a C++11 compiler.  It is\nknown to build on x86 and x86-64 architectures for Linux, macOS,\nWindows, and FreeBSD.\nThe mongocxx driver library includes a matching bson package, bsoncxx, that\nimplements the\nBSON specification\n. This\nlibrary can be used standalone for object serialization and deserialization\neven when one is not using MongoDB at all.\nReleases of the mongocxx driver have version numbers like v3.x.y.\nNote\nThere were no v2.x.y C++ drivers to avoid confusion with the deprecated legacy-0.0-26compat-2.x.y drivers.\nHow to get help\nAsk questions on our\nMongoDB Community Forums\n.\nVisit our\nSupport Channels\n.\nSee how to\nreport a bug\n.\nLicense\nMongoDB C++ drivers are available under the terms of the Apache License, version 2.0.\nNext\nGet Started",
    "url": "https://www.mongodb.com/docs/languages/cpp/cpp-driver/current/",
    "source": "mongodb",
    "doc_type": "general",
    "scraped_at": 31796.9672487
  },
  {
    "title": "Get Started with Atlas",
    "content": "Docs Home\n/\nMongoDB Atlas\n/\nAtlas CLI\n/\nManage Atlas\nGet Started with\nAtlas\nCopy page\nYou can get started with\nAtlas\nvia the Atlas CLI using a single\ncommand:\natlas setup\n.\nThis tutorial demonstrates how to use\natlas setup\nto:\nSign up for an\nAtlas\naccount.\nAuthenticate with the new\nAtlas\naccount.\nCreate one free database.\nLoad\nsample data\ninto your\nAtlas\ndatabase.\nAdd your IP address to your project's IP access list.\nCreate a MongoDB user for your\nAtlas\ndatabase deployment.\nConnect to your new database deployment using the MongoDB Shell,\nmongosh\n.\nYou can also run\natlas setup\nif you have an\nAtlas\naccount and\nan organization/project but you haven't set up a cluster.\nComplete The Prerequisites\nBefore you begin, you must\ninstall the Atlas CLI\n.\nFollow These Steps\nComplete the following procedure to get started with\nAtlas\n.\n1\nRun the\natlas setup\ncommand in the terminal.\natlas setup\nAfter you run the command, enter\nY\nto open the default browser. A browser window displays the\nCreate Your Account\nscreen.\nIf you want to log into an existing\nAtlas\naccount, click\nLog in now\nand log in.\nIf you're already logged into an existing\nAtlas\naccount,\nproceed to step 3.\n2\nSign up and verify your account.\nEnter your account information and click\nSign Up\n.\nFollow the prompts to verify your email or register using\nthird-party authentication.\n3\nVerify your Atlas CLI session.\nWhen you reach the\nActivation\nscreen, copy the\nverification code from the Atlas CLI and paste it into the\nbrowser. Then, click\nConfirm Authorization\nand return\nto the terminal window.\n4\nAccept the default cluster creation settings.\nAfter you verify your Atlas CLI session,\natlas setup\ncreates an\nM0\ncluster.\nM0\nclusters have some operational\nlimitations\n.\nIf you log into an existing account and have existing\norganizations and projects,\natlas setup\nprompts you to select\na default organization and default project. Select a default\norganization and project and press\nEnter\n.\nWhen the Atlas CLI prompts\nDo you want to set up\nyour first free database in Atlas with default\nsettings? It's free forever!\n, enter\nY\nto create your cluster with the default settings.\nThe command creates a sample\nM0\nshared-tier cluster with the\nfollowing default settings:\nCluster name:\nCluster<number>\nCloud provider and region:\nAWS - US_EAST_1\nDatabase Username:\nCluster<number>\nDatabase User Password:\nabcdef12345\nLoad Sample Data:\nYes\nAllow connection from IP:\n<YourIPAddress>\nDo you want to set up your first free database in\nAtlas with default settings? It's free forever! Y\nHIDE OUTPUT\nWe are deploying Cluster9876543...\nPlease store your database authentication access details in a secure location.\nDatabase User Username: Cluster9876543\nDatabase User Password: abcdef12345\nCreating your cluster... [Its safe to 'Ctrl + C']\nTake the Next Steps\nCongratulations! You have successfully set up your\nAtlas\naccount.\nUse the\nconnection string\nto connect to your cluster through\nmongosh\nor your application.\nTo view the status of your cluster, run the\natlas clusters\ncommand.\nBack\nManage Atlas\nNext\nCreate & Configure Clusters",
    "url": "https://www.mongodb.com/docs/atlas/cli/current/atlas-cli-getting-started/",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31797.2444947
  },
  {
    "title": "Related Services",
    "content": "Docs Home\n/\nAtlas\nRelated Services\nCopy page\nOverview\nThe following related services can integrate with\nAtlas\n:\nProduct\nDescription\nTriggers\nExecute application and database logic in\nAtlas\n.\nMongoDB Charts\nCreate visual representations of your MongoDB data and\nintegrates seamlessly with\nAtlas\n.\nYou can also use\nthe specific product integrations\nthat we developed with our partners to enhance\nAtlas\nand partner service capabilities.\nBack\nPush MongoDB Logs to AWS S3\nNext\nPartner Integrations",
    "url": "https://www.mongodb.com/docs/atlas/integrate/",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31797.6201675
  },
  {
    "title": "MongoDB .NET/C# Driver",
    "content": "Docs Home\n/\nDrivers\nMongoDB .NET/C# Driver\nCopy page\nIntroduction\nWelcome to the documentation site for the official MongoDB .NET/C#\ndriver and Entity Framework provider.\n.NET/C# Driver\nYou can add the driver to your application to work with MongoDB in .NET or C#. See\nthe\n.NET/C# driver documentation\nto learn how to install and begin using the driver.\nEntity Framework Provider\nYou can add the MongoDB Entity Framework provider to your .NET application as an\nobject-relational mapper (ORM) to work with data in MongoDB. See the\nEntity Framework provider documentation\nto learn how to begin using the provider.\nBack\nStart Developing with MongoDB\nNext\nJava Drivers",
    "url": "https://www.mongodb.com/docs/drivers/csharp-drivers/",
    "source": "mongodb",
    "doc_type": "driver",
    "scraped_at": 31797.9433195
  },
  {
    "title": "MongoDB Ops Manager",
    "content": "Docs Home\nMongoDB Ops Manager\nCopy page\nMongoDB Ops Manager\ncan automate, monitor, and back up your MongoDB\ninfrastructure.\nAutomation\nOps Manager\nAutomation enables you to configure and maintain MongoDB nodes\nand clusters.\nMongoDB Agents using Automation on each MongoDB host can maintain your\nMongoDB deployments. You can\ninstall\nthe\nMongoDB Agent\n. Automation can\nadd hosts\nand\ndeploy and upgrade new or existing clusters\n.\nMonitoring\nOps Manager\nMonitoring provides real-time reporting, visualization, and\nalerting on key database and hardware indicators.\nHow Monitoring Works\nWhen you activate Monitoring on a MongoDB host, Monitoring collects\nstatistics from the nodes in your MongoDB deployment. The Agent\ntransmits database statistics back to\nOps Manager\nto report deployment status\nin real time. You can\nset alerts\non\nindicators you choose.\nBackup\nOps Manager\nBackup provides scheduled\nsnapshots\nand\npoint-in-time\nrecovery\nof your MongoDB\nreplica sets\nand\nsharded clusters\n.\nHow Backup Works\nWhen you activate Backup for a MongoDB deployment, Backup takes\nsnapshots of data from the MongoDB processes you have specified.\nNote\nSharded clusters and replica sets are the only deployment types you can back up if\nyour databases run MongoDB FCV 4.2 and earlier. To back up a\nstandalone\nmongod\nprocess\nrunning MongoDB FCV 4.2 or earlier, you must\nconvert it to a single-member replica set\n.\nBackup Workflow\nBackups rely upon the\nMongoDB version compatibility\nof your database. This Feature Compatibility Version ranges from the\ncurrent version to one version earlier. For MongoDB 4.4, the\nFCV\ncan be\n4.2\nor\n4.4\n.\nThe backup process takes a snapshot of the data directory at its\nscheduled snapshot intervals\n.\nThis process copies the data files in a MongoDB deployment, sending\nthem over the network via\nOps Manager\nto your existing snapshot storage.\nYour deployment can still handle read and write operations during the\ncopying process.\nWith the new backup process, there are no longer initial syncs. As a\nresult of not having initial syncs,\nOps Manager\n(using a\nmongod\nrunning\nFCV\n4.2) can support a wider array of customers such as those\nheavily using\nrenameCollection\n.\nThe MongoDB Agent uses WiredTiger's incremental backup cursor to capture\nthe incremental changes.\nThe backup process works in this manner regardless of how snapshots are\nstored.\nBackup uses a MongoDB instance version equal to or greater than the\nversion of the replica set it backs up.\nBackup takes and stores snapshots based on a user-defined\nsnapshot retention policy\n.\nSharded cluster snapshots temporarily stop the balancer. The snapshots\nthen can insert a marker token into all shards and config servers in\nthe cluster.\nOps Manager\ntakes a\nsnapshot\nwhen the marker tokens\nappear in the snapshot data.\nOps Manager\ncan back up data as a full or incremental backup.\nOps Manager\nrequires a full backup:\nFor your first backup.\nAfter a snapshot has been deleted.\nIf the blockstore block size has been changed.\nIncremental backups reduce network transfer and storage costs.\nTo learn more about how to configure backups, see\nBackup Configuration Options\n.\nRestore Data\nBackup can restore data from a complete scheduled snapshot or from\na selected point between snapshots.\nYou can restore\nsharded clusters\nand\nreplica\nsets\nfrom selected points\nin time.\nWhen you restore from a\nsnapshot\n,\nOps Manager\nreads directly from\nthe snapshot storage. You can restore the snapshot:\nTo another cluster.\nTo download the snapshot files from an\nHTTPS\nlink.\nWhen you restore from a point in time,\nOps Manager\ndoes the following:\nRestores a full snapshot from the snapshot storage.\nApplies stored\noplogs\nuntil it reaches the\nspecified point.\nDelivers the snapshot and oplog updates using the same\nHTTPS\nmechanisms.\nYou can configure how much of the oplog you want to keep per\nbackup. This affects the amount of time a point-in-time restore\ncan cover.\nMongoDB welcomes your feedback. Let us know how we can\nimprove Ops Manager\n.\nNext\nOverview",
    "url": "https://www.mongodb.com/docs/ops-manager/current/",
    "source": "mongodb",
    "doc_type": "general",
    "scraped_at": 31798.1833962
  },
  {
    "title": "Release Notes",
    "content": "Docs Home\n/\nCompass\nRelease Notes\nCopy page\nMongoDB Compass\n1.46.6\nReleased Jan 17, 2025\nNew Features:\nMove\nRefresh databases\nbutton inline (\nCOMPASS-9497\n)\nBump\noidc-plugin\nto latest version (\nMONGOSH-2194\n)\nBug Fixes:\nAdd question mark to the modal title (\nCOMPASS-9505\n)\nFix inaccurate error message shown on bulk operations (\nCOMPASS-8529\n)\nFix\ngeoLayers\ncallbacks (\nCOMPASS-9540\n)\nFull Changelog available on Github\nMongoDB Compass\n1.46.5\nReleased Jan 2, 2025\nNew Features:\nAdd\n$rankFusion\naggregation stage for Atlas clusters on 8.1+\n(\nCOMPASS-9429\n)\nBug Fixes:\nFix ability to add a line break in the embedded shell and query bar\n(\nCOMPASS-9485\n)\nFull Changelog available on Github\nMongoDB Compass\n1.46.4\nReleased June 24, 2025\nNew Features:\nFilter both databases and collections with dot notation\nBug Fixes:\nLink to Vector Search Docs when editing vector indexes (\nCOMPASS-9435\n)\nPrevent Compass from crashing with mismatched GTK error (\nCOMPASS-9437\n)\nDon't override editor background colors in dark mode (\nCOMPASS-9248\n,\nCOMPASS-9248\n)\nFix issue where using the\n$out\naggregation stage on a collection in an\nexisting database would do nothing (\nCOMPASS-9375\n)\nDisplay and sort collections by the calculated storage size in the model\n(\nCOMPASS-9423\n)\nBump mongosh to latest version (\nCOMPASS-9424\n,\nCOMPASS-9419\n)\nFix stage description tooltip overflow (\nCOMPASS-9463\n)\nFull Changelog available on Github\nMongoDB Compass\n1.46.3\nReleased June 5, 2025\nNew Features:\nUpdate electron to v36 (\nCOMPASS-9098\n)\nBug Fixes:\nShow profiler collections (\nCOMPASS-9377\n)\nPrevent horizontal overflow scroll (\nCOMPASS-9418\n)\nDisable spellCheck in input (\nCOMPASS-9376\n)\nFull Changelog available on Github\nMongoDB Compass\n1.46.2\nReleased May 9, 2025\nNew Features:\nAllow users to disable dbStats and collStats in application preferences\n(\nCOMPASS-5387\n)\nAdd an error modal for viewing document validation error details\n(\nCOMPASS-8868\n)\nUse an iterable cursor for schema analysis (\nCOMPASS-9150\n,\nCOMPASS-9315\n)\nBug Fixes:\nDisable spellcheck to prevent network traffic in isolated mode (\nCOMPASS-8166\n)\nFull Changelog available on Github\nMongoDB Compass\n1.46.1\nReleased April 23, 2025\nNew Features:\nAdd special handling of vector sub-type (\nCOMPASS-8257\n)\nBug Fixes:\nAccount for pagination in expanded table view (\nCOMPASS-9161\n)\nNo 0 default for\nbulkDelete\ncount (\nCOMPASS-8603\n)\nFix tooltip triggers in indexes plugin (\nCOMPASS-8449\n)\nFix the shortcuts on Windows to be the app name, not executable name (\nCOMPASS-8367\n)\nFull Changelog available on Github\nMongoDB Compass\n1.46.0\nReleased April 4, 2025\nNew Features:\nEnable export schema feature (\nCOMPASS-8819\n)\nAdd in progress saving state (\nCOMPASS-9077\n)\nShow validation error details (\nCOMPASS-8864\n,\nCOMPASS-8867\n,\nCOMPASS-8865\n)\nAdd validation action option for 'errorAndLog' with 8.1 (\nCOMPASS-8983\n)\nBug Fixes:\nAllow deleting updatemany queries (\nCOMPASS-9121\n)\nRemove readPreferenceTags for database&collection lookups if the cluster is sharded and readPreferenceTags was set (\nCOMPASS-9111\n)\nCatch parse error on multi insert\nDon't allow empty commits in update generated files task; skip task when previous commit was also generated by it\nFull Changelog available on Github\nMongoDB Compass\n1.45.4\nReleased March 10, 2025\nNew Features:\nConnect in new window with connect split-button (\nCOMPASS-8473\n)\nAdd confirmation modal when applying rules, add explicit editing state (\nCOMPASS-8861\n)\nBump\nmongosh\n, driver to latest (\nCOMPASS-9056\n)\nAllow aborting analysis while in progress (\nCOMPASS-9052\n)\nBug Fixes:\nUpdate maps from Map Tiles API v2 to Raster Tiles API v3 (\nCOMPASS-8981\n)\nPrevent losing changes in validation editor (\nCOMPASS-9003\n)\nDocument preview overflow styles (\nCOMPASS-8997\n)\nHang when the shell output reaches\nmaxOutputLines\n(\nCOMPASS-7567\nand\nMONGOSH-2021\n)\nUpdate driver and patch\npackage-lock.json\nhoisting for\nmongosh\npackages\nUnified validation preview CTA (\nCOMPASS-8862\n)\nFull Changelog available on Github\nMongoDB Compass\n1.45.3\nReleased February 13, 2025\nNew Features:\nHandle non-existent namespaces (\nCOMPASS-5750\n)\nRun schema analysis with\nsecondaryPreferred\nwhen read preference is\nnot set (\nCOMPASS-8854\n)\nAllow users to set the default sort order to be recent first (\nCOMPASS-6706\n)\nAdds support for\ncreateFromHexString\nand\ncreateFromBase64\nbinary\nconstructors (\nCOMPASS-8942\n)\nBug Fixes:\nRetain current query when new query is applied (\nCOMPASS-8262\n)\nSign the Windows setup\n.exe\n(\nCOMPASS-8945\n,\nCOMPASS-8950\n)\nFull Changelog available on Github\nMongoDB Compass\n1.45.2\nReleased January 28, 2025\nNew Features:\nAdds a button that expands nested fields for all currently visible documents\n(\nCOMPASS-8233\n)\nRemove option to create capped collections (\nCOMPASS-8292\n)\nFull Changelog available on Github\nMongoDB Compass\n1.45.1\nReleased January 15, 2025\nBug Fixes:\nSend a nonce in oidc request by default (\nCOMPASS-8588\n)\nDon't cut off list view value editor tooltips (\nCOMPASS-8581\n)\nFixes element key editor tooltip and revert not updating validity\n(\nCOMPASS-8586\n)\nDon't fallback to zero if the count cannot be determined\n(\nCOMPASS-8841\n)\nUpdates top chart write lock percentage of load visual\n(\nCOMPASS-8604\n)\nDon't lose the isOperationInProgress state when the user switches\ntabs (\nCOMPASS-8576\n)\nBumps mongosh to 2.3.6 (\nCOMPASS-8689\n)\nDon't derive enableGenAIFeatures from org, allow disabling it\n(\nCOMPASS-8680\n)\nAdjusts status marker position (\nCLOUDP-289203\n)\nFull Changelog available on Github\nMongoDB Compass\n1.45.0\nReleased December 4, 2024\nBug Fixes:\nDisables UTF8 validation when retrieving Compass metadata\n(\nCOMPASS-8517\n)\nFull Changelog available on Github\nMongoDB Compass\n1.44.7\nReleased November 20, 2024\nNew Features:\nAdds an expandable json editor\nAdds a connect button to connections in the sidebar (\nCOMPASS-8381\n)\nAdds an active connections toggle in the sidebar (\nCOMPASS-8114\n)\nAdds a connections filter popover in the sidebar (\nCOMPASS-8503\n)\nBug Fixes:\nUse semaphore to limit reads (\nCOMPASS-7256\n)\nExpandable readonly document (\nCOMPASS-4635\n)\nRemove accelerators for linux (\nCOMPASS-8494\n)\nFull Changelog available on Github\nMongoDB Compass\n1.44.6\nReleased October 31, 2024\nIncrease the character size restriction for query and\naggregation genai (\nCOMPASS-8369\n)\nDo not lock users out of Compass when using authenticated\nproxies (\nCOMPASS-8382\n)\nBump electron to 32.2.1 (\nCOMPASS-8399\n)\nPersist modified document value (\nCOMPASS-8373\n)\nFull Changelog available on Github\nMongoDB Compass\n1.44.5\nReleased October 14, 2024\nNew Features:\nAdd polling for regular indexes (\nCOMPASS-8214\n)\nAllow editing name, color, and favorite checkbox of connected\nconnections (\nCOMPASS-8160\n)\nWhen\nProtect Connection String Secrets\nis enabled,\ndisplay banner on the connection form (\nCOMPASS-8264\n)\nAdd a\nSave & Connect\nbutton (\nCOMPASS-8360\n)\nBug Fixes:\nExpired certificates in the CA list cause connections to fail (\nCOMPASS-8322\n)\nUse\nditto\ninstead of\nzip\nto package on MacOS (\nCOMPASS-7737\n)\nShow friendly error when proxy config is not supported (\nCOMPASS-8345\n)\nBump\nmongosh\n, driver, and bson to the latest versions\nFull Changelog available on Github\nMongoDB Compass\n1.44.4\nReleased September 18, 2024\nNew Features:\nSupport for multiple\nKMS\noptions from the\nsame provider (\nCOMPASS-8082\n).\nBug Fixes:\nRemove outdated \"toggle shell\" keyboard shortcut (\nCOMPASS-8259\n).\nUse executionStats verbosity for explain plans (\nCOMPASS-8263\n).\nTrim whitespaces when creating or editing a namespace (\nCOMPASS-8123\n).\nFull Changelog available on GitHub\nMongoDB Compass\n1.44.3\nReleased September 5, 2024\nNew Features:\nWhen selecting a query history item from autocomplete, automatically move the\ncursor to the end of the editor.\nCompass supports Queryable Encryption range queries on encrypted fields\n(\nCOMPASS-7066\n).\nBug Fixes:\nUpdate query history autocompletion to be more selective\n(\nCOMPASS-8241\n).\nFix nextPage availability logic (\nCOMPASS-8239\n).\nCheck for Vector Search support when showing edit templates\n(\nCOMPASS-8235\n).\nHandle special characters in SSH URL correctly (\nCOMPASS-8254\n).\nRemove certificates without issuer from system CA list\n(\nCOMPASS-8252\n).\nFull Changelog available on GitHub\nMongoDB Compass\n1.44.0\nReleased September 3, 2024\nNew Features:\nSupport for users working with data stored on different connections\nstored in the Compass connections window (\nCOMPASS-6410\n).\nAdds per-connection proxy settings (\nCOMPASS-8142\n).\nEnables proxy support feature flag  (\nCOMPASS-8167\n).\nBug Fixes:\nStream import errors to the log file with proper back pressure\n(\nCOMPASS-7820\n).\nIn the bulk update preview, convert array indexes from strings to\nnumbers (\nCOMPASS-8218\n).\nBump shell-bson-parser to 1.1.2 (\nMONGOSH-1859\n).\nFull Changelog available on GitHub\nMongoDB Compass\n1.43.6\nReleased August 23, 2024\nNew Features:\nRelease query history autocompletion (\nCOMPASS-8096\n).\nAdds ability to load more documents per page in Documents view\n(\nCOMPASS-6903\n).\nRemove useSystemCA by making it default\n(\nCOMPASS-8077\n).\nAdd option to prefer ID token over access token (\nCOMPASS-8107\n).\nBug Fixes:\nWork around long paths issue on windows when building native\ndependencies to make sure use system ca option works\n(\nCOMPASS-8051\n).\nFix indexes ux issues (\nCOMPASS-7084\nand\nCOMPASS-4744\n).\nNew tab design (\nCOMPASS-8122\n).\nRestore Isolated and Readonly special behavior in packaged\napplication (\nCOMPASS-8129\n).\nFull Changelog available on GitHub\nMongoDB Compass\n1.43.5\nReleased Jan 31, 2024\nNew Features:\nA warning toast message now displays when Compass cannot access credential\nstorage (\nCOMPASS-7819\n).\nBug Fixes:\nFixed a bug when closing the\nPerformance\ntelemetry screen\ncaused a crash (\nCOMPASS-8056\n).\nFixed a bug when sidebar search displayed only database names when both\ndatabase and other object names matched the search\ncriteria (\nCOMPASS-8026\n).\nFixed a bug that prevented adding nested fields on object-type\nfields (\nCOMPASS-7929\n).\nFixed a bug that caused EJSON data types to export incorrectly when\nexport to JSON (\nCOMPASS-8099\n).\nFull Changelog available on GitHub\nMongoDB Compass\n1.43.4\nReleased Jan 1, 2024\nBug Fixes:\nPrevents application from hanging when selecting ranges too quickly on the\nSchema\ntab (\nCOMPASS-8048\n).\nUpdates Electron to\nversion 29.4.2\n, which includes various\nsecurity fixes.\nFull Changelog available on GitHub\nMongoDB Compass\n1.43.3\nReleased June 27, 2024\nNew Features:\nPrevents modified tabs from being closed by accident\n(\nCOMPASS-5022\n).\nBug Fixes:\nAggregations use\nmaxTimeMS\ndefault on preview documents\n(\nCOMPASS-7798\n).\nFixes a regression that prevented autoconnect from working properly\n(\nCOMPASS-8044\n).\nFull Changelog available on GitHub\nMongoDB Compass\n1.43.2\nReleased June 25, 2024\nNew Features:\nShows tool tip when a query or aggregation is generated without\ncontent (\nCOMPASS-7837\n).\nAdds a confirmation dialog when quitting Compass\n(\nCOMPASS-6435\n).\nExpands options when applied from a query history that has options.\nBug Fixes:\nRegular expression and number query history fix\n(\nCOMPASS-7215\n,\nCOMPASS-7008\n).\nKeeps listeners for insert document validity on document view\n(\nCOMPASS-3246\n).\nHides inaccurate collection statistics for timeseries\n(\nCOMPASS-6712\n).\nRemoves 'Preview' label from OIDC (\nCOMPASS-7666\n).\nUses system ca certificates in Atlas requests and OIDC\n(\nCOMPASS-7950\n).\nHides edit view button in read only mode (\nCOMPASS-7688\n).\nUpdates line numbers to be unselectable (\nCOMPASS-7941\n).\nSidebar tab fix.\nFull Changelog available on GitHub\nMongoDB Compass\n1.43.1\nReleased June 12, 2024\nNew Features:\nAdded a disabled state to the Generative AI query bar. This state\ndisplays while Generative AI is fetching results\nCOMPASS-7902\n.\nUpdated the close window hotkey to be\ncmd\n+\nshift\n+\nw\nto avoid conflict with close tab\ncmd\n+\nw\nCOMPASS-7301\n.\nAdded a setting for enabling sample documents. This setting improves\nGenerative AI queries\nCOMPASS-7931\n.\nUpdated Generative AI input to be resizable text area\nCOMPASS-7940\n.\nBug Fixes:\nFixed an issue with base64 regular expressions\nCOMPASS-7541\n.\nFixed a display issue with the\nCreate collection\nbutton when\nusing Compass in readonly mode.\nRemoved the Hackolade banner from the schema tab\nCOMPASS-7974\n.\nFixed an issue when a connection gets saved as favorite from the old\nsidebar\nCOMPASS-7980\n.\nFixed a display issue for long index names\nCOMPASS-7016\n.\nVarious user interface message and verbiage improvements.\nFull changelog available on GitHub\nMongoDB Compass\n1.43.0\nReleased May 02, 2024\nNew Features\nAllows users to specify hints (\nCOMPASS-7829\n)\nShows an error message when connecting to Stream Processing (\nCOMPASS-7809\n)\nWarns users when generated aggregation contains a write operation (\nCOMPASS-7298\n)\nAdds support to notify users for an update on Linux/MSI (\nCOMPASS-7686\n)\nAdds gradient while a generate request is in progress (\nCOMPASS-7836\n)\nIncludes the error count in the import toast (\nCOMPASS-7826\n)\nRemoves the GenAI \"Preview\" badge (\nCOMPASS-7890\n)\nDisables the query bar controls while GenAI is running (\nCOMPASS-7839\n)\nBug Fixes\nEnsures that the confirmation modal always asks for confirmation input (\nCOMPASS-7613\n)\nBumps Electron to 29 and removes support for RHEL7 (\nCOMPASS-7868\n)\nParses AI response correctly (\nCOMPASS-7780\n)\nFixes CPU hikes because of bad useEffect dependency\nFull changelog available on GitHub\nNew Features\nQueries generated by Natural Language Querying have improved\nquality and accuracy.\nMongoDB Compass\n1.42.5\nReleased April 08, 2024\nNew Features\nUpdates Atlas login screen flow (\nCOMPASS-7755\n)\nHandles collection subtab from link (\nCOMPASS-7731\n)\nZ-indexed stacked components (\nCOMPASS-7732\n)\nRemoves the ability to collapse the sidebar (\nCOMPASS-7812\n)\nUpdates the “Use Generative AI” settings flow (\nCOMPASS-7756\n)\nBug Fixes\nShow the in-progress index in the list again (\nCOMPASS-7789\n)\nDo not throw when rendering invalid dates (\nCOMPASS-7749\n)\nCan't sign out if not signed in yet (\nCOMPASS-7787\n)\nClick current op for details scoping error (\nCOMPASS-7805\n)\nFull changelog available on GitHub\nNote\nMongoDB Compass\nversion 1.42.4 was not released.\nMongoDB Compass\n1.42.3\nReleased March 20, 2024\nNew Features\nInstall updates without confirmation in the background by default\n(\nCOMPASS-7616\n)\nEnable rename collection feature flag (\nCOMPASS-7699\n)\nBump OIDC dependencies to latest versions\nBump\nmongosh\n, driver, and bson to latest versions\nBug Fixes\nCalculate the maximum line length in a more stack efficient way\n(\nCOMPASS-7647\n)\nOnly access defaultSession when app is ready\nDon't allow the\nreadonly\nfilter to grow in width indefinitely\n(\nCOMPASS-7728\n)\nFull changelog available on GitHub\nMongoDB Compass\n1.42.2\nReleased March 01, 2024\nNew Features\nInstall updates without confirmation in the background by default\n(\nCOMPASS-7616\n)\nBug Fixes\nDouble space not applied from schema or query history\n(\nCOMPASS-6980\n)\nReset atlas search index on reopen and type change\nDon't include the version number in process.title because it shows in\nthe menubar in macOS Sonoma (\nCOMPASS-7513\n)\nShow the folder through the main process (\nCOMPASS-7671\n)\nFull changelog available on GitHub\nMongoDB Compass\n1.42.1\nReleased February 15, 2024\nNew Features\nAdded vector search index creation to the create search indexes\nmodal (\nCOMPASS-7302\n).\nVector search type indexes now display in the search index table\n(\nCOMPASS-7509\n).\nBug Fixes\nImproved validation of command line arguments (\nCOMPASS-7260\n).\nFixed a display issue that made update and delete labels hidden on\nnarrow windows.\nFull changelog available on GitHub\nMongoDB Compass\n1.42.0\nReleased January 31, 2024\nNew Features\nCompass now supports\nBulk Update\nand\nBulk Delete\noperations (\nCOMPASS-7329\n,\nCOMPASS-7330\n).\nBug Fixes\nFixed namespace stats that refresh after document updates.\nFixed table card autosizing (\nCOMPASS-7548\n).\nFixed an issue when opening a new collection tab if an existing\ncollection tab with the same name was already open\n(\nCOMPASS-7556\n).\nFixed an issue where switching tabs would reset vertical scrolling\nto the top position (\nCOMPASS-7370\n).\nFixed an issue where invalid dates resulted in a blank export page\n(\nCOMPASS-7515\n).\nFull changelog available on GitHub\nMongoDB Compass\n1.41.0\nReleased December 18, 2023\nNew Features:\nAuto-insert empty document for all fields in the query bar.\nAdd filter to saved connections when there are more than ten saved\n(\nCOMPASS-7439\n).\nImplement text search for aggregation stage wizard.\nImprove stage wizard discoverability and interaction\n(\nCOMPASS-7350\n).\nUse dark colors for background on initial app loading with dark theme.\nRemove outdated guide cues (\nCOMPASS-7396\n)\nIntroduced workspaces plugin and implemented single top-level tabs\n(\nCOMPASS-7354\n).\nBug Fixes:\nInclude OIDC in $external auth mechanism list (\nCOMPASS-7512\n).\nProperly render syntax errors in embedded shell (\nCOMPASS-7497\n).\nExpanded documents retain state after switching tabs (\nCOMPASS-7318\n).\nPrevent shell container from overlaying sidebar content\n(\nCOMPASS-7395\n).\nPrevent AI entry button from being submitted when the sort is submitted\n(\nCOMPASS-7356\n).\nFull changelog available on GitHub\nMongoDB Compass\n1.40.4\nReleased October 18, 2023\nNew Feature:\nIntegrated search index signals (\nCOMPASS-7176\n).\nBug Fixes:\nFixed tab behavior with selection (\nCOMPASS-7013\n).\nUI fixes (\nCOMPASS-7304\n).\nMade column widths smaller (\nCOMPASS-7341\n).\nRemediated vulnerability SNYK-JS-BABELTRAVERSE-5962462 (\nCOMPASS-7345\n).\nFull changelog available on GitHub\nMongoDB Compass\n1.40.3\nReleased October 11, 2023\nNew Features:\nIntroduced the ability to create and manage Atlas Search indexes in\nCompass.\nAfter creating a\n$search\nindex, you are redirected to the\nsearch indexes modal (\nCOMPASS-7247\n).\nField names are now autocompleted when defining indexes.\n(\nCOMPASS-7174\n).\nSyntax errors are now highlighted when defining indexes.\n(\nCOMPASS-7246\n).\nThe\nvectorEmbedding\nindex template definition replaced\nknnVector\n.\n(\nCOMPASS-7288\n).\nFor more details, see\nManage Indexes\nand\nCreate and Manage an Atlas Search Index\n.\nImproved AI feedback experience. For more information on\nGenerative AI natural language queries in Compass, see\nQuery with Natural Language\n.\n(\nCOMPASS-7211\n,\nCOMPASS-7251\n).\nRemoved\nuseNewUrlParser\nand\nuseUnifiedTopology\nfrom export\nto language options (\nCOMPASS-4897\n).\nBug Fixes:\nFixed GUI flicker when closing the search index modal (\nCOMPASS-7248\n).\nDowngraded Electron to\nversion 25.8.4\n(\nCOMPASS-7291\n).\nCorrected an error displaying collections in Atlas Data Federation\nthrough the side bar (\nCOMPASS-7307\n).\nFixed an issue when the insert dialog did not catch invalid bson (\nCOMPASS-7316\n).\nFull changelog available on GitHub\nMongoDB Compass\n1.40.2\nReleased September 28, 2023\nBug Fixes:\nHot fixed an issue where users were not able to run Compass after\nupgrading to 1.40.0 (\nCOMPASS-7270\n,\nCOMPASS-7269\n).\nRepairs broken preferences by setting default values.\nFull changelog available on GitHub\nMongoDB Compass\n1.40.1\nReleased September 27, 2023\nNew Features:\nWhen using a search index, a new tab prompts you and redirects you to\nthe aggregation tab with the $search operator and the index name populated\n(\nCOMPASS-7168\n).\nAdded a drop-down to choose a search index template (\nCOMPASS-7173\n).\nBug Fixes:\nEnsure Atlas Login doesn't show in settings if you didn't get the\nGenerative AI feature rollout.\nFull changelog available on GitHub\nMongoDB Compass\n1.40.0\nReleased September 26, 2023\nNew Features:\nUpgrade\nembedded MongoDB shell\nto\nversion 2.0.0 (\nCOMPASS-7057\n).\nUpgrade Node driver to version 6.0.0 (\nCOMPASS-7057\n).\nUpgrade Electron to\nversion 26\n.\nSet up local Atlas detection (\nCOMPASS-7213\n).\nDisplay local Atlas development environments as such\n(\nCOMPASS-7156\n).\nIntroduce\n$vectorSearch\naggregation stage to MongoDB 7.1 and 7.0.x\n(\nCOMPASS-7064\n).\nEnable\nAtlas search index\nmanagement (\nCOMPASS-7238\n).\nEnable natural language query and pipeline generation (incrementally\nrolled out to users) (\nCOMPASS-6866\n).\nBug Fixes:\nAccount for changed key order in query (\nCOMPASS-7194\n).\nRemove out stages before running explain plan (\nCOMPASS-7012\n).\nDon't automatically select regex when detecting regex\n(\nCOMPASS-7144\n).\nFull changelog available on GitHub\nMongoDB Compass\n1.39.4\nReleased September 6, 2023\nBug Fixes:\nAllows\n[object Object]\nas a valid string value in TypeChecker\n(\nCOMPASS-7132\n).\nDoesn't treat non-numbers in CSV headers as array indexes\n(\nCOMPASS-7157\n).\nLimits when custom paste handling is applied and uses clipboard data when\nauto-fixing user input (\nCOMPASS-7149\n).\nUpdates Electron to v24.8.2 to address security vulnerabilities. This updates\nspecifically addresses CVE-2023-4427 and CWE-119.\nFull Changelog available on GitHub\nMongoDB Compass\n1.39.3\nReleased August 28, 2023\nBug Fix:\nFix compatibility issue for saved connections in older versions of\nMongoDB Compass\n(\nCOMPASS-7141\n).\nMongoDB Compass\n1.39.2\nReleased August 22, 2023\nNote\nStarting in version 1.39.2,\nMongoDB Compass\nno longer supports migrating from legacy\nconnection files that pre-date version 1.31.0. Legacy connections refer to an\ninternal\nCompass\nconnection-options format that is stored on disk and no\nlonger supported after version 1.39.0.\nIf you have legacy connections saved in your favorites, export the\nconnections on\nversion 1.39.0\nto convert them\ninto the new format before updating to version 1.39.2 or later.\nNew Features:\nShow insights for unbound arrays (\nCOMPASS-6836\n).\nUse modal to highlight legacy connections (\nCOMPASS-7072\n).\nAutomatically add\n{ }\nto Find queries (\nCOMPASS-6530\n).\nShow list of legacy connections. (\nCOMPASS-7081\n).\nBug Fixes:\nFix error that would occur when modifying a filter in the schema tab\n(\nCOMPASS-6944\n).\nUse correct tab name for indexes & validation (\nCOMPASS-7022\n).\nStrip unknown preferences when loading (\nCOMPASS-7026\n).\nSplit connection storage between processes (\nCOMPASS-7078\n).\nClear drop collections input state if drop collections success\n(\nCOMPASS-7035\n).\nAllow empty optional string flags (\nCOMPASS-7101\n).\nFull Changelog available on GitHub\nMongoDB Compass\n1.39.1\nReleased August 8, 2023\nNew Features:\nUpdates Electron to v23.3.12 to address security vulnerabilities. The\nspecific CVEs addressed in this update are CVE-2023-3730, CVE-2023-3732,\nand CVE-2023-3728.\nFull Changelog available on GitHub\nMongoDB Compass\n1.39.0\nReleased Jan 14, 2023\nNew features:\nEnable\nproactive performance insights\nby default. Proactive performance\ninsights analyze your queries and suggest ways to improve performance.\n(\nCOMPASS-7000\n)\nBug Fixes:\nFix issue where Compass would create an incorrect index.\n(\nCOMPASS-6981\n)\nEnsure that Compass displays indexes in the correct case.\n(\nCOMPASS-6510\n)\nCap number of log files to 100. (\nMONGOSH-1449\n)\nMap project to projection before emitting open-explain-plan event\n(\nCOMPASS-6995\n)\nFix issue with Windows hotkeys. (\nCOMPASS-6777\n)\nHandle missing execution stats in raw explain.\nFull Changelog available on GitHub\nMongoDB Compass\n1.38.2\nReleased June 30, 2023\nBug Fix:\nFix issue with projecting document size. (Reverts\nCOMPASS-6837\n)\nFull Changelog available on GitHub\nMongoDB Compass\n1.38.1\nNew Features:\nAuto expand object and array field types on field add (\nCOMPASS-6939\n).\nShow unindexed query insight in explain plan modal (\nCOMPASS-6933\n).\nShow array length on array fields on documents (\nCOMPASS-6938\n).\nAdd ctrl + tab and ctrl + shift + tab hotkeys for switching tabs.\nEnable new explain plan by default.\nAdds insights for usage of $text and $regex in aggregation builder and\ncollection header (\nCOMPASS-6834\n).\nAdd cues (\nCOMPASS-6614\n).\nSignal for bloated documents during import.\nFull Changelog availble on GitHub\nMongoDB Compass\n1.38.0\nReleased June 21, 2023\nNew Features:\nOpenID Connect (OIDC) authentication (\nCOMPASS-6803\n).\nStage wizard, which helps build aggregation pipelines\n(\nCOMPASS-6814\n).\nAdd visual tree and update summary for aggregation explain plans\n(\nCOMPASS-6821\nand\nCOMPASS-6888\n).\nOpen a collection in a new tab shortcut.\nAdd performance tab indicator to state that information about certain\ncollections is missing (\nCOMPASS-6593\n).\nAdd Atlas error message when connection fails because of IP access\nissue (\nCOMPASS-6842\n).\nShow insight when query is unindexed (\nCOMPASS-6832\n).\nShow insight when number of collections is too high\n(\nCOMPASS-6835\n).\nAdd unindexed aggregation insight (\nCOMPASS-6833\n).\nImplement guide cue component in Compass to provide contextual user\ninterface assistance (\nCOMPASS-6334\n).\nAdd support for hiding and unhiding indexes in the Index tab.\nFor Windows installations, Compass now requires Windows version 10 or later\n(\nCOMPASS-6897\n).\nBug Fixes:\nAdd file type filters when exporting data (\nCOMPASS-6890\n).\nRename \"Less Options\" to \"Fewer Options\" (\nCOMPASS-6774\n).\nSupport dark mode for\nTypeEditor\ndrop down (\nCOMPASS-6893\n).\nFix execution time for aggregation explain plan\n(\nCOMPASS-6496\n).\nFix fast XML parser issue (\nCOMPASS-6905\n).\nUse\nenableShell\nsetting to control the runtime start and stop.\nInclude\nhas_sort\nin telemetry.\nVarious jQuery fixes (\nCOMPASS-6885\n,\nCOMPASS-6884\n,\nCOMPASS-6883\n, and\nCOMPASS-6882\n).\nReplace\ngot\nwith\nfetch\nin redirect (\nCOMPASS-6881\n).\nFix redirects (\nCOMPASS-6880\nand\nCOMPASS-6879\n).\nShow 1 as the page number for collections when no entries are present.\nSelect combo box option\nonBlur\nissue (\nCOMPASS-6511\n).\nUse\nmongodb-cloud-info\nv2 for IPv6 support in cloud metrics\n(\nCOMPASS-6795\n).\nFix editor dark mode background selection color\n(\nCOMPASS-6910\n).\nDisable the autoupdater for MSI installs (Windows without Squirrel)\n(\nCOMPASS-6857\n).\nFix for\nbulkWrite\nwhen importing data (\nCOMPASS-6928\n).\nFull Changelog available on GitHub\nMongoDB Compass\n1.37.0\nReleased May 25, 2023\nNew Features:\nDark mode has been updated with a modern theme. If the modern theme\nis enabled in\nSettings>Feature Preview\n, Compass defaults to\nthe modern dark mode theme.\nVarious user experience improvements when using the Import and Export functionality in Compass (\nCOMPASS-5576\n,\nCOMPASS-6543\n).\nCombine array fields into one in the import CSV preview (\nCOMPASS-6766\n).\nAdd settings to sidebar menus (\nCOMPASS-6796\n).\nShow password only when user focuses on input (\nCOMPASS-6161\n).\nAdd autocomplete support for $percentile, $median and $$USER_ROLES (\nCOMPASS-6780\n,\nCOMPASS-6781\n).\nToggle state of\nEdit connection string\nis based on the new global preference. This setting controls whether a password is visible when creating a new connection.\nCompass supports the new Queryable Encryption protocol. Starting\nin v1.37.0, Compass is not compatible with MongoDB server versions\nearlier than 7.0\nwhen using Queryable Encryption (\nCOMPASS-6601\n,\nCOMPASS-6602\n).\nWhen using queryable encryption on pre-7.0 servers, you can decrypt\nencrypted data, but you cannot insert or query data.\nAdd export aggregation code preview to export modal (\nCOMPASS-6725\n).\nBug Fixes:\nDifferentiate between new Date() and Date() (\nCOMPASS-6755\n).\nFix guessFileType() when JSON fails and CSV lines are huge.\nFlush import progress throttle on import error.\nFeature flag default values (\nCOMPASS-6525\n).\nAllow updates on a sharded collection (\nCOMPASS-6058\n).\nDetect line breaks, pass it on to papaparse (\nCOMPASS-6819\n).\nUpdate reset on query bar to reset results and emit query-changed (\nCOMPASS-6805\n).\nFull Changelog available on GitHub\nMongoDB Compass\n1.36.4\nReleased April 27, 2023\nNew Features:\nUpdate add data icon to plus with circle from download (\nCOMPASS-6494\n)\nShow import progress in toast, make import background\n(\nCOMPASS-6540\n,\nCOMPASS-6555\n)\nImport progress (\nCOMPASS-6721\n)\nUpdate new connection text to new window (\nCOMPASS-6723\n)\nBug Fixes:\nRemove re-count when not available (\nCOMPASS-5179\n,\nCOMPASS-6649\n)\nFill autocomplete on tab (\nCOMPASS-6695\n)\nShow error border when focused (\nCOMPASS-6724\n)\nCompass readonly allows to drop namespaces from the sidebar (\nCOMPASS-6687\n)\nFixes the problem of refresh button on collection tab not refreshing\nthe collection stats (\nCOMPASS-6738\n)\nUpdate windows config file fetching location one folder up (\nCOMPASS-6527\n)\nIf listCSVFields() or analyzeCSVFields() fails it will display the error\nin the modal (\nCOMPASS-6737\n)\nFull Changelog available on GitHub\nMongoDB Compass\n1.36.3\nReleased April 13, 2023\nNew Features:\nAdd links to the documentation to the agg and stage\nautocompleter suggestions (\nCOMPASS-6688\n)\nBug Fixes:\nListen to query-history events in query-bar and open saved items\n(\nCOMPASS-6680\n,\nCOMPASS-6681\n,\nCOMPASS-6685\n)\nFix loading configuration file on windows, remove arg check (\nCOMPASS-6527\n)\nRemove count when exporting views and time series collections\n(\nCOMPASS-5179\n,\nCOMPASS-6649\n)\nMongoDB Compass\n1.36.2\nReleased March 29, 2023\nNew Features:\nRemoves focus mode feature flag, always show (\nCOMPASS-6474\n)\nAnalyze CSV fields and auto-select the correct type (\nCOMPASS-6536\n)\nAdd GitHub source code link to help menu and window menu\n(\nCOMPASS-6585\n)\nMakes analyzeCSVFields() skippable (\nCOMPASS-6638\n)\nBug Fixes:\nApply readPref to initial ping command (\nCOMPASS-6595\n)\nFix guessFileType() for large JSON docs (\nCOMPASS-6629\n)\nFix memory leak in listCSVFields() (\nCOMPASS-6630\n)\nAdd dark mode colours for the mixed warning\nAbort analyzeCSVFields() when closing the import modal\n(\nCOMPASS-6633\n)\nOptimize CSV field type detection\nFull Changelog available on GitHub\nMongoDB Compass\n1.36.0\nReleased March 15, 2023\nNew Features:\nEnable focus mode (\nCOMPASS-6474\n)\nAdd stage button between stages (\nCOMPASS-6382\n)\nUse type from last array element when inserting new element to array\n(\nCOMPASS-6432\n)\nRedirect to the new collection after creating it (\nCOMPASS-6019\n)\nStage toolbar (\nCOMPASS-6381\n)\nLG darkmode support and UI cleanup in the explain tab (\nCOMPASS-6463\n)\nAdds a cancellable loader to explain\nEnable column store indexes for MongoDB 6.3 (\nCOMPASS-6487\n)\nflexi bucket options for Timeseries\nUpgrade mongosh to 1.7.0\nInclude preview rows in the listCSVFields() result (\nCOMPASS-6422\n)\nEnable focus mode (\nCOMPASS-6474\n)\nWhen dropping a collection or database, redirect to either the database or\ndatabases view (\nCOMPASS-6018\n,\nCOMPASS-6434\n)\nDark theme improvements in the settings modal (\nCOMPASS-6552\n)\nConditional confirmation modal (\nCOMPASS-6355\n)\nAdds the refresh CTA to sidebar\nOpen file input before import modal (\nCOMPASS-6535\n)\nEnable LG darkmode as public preview (\nCOMPASS-6515\n,\nCOMPASS-6556\n)\nHook for keyboard shortcuts (\nCOMPASS-6551\n)\nAdds refresh CTA on database and collection list view (\nCOMPASS-6431\n)\nPlace settings under the most idiomatic menu for the platform\n(\nCOMPASS-6430\n)\nBug Fixes:\nIf a date is in the safe range, go with relaxed EJSON rather than canonical\n(\nCOMPASS-5744\n)\nRedesign of add stage button (\nCOMPASS-6449\n)\nOptimises the opening of tab\nDon't show negative count on delete when no document count (\nCOMPASS-5996\n)\nStop on errors when stopOnErrors is true (\nCOMPASS-6518\n)\nUndefined rather than false if getCloudInfo fails, support SRV URIs\n(\nCOMPASS-6111\n)\nCancel edit on non-existent field (\nCOMPASS-6505\n)\nHalt autoupdater on application exit to prevent logger crashing\n(\nCOMPASS-6051\n)\nDo not reset stage value if it was already changed (\nCOMPASS-6584\n)\nFull Changelog available on Github:\nMongoDB Compass\n1.35.0\nReleased January 11, 2023\nNew Features:\nUpdate export modal to LeafyGreen components (\nCOMPASS-6220\n)\nReplace types dropdown with LG select\nUse leafygreen combobox to select stages\nReplace export-to-language with leafygreen components (\nCOMPASS-6219\n)\nAdd connection import/export UI\nConvert compass query history to new components (\nCOMPASS-6221\n)\nUse the same date hook in query history as in saved aggregations (\nCOMPASS-6221\n)\nAdd forceConnectionOptions option (\nCOMPASS-6068\n)\nImplement readOnly option (\nCOMPASS-6064\n)\nUpdate import modal to LeafyGreen components (\nCOMPASS-6220\n)\nAdd --username and --password for auto-connect (\nCOMPASS-6216\n)\nExpose protectConnectionStrings in settings UI (\nCOMPASS-6262\n)\nKerberos password field setting (\nCOMPASS-5950\n)\nAdd maxTimeMS as setting (\nCOMPASS-6063\n)\nUpdate compass validation components to leafygreen (\nCOMPASS-6237\n)\nUpdate explain plan components (\nCOMPASS-6236\n)\nImplement enableDevTools option (\nCOMPASS-6061\n), (\nCOMPASS-5615\n)\nUse rebranded components in the document table view\nAdd tracking event when stage value changes (\nCOMPASS-6310\n)\nUpdate Compass aggregations modals (\nCOMPASS-6286\n)\nAdd LG darkTheme support for table view\nRemove trackErrors setting (\nCOMPASS-5708\n)\nMove all autoupdates logic to compass main process, allow to dismiss updates (\nCOMPASS-6057\n) (\nCOMPASS-6303\n)\nConvert more insert dialog code to compass components & leafygreen (\nCOMPASS-6285\n)\nRegister Compass as a protocol handler for\nmongodb://\n(\nCOMPASS-6085\n)\nAdd\n--show-example-config\nflag (\nCOMPASS-6084\n)\nCancellable aggregate and schema analysis (\nCOMPASS-5668\n)\nCancellable find and explain (\nCOMPASS-5668\n)\nImplement new input docs card design (\nCOMPASS-6234\n)\nUpdate scrollbar styles (\nCOMPASS-5597\n)\nCancellable counts (\nCOMPASS-5668\n)\nUpdate aggregations stage components (\nCOMPASS-6234\n)\nEnable pipeline as text feature (\nCOMPASS-6299\n)\nIndex tab UI improvements (\nCOMPASS-6323\n), (\nCOMPASS-6329\n)\nAdd refresh document count in aggregation results (\nCOMPASS-6156\n)\nConfirm when deleting pipeline (\nCOMPASS-4137\n)\nBug Fixes:\nImprove table view interactions\nDo not save auto-connection in recents (\nCOMPASS-6290\n)\nCheck for root level when deciding if _id key is editable (\nCOMPASS-6160\n)\nFix the saved pipelines popover's scrolling (\nCOMPASS-6277\n)\nDisable deprecation warnings in production (\nCOMPASS-6322\n)\nIgnore non-digits in number input (\nCOMPASS-6326\n)\nSpeed up export (\nCOMPASS-6332\n)\nIncrease compass schema value bubble contrast (\nCOMPASS-6230\n)\nFix macOS protocol handler connection string passing\nFix typo on Indexes screen\nAvoid race condition when installing listeners\nHide delete for db/coll cards in readonly mode (\nCOMPASS-6292\n)\nFreeze settings modal height and adjust categories (\nCOMPASS-6325\n)\nFix nested field autocomplete (\nCOMPASS-6335\n)\nReset contains error check on document json view edit cancel (\nCOMPASS-6059\n)\nPass the preference as a prop when nesting Field (\nCOMPASS-6363\n)\nHide add stage in toolbar (\nCOMPASS-6373\n)\nMake $out options more clear in agg pipeline builder (\nCOMPASS-6304\n)\nSpeed up document json view (\nCOMPASS-6365\n)\nExport to Language (Java) has incorrect class name (\nCOMPASS-6159\n)\nEnable next page button when count is unknown (\nCOMPASS-6340\n)\nInitialize before identify and use get-os-info from npm\nOutput stage destination name (\nCOMPASS-6407\n)\nSet width of compass shell to avoid overflow (\nCOMPASS-6411\n)\nMongoDB Compass\n1.34.2\nReleased December 16, 2022\nBug Fixes:\nFix (compass-editors): fix nested field autocomplete (\nCOMPASS-6335\n)\nFix (schema): fix display of geo visualizations for nested fields\n(\nCOMPASS-6363\n)\nMongoDB Compass\n1.34.1\nReleased November 21, 2022\nNew Features:\nAdd command-line interface and global configuration (\nCOMPASS-6069\n,\nCOMPASS-6070\n,\nCOMPASS-6071\n,\nCOMPASS-6073\n,\nand\nCOMPASS-6074\n)\nFlip the new toolbars feature flag, always show new toolbars\n(\nCOMPASS-5679\n)\nAdd autocompleter for aggregation, use autocompleter in import pipeline\nmodal (\nCOMPASS-6175\n)\nAdd\nprotectConnectionStrings\noption (\nCOMPASS-6066\n)\nAdds the\nnetworkTraffic\nconfiguration option to block outgoing network\nconnections (\nCOMPASS-6065\n)\nShow icons in the sidebar menus (\nCOMPASS-6081\n)\nRebranding components (\nCOMPASS-6100\n,\nCOMPASS-6101\n,\nCOMPASS-6121\n,\nCOMPASS-6048\n, and\nCOMPASS-6187\n)\nLayout improvements (\nCOMPASS-6148\n,\nCOMPASS-6150\n, and\nCOMPASS-5582\n)\nAdd theme as regular setting (\nCOMPASS-6067\nand\nCOMPASS-5284\n)\nBug Fixes:\nFix installation issues on Windows (\nCOMPASS-6315\n)\nFix map rendering and add e2e tests (\nCOMPASS-6131\n)\nReconnect CSFLE client after\ncollMod\n(\nCOMPASS-5989\n)\nImprove selection area for insert document editor\nAdd map for collection stats for tab namespace isolation\n(\nCOMPASS-6146\n)\nOpen info links in browser (\nCOMPASS-6193\n)\nMenu not fully showing for field actions, remove old backgrounds\n(\nCOMPASS-6186\n)\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.33.1\nReleased September 14, 2022\nNew Features:\nUpdate saved aggregations to open as popover (\nCOMPASS-5852\n)\nAdd error message hint for crud timeout message (\nCOMPASS-4638\n)\nShow namespace on saved queries and pipelines popovers (\nCOMPASS-6028\n)\nAdd sparse option for indexes (\nCOMPASS-1963\n)\nOnly show columnstore index option for mongodb server >= 7 (\nCOMPASS-5970\n)\nAdd progress badge to the indexes table (\nCOMPASS-5944\n)\nFix table header for indexes (\nCOMPASS-6042\n)\nBug Fixes:\nAdjust crypt shared library download script for M1 builds\nHide collection submenu on disconnect (\nCOMPASS-6047\n)\nAlign delete index modal text\nFix updating arrays with dots in names (\nCOMPASS-6011\n)\nHide document views when there are no documents\nFix import deep JSON overwriting variables (\nCOMPASS-5971\n)\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.33.0\nReleased August 31, 2022\nNew Features:\narm64 build for darwin is now available (\nCOMPASS-5574\n)\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.32.3\nReleased Jan 13, 2022\nNew Features:\nbson-transpilers: Export to PHP from Compass (\nPHPLIB-719\n)\ncompass-components: Enable ACE code formatter (\nCOMPASS-5923\n)\ncompass-indexes: Update toolbar to leafygreen components (\nCOMPASS-5676\n)\nconnect: Add Save & Connect button (\nCOMPASS-5776\n)\nexplain-plan-helper: Add support for indexes in stages (\nCOMPASS-5878\n)\nBug Fixes:\nAutomatically refresh after CSFLE insert (\nCOMPASS-5806\n)\nImprove Binary handling (\nCOMPASS-5848\n)\nAlign elements on the create collection modal (\nCOMPASS-5921\n)\nconnection-form: Align advanced tab and input field widths\nimport: Pre-create an empty object before creating its properties (\nCOMPASS-5076\n)\nace-autocompleter: Provide a special snippet for $merge stage in ADL\nconnect-form: Connect to the newly created favourite (\nCOMPASS-5776\n)\ndocuments: Fixes to recent queries\nqueries: Ignore duplicate recent queries (\nCOMPASS-2237\n)\naggregation-explain: Show indexes (\nCOMPASS-5879\n)\nexplain-plan-helper: Use execution time of cursor stage (\nCOMPASS-5858\n)\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.32.2\nReleased June 7, 2022\nFixes an issue where\n$merge\nand\n$out\naggregation stages would\nnot appear in the pipeline builder when connected to a\nData Lake\n.\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.32.1\nReleased June 3, 2022\nNew Features:\nEnables In-Use Encryption (\nCOMPASS-5634\n)\nShow index keys in aggregation explain plan (\nCOMPASS-5857\n)\nBug Fixes:\nOpen aggregation pipeline in correct namespace (\nCOMPASS-5872\n)\nHide\n$documents\noperator in collection aggregations\n(\nCOMPASS-5843\n)\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.32.0\nReleased May 31, 2022\nNew Features:\nAdds explain plan for aggregations (\nCOMPASS-5788\n)\nAllows import into Queryable Encryption collections\n(\nCOMPASS-5810\n)\nBug Fixes:\nIn the pipeline builder, hide the stage error message when changing\nthe aggregation operator (\nCOMPASS-5684\n)\nRemove unique option on columnstore index creation\n(\nCOMPASS-5830\n)\nReconnect the SSH tunnel when it gets disconnected\n(\nCOMPASS-5454\n)\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.31.3\nReleased May 17, 2022\nNew Features:\nExport aggregation pipelines to Go.\nBump Node driver to version 4.6.0 and embedded shell to version 1.4.1\n(\nCOMPASS-5619\n).\nSupport columnstore indexes and clustered collections\n(\nCOMPASS-5665\n,\nCOMPASS-5666\n).\nAdd\nsrvMaxHosts\nto advanced connection options.\nDisable aggregation toolbar options when pipeline is invalid.\nStyle improvements.\nBug fixes:\nIn connection form, allow empty hosts.\nIn aggregation pipeline builder, update default document preview\namount from 10 to 20.\nResize elements for improved visibility.\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.31.2\nReleased April 14, 2022\nBug fixes:\nAggregation screen no longer shows a \"Cannot have two html5 backends\"\nerror. (\nCOMPASS-5655\n)\nConnections that use certificates no longer fail with \"option\nusesystemca is not supported\" (\nCOMPASS-5729\n)\nYou can edit null values in CRUD view (\nCOMPASS-5697\n)\nInvalid UUID values display correctly in CRUD view\n(\nCOMPASS-5726\n)\nEditing Int64 values in JSON view no longer changes their type to\nInt32 (\nCOMPASS-5710\n)\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.31.1\nReleased April 05, 2022\nBug Fixes:\nFixed \"rendering AggregationsPlugin\" error.\nCRUD Fixes\nFixed a bug that updated a documents data type to String when\nediting a field of data type Date in CRUD Document view.\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.31.0\nReleased March 31, 2022\nNew Features:\nNew connection experience\nAdd new form for Kerberos options.\nSupport loading system CA store.\nUse new favorite connection modal in sidebar.\nAdd support for MONGODB_AWS.\nAggregation and Query Improvements\nNew saved aggregation and queries view.\nAdd link and descriptions for the\n$densify\naggregation stage.\nAdd ability to export queries and aggregations to Ruby.\nUpdate aggregation result preview card styles.\nBug Fixes:\nSchema Tab Fixes\nFix shift selecting multiple items in schema tab.\nUnambiguously display latitude and longitude on map.\nCRUD Fixes\nAllow empty JSON input.\nIncrease input width for query bar max timeout ms area.\nMiscellaneous Fixes\nAdd\ndirectConnection=true\nwhen converting from old model.\nPick only specified columns when exporting data as JSON.\nHide SSH tunnel password.\nView this release on\nGitHub\n.\nAll JIRA issues closed in 1.31.0\n.\nMongoDB Compass\n1.30.1\nReleased January 13, 2022\nNew Features:\nAdd link and descriptions for the\n$documents\n.\naggregation stage.\nConnect form: Add SSL/TLS option radio box group.\nDatabases and Collections: Add async loading states for databases and\ncollections list.\nExport secrets methods and parse raw models.\nStyles: Add darkreader dark theme option.\nConnections: Add general tab contents to connect form.\nSupport MongoDB 5.2 aggregations.\nBug Fixes:\nRemove unused vars.\nSSH Tunnel: Remove unused import.\nBSON Transpilers: Account for bson Decimal128 validation changes.\nMake SSH tunnel use Socks5. You can now connect to replica sets and\nsharded clusters using an SSH tunnel.\nCompass Logging: Bump mongodb-log-writer to allow browser envs.\nMove theme menu from help to view in non mac/darwin.\nConnections:\nHide socks tab on SSH form.\nAdd SSH label.\nRemove compass-components from prod dependencies.\ntoggle-shell: Use key instead of keyCode.\ndata-service:\nDo not return name from adapted dbStats\nDo not ignore directConnection=false\nmocha-config-compass: Disable source map processing when running code\nin electron / web runtime in tests\nAll JIRA issues closed in 1.30.1\n.\nMongoDB Compass\n1.29.6\nReleased December 20, 2021\nNew Features:\nAdds loading states for collection in sidebar.\nImproves identification of Atlas cluster.\nImprove telemetry connection tracking.\nDependency Upgrades:\nBumps\nreact-ace\nto\nversion 9.5.0.\nBumps Node.js driver to version\n4.2.1\n.\nBump\nmongosh\nversion to\n1.1.6\n.\nBug Fixes:\nRemoves expired link from license.\nFixes error handling in\nlistCollections\n.\nKeeps\ntlsCertificateFile\nas URI parameter.\nHides full-text search stages for time series and views.\nDoes not overfetch\nconnectionInfo\nand update the state\ntoo often.\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.29.5\nReleased November 24, 2021\nBug Fixes:\nFixes connection with TLS / SSL options.\nFixes document searching for Serverless Atlas.\ncollStats\nnow always shows for collections on the\ncollection screen.\nCollection menu now appears when collection is selected.\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.29.4\nReleased November 16, 2021\nNew Features:\nAdds\nclient-side logging\nfor\nMongoDB Compass\noperations.\nImproved\nMongoDB Compass\nstartup time.\nAdds support for MongoDB 5.1 features.\nImproved reliability for connections.\nMongoDB Compass\nnow uses:\nElectron version 13\nNode version 14\nSupported Platforms:\nMongoDB Compass\nfor macOS can now run on M1 platforms that have Rosetta or\nRosetta 2 installed. For more information, see\nSoftware\nRequirements\n.\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.28.4\nReleased August 30, 2021\nNew Features:\nEnables resizing the preview area for aggregation pipelines.\nAllows hiding the\nQuery History\nand\nExport to Language\nbuttons in the query view.\nBumps\nmongosh\nversion for the embedded shell to\n1.0.4\n.\nBug Fixes:\nProperly supports all Kerberos options.\nFixed an issue with geospatial queries being incorrectly merged.\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.28.1\nReleased Jan 13, 2021\nNew Features:\nAdds support for load balancer connections.\nAdds a\nGranularity\noption when creating a time series\ncollection.\nDisallows editing schema validation for time series collections.\nBug Fixes:\nHides the\nDrop Collection\nbutton in readonly Compass.\nGeoqueries no longer populate query bar fields with\nnull\n.\nFull changelog available on GitHub\n.\nMongoDB Compass\n1.26.1\nReleased April 9, 2021\nNew Features:\nAllows functions in the query bar and aggregations.\nWhen navigating to the\nDatabases\nview,\nCompass\nnow clears a previously selected collection\nfrom the left navigation.\nUpdates the\nembedded MongoDB Shell\nto\nversion\n0.9.0\n.\nBug Fixes:\nCompass\nnow displays the expected value when you update\nfields in the table view.\nCreating a collection or database is now prohibited when form\nfields are empty.\nSchema tab graphs no longer fail to render when switching tabs.\nSSH tunnel no longer hangs on disconnect.\nMongoDB Compass\n1.26.0\nReleased March 3, 2021\nNew Features:\nAdds ability to create\ntext indexes\n.\nAdds ability to cancel a connection attempt.\nData is now refreshed when\nFind\nis clicked in the query\nbar.\nImprovements to\nschema analysis\nto prevent\ntimeouts with large datasets.\nBug Fixes:\nImproves connection form input and validation.\nMongoDB Compass prevents inserting data via the JSON editor without\nspecifying a document. Previously, Compass would silently error when a\ndocument was not specified.\nSaving a favorite connection no longer freezes MongoDB\nCompass.\nStylistic fixes.\nMongoDB Compass\n1.25.0\nReleased January 13, 2021\nNew Features:\nUpdates the\nembedded MongoDB Shell\nto\nversion\n0.6.1\n.\nImprovements to connection validation.\nBug Fixes:\nCompass no longer crashes during startup on certain versions of\nWindows. For more information see\nCOMPASS-4510\n.\nWhen connecting to a MongoDB deployment, Compass no longer\nautomatically inserts a value of\n27017\nfor\nPort\nwhen\nPort\nis left blank.\nCompass no longer displays the incorrect port number when connecting\nto MongoDB via SRV record.\nMongoDB Compass\n1.24.1\nReleased December 9, 2020\nNew Features:\nAdds support for updates on sharded collections.\nAdds support for the\nprint()\nmethod in the\nembedded MongoDB Shell\n.\nUpdates the\nembedded MongoDB Shell\nto\nversion\n0.5.2\n.\nProvides better readonly and view handling.\nAdds support for multi-line string editing in the field-by-field\neditor.\nProvides a descriptive tooltip when selecting an aggregation stage\nin the\nAggregation Pipeline Builder\n.\nBug Fixes:\nNon-editable fields can now be deleted in the field-by-field editor.\nFixes connection URI issues with SCRAM-SHA-256.\nAdds support for\n$out\nwhen connected to a\nData Lake\n.\nRemoves broken import and export sidebar actions.\nImproves\nx.509\nauthentication. Makes the\nx.509\nusername\noptional in connection validation and improves validation error\nmessages.\nVarious other bug fixes and improvements.\nMongoDB Compass\n1.23\nReleased November 4, 2020\nNotarizes\nMongoDB Compass\nfor macOS Catalina. You should no longer need to\nmanually allow macOS to trust\nMongoDB Compass\nbefore running.\nKerberos authentication improvements on RHEL7.\nImporting a text pipeline containing a\n$out\nstage no longer\ncrashes\nMongoDB Compass\n.\nVarious other bug fixes and improvements.\nMongoDB Compass\n1.22\nReleased September 3, 2020\nAdded an\nembedded MongoDB Shell\n. You\ncan use MongoDB Shell to test queries and operations in an interactive\nJavaScript interface.\nMongoDB Compass\n1.21\nReleased April 28, 2020\nImproved experience for\nimporting and exporting data\n.\nImproved CSV parsing when importing data.\nAdded support for importing a subset of fields from CSV.\nProvides guidance to upgrade from Community Edition. Community Edition\nis now deprecated. To learn more, see\nMigrate to Compass from Compass Community\n.\nVarious bug fixes and improvements.\nMongoDB Compass\n1.20\nNote\nOn macOS systems, the first time that you update\nMongoDB Compass\nto version\n1.20 or later, you will need to allow access to your system storage\nfor each\nsaved connection in\nRecents\nand\nFavorites\n. To learn more, see\nAllow Keychain Access for Recent and Favorite Connections\n.\nReleased December 5, 2019\nAdded the option to include driver syntax when\nexporting queries to a language\n.\nNew and improved\nConnection\nexperience\nwith support for all connection options.\nImproved user experience for saving and sharing\nFavorite Connections\n.\nAdded JSON mode for managing documents. With JSON mode, you can\nnow insert multiple documents at once.\nAdded support for querying UUIDs via the\nDocuments\nquery bar or in the\nAggregation Pipeline Builder\n.\nAdded support for the following aggregation pipeline operators:\n$set\n$unset\n$replaceWith\nImproved inline documentation for aggregation pipeline arguments.\nRemoved\n$limit\nahead of the\n$count\nstage in\nthe aggregation pipeline builder to ensure accurate counts on large\ncollections. Prior versions of\nMongoDB Compass\nplaced a\n$limit\nstage\nbefore\n$count\nstages in the\nAggregation Pipeline Builder\nfor large\ncollections, even when sample mode was disabled.\nVarious bug fixes and improvements.\nMongoDB Compass\n1.19\nReleased August 11, 2019\nAdded support for:\nViews\n. You can create\nviews based on results from an\naggregation pipeline\n.\nWildcard Indexes\n.\nKilling long-running operations from the\nPerformance Tab\n.\nAdjusting the maximum timeout\nfor\nqueries executed in the Query Bar.\nNew settings available in the\naggregation pipeline builder\n. You can\nnow specify a sample size, number of documents to preview, and a\nmaximum timeout for your pipeline operations.\nObscures fields encrypted with Field-Level Encryption. These fields\ncannot be modified by\nCompass\nusers.\nCompass\nnow warns users who are connected to non-genuine\nMongoDB servers. For more information, see\nthis entry in the FAQ\n.\nMongoDB Compass\n1.18\nReleased May 17, 2019\nProvided fixes to the\nCompass\nWindows installer. With the\nnew\n.msi\ninstaller you can:\nSelect the destination of the\nCompass\ninstallation.\nInstall\nCompass\nfor all users.\nScript the\nCompass\ninstallation and perform a quiet\ninstall.\nAdded support for Ubuntu 18.10 and other recent Linux distributions.\nNew\nSchema Validation\nexperience.\nAdded support for\nJSON schema validation\n.\nIncludes smart editor with autocomplete.\nFor macOS systems,\nCompass\nnow requires macOS 10.12 or greater.\nMongoDB Compass\n1.17\nReleased March 4, 2019\nPerformance improvements to the\nDocuments\nand\nAggregation\ntabs, specifically with\ndeeply nested documents.\nFixed several connection issues.\nFixed Kerberos connections where hostname is not the canonical\nname.\nFixed SRV connections with special characters in the password.\nCompass\nno longer allows direct connections to\nsecondary\ndatabases, which would result in hangs on the\nloading navigation screen.\nFixed connections to\nM0\nAtlas\nclusters with\nreadonly users.\nFixed issue where\nusersInfo\ncommand was not available\nto the data service.\nauthSource\nnow correctly defaults to\nadmin\nwhen connecting to\nAtlas\n.\nCompass\nnow properly forces a disconnect when requested.\nMongoDB Compass\n1.16\nReleased November 12, 2018\nAdded\ncollation\nsupport to the following features:\nCreate a Collection\nCreate a Database\nCreate an Index\nQuery Your Data\nCreate an Aggregation Pipeline\nAdded the ability to find text within a page using either\nCtrl + F\nor\nCommand + F\n, depending on your\nplatform.\nReduced the required permissions to use\nMongoDB Compass\n. As of this\nversion of\nMongoDB Compass\n, users require the\nread\npermission to access a database in\nCompass\n.\nUpdated dates to display in\nUTC\ntime.\nAdded support for\nSCRAM-SHA-256\nauthentication mechanism.\nVarious bug fixes and improvements.\nAs of this version, you should not provide a\nPassword\nwhen\nusing\nKerberos\nas the authentication mechanism.\nMongoDB Compass\n1.15\nReleased August 23, 2018\nAdded support for importing plain text pipelines into the\nAggregation Pipeline Builder\n.\nAdded support for\nexporting aggregation pipelines\nand\nexporting queries\nin the syntax of the following languages:\nJava\nNode\nC#\nPython 3\nMongoDB Compass\n1.14\nReleased June 26, 2018\nAdded\nAggregation Pipeline Builder\n,\nwhich provides the ability to execute\naggregation pipelines\nto\ngain additional insight into your data.\nAdded\nMongoDB Compass\nIsolated Edition\nfor highly secure\nenvironments. This edition does not initiate any network requests\nexcept to the MongoDB server.\nMongoDB Compass\n1.13\nReleased May 3, 2018\nAdded ability to\nimport and export data\nin\nJSON\nand\nCSV\nformat.\nMongoDB Compass\n1.12\nReleased March 5, 2018\nAdded\nMongoDB Compass\nReadonly Edition\nwhich limits certain\nCRUD operations\nwithin your organization.\nThe following actions are\nnot\npermitted in Compass Readonly Edition:\nCreate and drop databases\nCreate and drop collections\nCreate, delete, edit, and clone documents\nCreate and drop indexes\nCreate, delete, and edit document validation rules\nAll other functionality remains the same as in standard\nMongoDB Compass\n.\nAdded support for\nconnecting to Compass\nusing an\nSRV record\n. In the connect dialog, if\nCompass detects an SRV record URI on the clipboard it\nauto-completes the dialog based on the SRV record.\nMade various performance and stability inprovements to the documents tab.\nMongoDB Compass\n1.11\nReleased December 17, 2017\nAdded support for plugins\nthat extend the functionality of\nMongoDB Compass\n.\nAdded support for\ndisconnecting\nfrom the active\nMongoDB instance without restarting\nMongoDB Compass\n.\nAdded\nTable View\nfor documents as a\nmethod of viewing the contents of a collection in tabular format.\nMongoDB Compass\n1.10\nReleased Oct 25, 2017\nNow available in two editions, Compass Community and Compass.\nCompass provides the following features not in the Community edition:\nKerberos Authentication\nLDAP Authentication\nx509 Authentication\nSchema Analysis\nReal Time Server Stats\nDocument Validation\nMongoDB Compass\n1.9\nReleased Oct, 2017\nAdded autocomplete functionality to the query bar.\nQuery History\nCompass automatically stores up to 20 most recent queries for each\ncollection. From the past queries view for a collection, you can\nview the\nrecent\nqueries as well as the\nqueries saved as\nfavorites\n. For more\ninformation, see\nView Recent Queries\n.\nDeployment Awareness\nWhen a user connects to a\nMongoDB instance\n,\nCompass now displays:\nThe connection name if the connection is a favorite connection or\n\"My Cluster\" if it is not.\nThe type of deployment (standalone, replica set, sharded cluster).\nIf the deployment is a replica set and the replica set name is\nspecified in the connection window, the number of replica set\nmembers will also be displayed.\nFor more information, see\nCompass Home\n.\nMongoDB Compass\n1.8\nReleased Aug 2, 2017\nDocuments tab is the default\nSchema sampling only on demand\nExplain executed only on demand\nImproved Document Editing\nDeployment Awareness (and read preference)\nAdded ability to specify replica set name and read preference in\nconnection screen.\nAdded ability to parse MongoDB URI string in the connection screen.\nAllow typing index field names in addition to dropdown\nUse Client Meta Data to identify Compass application name in server logs\nNew Loading animation\nMongoDB Compass\n1.7\nReleased Jun 7, 2017\nAdded ability to include options in the\nquery bar\n.\nAdded ability to add or delete database/collection from the left-hand navigation sidebar.\nAdded ability to collapse the left-hand navigation sidebar.\nMongoDB Compass\n1.6\nReleased Mar 1, 2017\nAdded support for Linux: Ubuntu 14.04+ and RHEL 7+.\nAdded ability to zoom in and zoom out of panels.\nMongoDB Compass\n1.5\nReleased Nov 29, 2016\nAdded ability to\ncreate\nand\ndrop\ndatabases.\nAdded ability to\ncreate\nand\ndrop\ncollections.\nAdded ability to\ncreate indexes\n.\nAdded support for\ndocument validation\n.\nImproved security when connecting to Atlas. During Connection setup,\nMongoDB Compass\nsupports the use of System Certificate Authority for\nTLS/SSL connections to Atlas Deployment.\nProvides\nReal Time Performance\nstats.\nMongoDB Compass\n1.4\nReleased Nov 1, 2016\nAdd support for connecting to Atlas.\nVarious bug fixes and improvements.\nMongoDB Compass\n1.3\nReleased Sep 15, 2016\n1.3.0-beta.3 - Jan 12, 2016\nAllow specifying the value of the\n_id\nfield when inserting new\ndocuments.\nSet the default field and value sizes to\n1\nwhen adding a new key\nto a document.\nTyping \"\n:\n\" in the key input field tabs to the value input field when\nediting a document.\nOnly allow addition of one element at a time if the field name in the\nnew element is blank when editing a document.\nCRUD documentation now available in the application help menu.\nFix element autofocus on add.\n1.3.0-beta.2 - June 29, 2016\nBug: Small Bug Fixes identified at MongoDB World\n1.3.0-beta.0 - June 27, 2016\nNew: CRUD single document create, update, delete\nNew: SSH tunnel support\nNew: Tree explain plan view\nNew: Geographic query builder and visualization\nExplicit opt-in for \"3rd party maps\" usage\nImprove display of binary data in in the document viewer\nQuery builder on numeric histograms should leave bounds open\nIntercom overlay button now visible\nLoad preferences defensively, catching parsing errors and reset preferences\nCompass Treasure Hunt for MongoDB World\nMongoDB Compass\n1.2\nReleased Jun 27, 2016\nBeta installs alongside the stable release as a distinct application,\nwith distinct preferences\nIndex view\nExplain plan view\nDocuments view moved to a separate tab\nAutomatic updates\n1.2.0-beta.3 - June 23, 2016\nBug: Feature Tour does not show on first launch\nBug: Compass fails to start with JavaScript error in main process:\nSyntaxError: Unexpected end of input\nBug: No error displayed message when an authentication error occurs\nBug: Compass does not handle $indexStats fetch failure on MongoDB 3.2\n1.2.0-beta.2 - June 1, 2016\nNEW: Added explain plan view\nAdded feature tour points of new 1.2 features\nBugfix: After increasing maxTimeMS timeout, query default falls back\nto 100 docs\n1.2.0-beta.1 - May 10, 2016\nMongoDB Compass\n1.1\nReleased Jan 19, 2016\n1.1.1 -- Jan 19, 2016\nMongoDB Compass\n1.0\nReleased Dec 7, 2015\nMongoDB Compass 1.0 is now available. It helps users\nto visually analyze and understand their MongoDB data.\n1.0.1 -- Dec 18, 2015\nBug Fixes\nQuery builder bug in unique minicharts when resetting\nHang: Do something graceful after closing/opening laptop lid & using\nCompass again\nError in\nCompass.app/Contents/Resources/app/node_modules/scout-server/lib/models/token.js:20\nPass\nreadPreference.nearest\nin\nlib/routes/collection.js\nEnterprise/Community version not correctly detected for MongoDB 2.6, 3.0\nCompass hangs when upper case host alias used in connection\nReduce reservoir sampling limit to 10,000 documents\nPossible race condition when reading from IndexedDB\nCannot access DBs with colon (\":\") in their name\nCannot read property 'authInfo' of undefined in mongodb-instance-model/lib/fetch.js:297\nCannot access DBs with octothorp (\n#\n) in their name\nFailure to sample on first collection selected\nImprovements\nObjectID visualization missing last tooltip\nChange intercom message/menu item to \"Provide Feedback\"\nOpen external links in user's web browser, not Electron\nPlace SSL \"Client Certificate\" field above \"Client Private Key\"\nRe-enable highlighting/selecting of some UI elements\nReplace Help entry stubs with actual text\nUse consistent titles across windows\nSimplify language in opt-in panel\nReduce font size of header to accommodate full db.collection name\nRemove \"–\" (minimize) on Intercom\nRun shrinkwrap to lock Compass 1.0 dependencies\nConfirm successful Evergreen builds from release-1 branch\nCompass fails to connect to hostname\nBack\nLearn More\nNext\nSubmit Feedback",
    "url": "https://www.mongodb.com/docs/compass/release-notes/",
    "source": "mongodb",
    "doc_type": "compass",
    "scraped_at": 31798.8850913
  },
  {
    "title": "Back Up, Restore, and Archive Data",
    "content": "Docs Home\n/\nAtlas\nBack Up, Restore, and Archive Data\nCopy page\nBackups are copies of your data that encapsulate the state of your\ncluster at a given time. Backups provide a safety measure in the\nevent of data loss. If you have strict data protection requirements,\nyou can\nenable a Backup Compliance Policy\nto protect\nyour backup data.\nRequired Access\nTo manage or restore backups for a cluster, you must have\nProject Backup Manager\nor\nProject Owner\naccess\nto the project.\nUsers with\nOrganization Owner\naccess must add themselves as a\nProject Backup Manager\nor\nProject Owner\nto the project before they can manage or restore\nbackups.\nConsiderations\nBe aware that:\nAtlas\nbackups are not available for\nM0\nFree clusters.\nYou may use\nmongodump\nto back\nup your\nM0\ncluster data and\nmongorestore\nto restore that\ndata. To learn how to manually back up your data, see\nCommand Line Tools\n.\nYou can't write to your cluster while\na backup restore is in progress for that cluster.\nYou can restore a backup only to a cluster running either:\nThe same major release version with an equal or higher minor\nversion. For example, if you create a backup for a cluster running\nMongoDB 8.1.x, you can restore this backup to a cluster running\nany other 8.1.x version, or any 8.y.x version where\ny >\n1\n, but you can't restore it to any 8.0.x version.\nThe next higher major release version. For example, if you create\na backup for a cluster running MongoDB 8.y.x, you can restore this\nbackup to a cluster running 9.y.x, but you can't restore it to any\n7.y.x version.\nIf the backup has a pinned FCV, the major version of the target\ncluster must match the major version of that pinned FCV.\nCloud Backups\nAvailable in M10+ Clusters.\nAtlas\nuses the native snapshot capabilities of your cloud provider\nto support full-copy snapshots and localized snapshot storage.\nAtlas\nsupports\nCloud Backups\non:\nMicrosoft Azure\nAmazon Web Services (AWS)\nGoogle GCP\nTo learn more, see\nBack Up Your Cluster\n.\nTo learn how to restore cluster from a Cloud Backup, see\nRestore from a Scheduled or On-Demand Snapshot\n.\nSnapshots for Flex Clusters\nImportant\nAs of February 2025, you can create Flex clusters, and can no longer\ncreate\nM2\nand\nM5\nclusters or Serverless instances in the\nAtlas UI, Atlas CLI, Atlas Administration API,\nAtlas Kubernetes Operator\n, HashiCorp Terraform,\nor\nAtlas\nCloudFormation Resources.\nYou can still use existing Serverless instances.\nAtlas\nno longer supports\nM2\nand\nM5\nclusters.\nAtlas\ndeprecated Serverless instances. As of May 25, 2025,\nAtlas\nhas automatically migrated all existing\nM2\nand\nM5\nclusters to Flex clusters.\nFor Serverless instances, beginning May 5 2025,\nAtlas\nwill\ndetermine whether to migrate instances to Free clusters,\nFlex clusters, or Dedicated clusters according to your usage.\nTo see which tiers\nAtlas\nwill migrate your instances\nto, consult the\nAll Clusters\npage in the Atlas UI.\nBackups are automatically enabled for Flex clusters and can't be disabled.\nAtlas\ntakes daily snapshots of your Flex clusters, which you can\nrestore to Flex cluster or\nM10\nand greater tiers.\nTo learn more about backing up your cluster, see\nFlex Cluster Backups\n.\nTo learn more about restoring your cluster, refer to\nRestore from a Scheduled or On-Demand Snapshot\n.\nSnapshots for Serverless Instances (Deprecated)\nImportant\nAs of February 2025, you can create Flex clusters, and can no longer\ncreate\nM2\nand\nM5\nclusters or Serverless instances in the\nAtlas UI, Atlas CLI, Atlas Administration API,\nAtlas Kubernetes Operator\n, HashiCorp Terraform,\nor\nAtlas\nCloudFormation Resources.\nYou can still use existing Serverless instances.\nAtlas\nno longer supports\nM2\nand\nM5\nclusters.\nAtlas\ndeprecated Serverless instances. As of May 25, 2025,\nAtlas\nhas automatically migrated all existing\nM2\nand\nM5\nclusters to Flex clusters.\nFor Serverless instances, beginning May 5 2025,\nAtlas\nwill\ndetermine whether to migrate instances to Free clusters,\nFlex clusters, or Dedicated clusters according to your usage.\nTo see which tiers\nAtlas\nwill migrate your instances\nto, consult the\nAll Clusters\npage in the Atlas UI.\nAtlas\nuses the native snapshot capabilities of your cloud provider\nto support full-copy snapshots and localized snapshot storage.\nBackups are automatically enabled for Serverless instances.\nYou can't disable Serverless instance backups.\nAtlas\noffers the following backup options for\nServerless instances:\nOption\nDescription\nServerless Continuous Backup\nServerless instances are\ndeprecated\n.\nYou can't create new Serverless instances, but you can still\nconfigure their backup.\nAtlas\ntakes incremental\nsnapshots\nof the data in your\nServerless instance every six hours and lets you restore the\ndata from a selected point in time within the last 72 hours.\nAtlas\nalso takes daily snapshots and retains these daily\nsnapshots for 35 days. To learn more, see\nCosts for Serverless Instances (Deprecated)\n.\nBasic Backup\nAtlas\ntakes incremental\nsnapshots\nof the data in your\nServerless instance every six hours and retains only the two\nmost recent snapshots. You can use this option for free.\nYou can restore Serverless instance snapshots to other\nServerless instances and dedicated clusters.\nTo learn more, see:\nConfigure Backup for a Serverless Instance (Deprecated)\nBackups for Serverless Instances (Deprecated)\nRestore from a Scheduled or On-Demand Snapshot\nBack\nChangelog\nNext\nBackup",
    "url": "https://www.mongodb.com/docs/atlas/backup-restore-cluster/",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31799.8199748
  },
  {
    "title": "Back Up Your Cluster",
    "content": "Docs Home\n/\nAtlas\n/\nBackup, Restore, and Archive\nBack Up Your Cluster\nCopy page\nNote\nFeature Unavailable in Free-Tier Clusters\nThis feature is not available for\nM0\nFree clusters. To learn\nmore about which features are unavailable, see\nAtlas\nM0 (Free Cluster) Limits\n.\nAtlas\nCloud Backups provide localized backup storage using the\nnative snapshot functionality of the cluster's cloud service provider.\nAtlas\nsupports cloud backup for clusters served on:\nMicrosoft Azure\nAmazon Web Services (AWS)\nGoogle Cloud Platform (GCP)\nYou can enable cloud backup during the\ncluster creation\nor during the\nmodification of an existing cluster\n.\nFrom the cluster configuration modal, toggle\nTurn on Cloud Backup\nto\nYes\n.\nIf you have strict data protection requirements,\nyou can\nenable a Backup Compliance Policy\nto protect\nyour backup data.\nCloud Backups inherit the snapshot redundancy native to your cluster's\ncloud provider. As such, Cloud Backups have at least the following\nredundancies depending on the cloud provider:\nAWS\nstores objects on multiple devices across a minimum of three\nAvailability Zones in an\nAWS\nRegion.\nMicrosoft Azure uses locally redundant storage (LRS) which replicates your\ndata three times within a single data center in the selected region.\nGoogle Cloud spreads your data across multiple zones in the backup region.\nTo ensure greater redundancy for your Cloud Backups, you can\nalso enable\nMulti-Region Snapshot Distribution\nin\nAtlas\n. This automatically creates copies of your snapshots and oplogs\nand stores them in other\nAtlas\nregions. With snapshots distributed\nacross multiple regions, you can still restore your cluster if the primary\nregion goes down.\nRequired Access\nTo manage or restore backups for a cluster, you must have\nProject Backup Manager\nor\nProject Owner\naccess\nto the project.\nUsers with\nOrganization Owner\naccess must add themselves as a\nProject Backup Manager\nor\nProject Owner\nto the project before they can manage or restore\nbackups.\nLimitations of Cloud Backup\nCloud Backups support sharded clusters. You cannot restore an\nexisting snapshot to a cluster after you add or remove a shard\nfrom it. You may restore an existing snapshot to another cluster\nwith a matching topology.\nIn single-region and multi-region sharded clusters,\nAtlas\ncreates\na separate backup for each shard in the shard's primary region.\nBack\nBackup, Restore, and Archive\nNext\nDedicated Cluster",
    "url": "https://www.mongodb.com/docs/atlas/backup/cloud-backup/overview/#std-label-backup-cloud-provider",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31800.7625392
  },
  {
    "title": "Databases and Collections",
    "content": "Docs Home\n/\nLanguages\n/\nC++\n/\nC++ Driver\nDatabases and Collections\nCopy page\nOverview\nIn this guide, you can learn how to use the C++ driver to interact\nwith MongoDB databases and collections.\nMongoDB organizes data into a hierarchy of the following levels:\nDatabases\n: Top-level data structures in a MongoDB deployment that store collections.\nCollections\n: Groups of MongoDB documents. They are analogous to tables in relational databases.\nDocuments\n: Units that store literal data such as string, numbers, dates, and other embedded documents.\nFor more information about document field types and structure, see the\nDocuments\nguide in the MongoDB Server manual.\nAccess a Database\nYou can access a database by calling the\ndatabase()\nfunction on\na\nmongocxx::client\nobject and passing the name of the database as\nan argument.\nThe following example accesses a database named\n\"test_database\"\n:\nauto\ndb\n= client.\ndatabase\n(\n\"test_database\"\n)\n;\nAlternatively, you can use the\n[]\noperator on a\nmongocxx::client\nas a shorthand for the\ndatabase()\nfunction, as shown in the following code:\nauto\ndb\n= client[\n\"test_database\"\n];\nAccess a Collection\nYou can access a collection by calling the\ncollection()\nfunction on\na\nmongocxx::database\nobject and passing the name of the collection as\nan argument.\nThe following example accesses a collection named\n\"test_collection\"\n:\nauto\ncoll\n= database.\ncollection\n(\n\"test_collection\"\n)\n;\nAlternatively, you can use the\n[]\noperator on a\nmongocxx::database\nas a shorthand for the\ncollection()\nfunction, as shown in the following code:\nauto\ncoll\n= database[\n\"test_collection\"\n];\nTip\nIf the provided collection name does not already exist in the database,\nMongoDB implicitly creates the collection when you first insert data\ninto it.\nCreate a Collection\nYou can use the\ncreate_collection()\nfunction to explicitly create a collection in a\nMongoDB database.\nThe following example creates a collection called\n\"example_collection\"\n:\nauto\ncoll\n= database.\ncreate_collection\n(\n\"example_collection\"\n)\n;\nYou can specify collection options, such as maximum size and document\nvalidation rules, by passing them inside a BSON document as the second parameter\nto the\ncreate_collection()\nfunction. For a full list of\noptional parameters, see the\ncreate command\ndocumentation in the MongoDB Server manual.\nGet a List of Collections\nYou can retrieve a list of collections in a database by calling the\nlist_collections()\nfunction. The function returns a cursor containing all\ncollections in the database and their associated metadata.\nThe following example calls the\nlist_collections()\nfunction and iterates over\nthe cursor to print the results:\nauto\ncursor\n= database.\nlist_collections\n()\n;\nfor\n(\nauto\n&& doc : cursor)\n{\nstd::\ncout <<\nbsoncxx::\nto_json\n(doc)\n<<\nstd::\nendl;\n}\nVIEW OUTPUT\nCollection: { \"name\" : \"test_collection\", \"type\" : \"collection\", ...}\nCollection: { \"name\" : \"example_collection\", \"type\" : \"collection\", ... }\nTo query for only the names of the collections in the database, call the\nlist_collection_names()\nfunction as shown in the following example:\nauto\nlist\n= database.\nlist_collection_names\n()\n;\nfor\n(\nauto\n&& name : list)\n{\nstd::\ncout << name <<\nstd::\nendl;\n}\nVIEW OUTPUT\ntest_collection\nexample_collection\nFor more information about iterating over a cursor, see the\nAccess Data From a Cursor\nguide.\nDelete a Collection\nYou can delete a collection from the database by using the\ndrop()\nfunction.\nThe following example deletes the\n\"test_collection\"\ncollection:\nauto\ncoll\n= database[\n\"test_collection\"\n];\ncoll.\ndrop\n()\n;\nWarning\nDropping a Collection Deletes All Data in the Collection\nDropping a collection from your database permanently deletes all\ndocuments and all indexes within that collection.\nDrop a collection only if the data in it is no longer needed.\nConfigure Read and Write Operations\nYou can control how the driver routes read operations by setting a\nread preference\n.\nYou can also control options for how the driver waits for acknowledgment of\nread and write operations on a replica set by setting a\nread concern\nand a\nwrite concern\n.\nBy default, databases inherit these settings from the\nmongocxx::client\nobject,\nand collections inherit them from the database. However, you can change these\nsettings by using one of the following functions on your database or collection:\nread_preference()\nread_concern()\nwrite_concern()\nTo learn more about read and write settings, see the following guides in the\nMongoDB Server manual:\nRead Preference\nRead Concern\nWrite Concern\nConfigure Database Settings\nThis example shows how to configure read settings for your database\nby using the following functions:\nread_preference()\n: Sets the read preference to\nk_secondary\nread_concern()\n: Sets the read concern to\nk_majority\nauto\ndb\n= client[\n\"test_database\"\n];\nmongocxx::\nread_preference rp;\nrp.\nmode\n(mongocxx::read_preference::read_mode::k_secondary)\n;\nmongocxx::\nread_concern rc;\nrc.\nacknowledge_level\n(mongocxx::read_concern::level::k_majority)\n;\ndb.\nread_preference\n(rp)\n;\ndb.\nread_concern\n(rc)\n;\nTip\nTo see a description of each read preference and read concern option, see the\nfollowing API documentation:\nRead preference modes\nRead concern levels\nConfigure Collection Settings\nThis example shows how to specify your collection's read and write concern\nby using the following functions:\nread_concern()\n: Sets the read concern to\nk_local\nwrite_concern()\n: Sets the write concern to\nk_acknowledged\nauto\ncoll\n= client[\n\"test_database\"\n][\n\"test_collection\"\n];\nmongocxx::\nread_concern rc;\nrc.\nacknowledge_level\n(mongocxx::read_concern::level::k_local)\n;\nmongocxx::\nwrite_concern wc;\nwc.\nacknowledge_level\n(mongocxx::write_concern::level::k_acknowledged)\n;\ncoll.\nread_concern\n(rc)\n;\ncoll.\nwrite_concern\n(wc)\n;\nTip\nTo see a description of each read and write concern level, see the\nfollowing API documentation:\nRead concern levels\nWrite concern levels\nTag Sets\nIn the MongoDB Server, you can apply key-value\ntags\nto replica-set\nmembers according to any criteria you choose. You can then use\nthose tags to target one or more members for a read operation.\nBy default, the C++ driver ignores tags\nwhen choosing a member to read from. To instruct the C++ driver\nto prefer certain tags, create a\nmongocxx::read_preference\nobject\nand call its\ntags()\nmember function. Pass your preferred tags as\nan array argument to\ntags()\n.\nIn the following code example, the tag set passed to the\ntags()\nfunction instructs the C++ driver to prefer reads from the\nNew York data center (\n\"dc\": \"ny\"\n) and to fall back to the San Francisco data\ncenter (\n\"dc\": \"sf\"\n):\nauto\ntag_set_ny\n=\nmake_document\n(\nkvp\n(\n\"dc\"\n,\n\"ny\"\n)\n)\n;\nauto\ntag_set_sf\n=\nmake_document\n(\nkvp\n(\n\"dc\"\n,\n\"sf\"\n)\n)\n;\nmongocxx::\nread_preference rp;\nrp.\nmode\n(mongocxx::read_preference::read_mode::k_secondary)\n;\nrp.\ntags\n(\nmake_array\n(tag_set_ny, tag_set_sf)\n.\nview\n()\n)\n;\nLocal Threshold\nIf multiple replica-set members match the read preference and tag sets you specify,\nthe C++ driver reads from the nearest replica-set members, chosen according to\ntheir ping time.\nBy default, the driver uses only those members whose ping times are within 15 milliseconds\nof the nearest member for queries. To distribute reads between members with\nhigher latencies, include the\nlocalThresholdMS\nparameter in your connection string URI.\nThe following example connects to a MongoDB deployment running on\nlocalhost:27017\nand specifies a local threshold of 35 milliseconds:\nmongocxx\n::\nuri\nuri\n(\n\"mongodb://localhost:27017/?localThresholdMS=35\"\n)\n;\nmongocxx\n::\nclient\nclient\n(uri)\n;\nIn the preceding example, the C++ driver distributes reads between matching members\nwithin 35 milliseconds of the closest member's ping time.\nAPI Documentation\nTo learn more about any of the functions discussed in this\nguide, see the following API documentation:\ndatabase()\ncollection()\ncreate_collection()\nlist_collections()\nlist_collection_names()\ndrop()\nread_preference()\nread_concern()\nwrite_concern()\nBack\nTransactions\nNext\nIndexes",
    "url": "https://www.mongodb.com/docs/languages/cpp/cpp-driver/current/databases-collections/",
    "source": "mongodb",
    "doc_type": "general",
    "scraped_at": 31801.4807756
  },
  {
    "title": "Connect from the Atlas CLI",
    "content": "Docs Home\n/\nMongoDB Atlas\n/\nAtlas CLI\nConnect from the Atlas CLI\nCopy page\nSelect a Connection Method\nWhen you connect to an existing\nAtlas\naccount from the\nAtlas CLI, you can authenticate with one of the following commands:\nCommand\nAuthentication Method\nUse Case\natlas auth login\nAtlas\nlogin credentials and an authentication token\nBest for non-programmatic use\natlas config init\nAPI\nkeys\nBest for programmatic use\nImportant\nAPI\nkeys are stored in plaintext in the Atlas CLI\nconfiguration file. Your\nAPI\nkeys are like passwords.\nEnsure that you secure the configuration file appropriately.\nTo create a new\nAtlas\naccount or onboard an existing account that\ndoesn't have any clusters, see\nGet Started with\nAtlas\n.\nSelect a use case below to learn more about the available connection\noptions:\nNon-programmatic Use\nProgrammatic Use\nUse the\natlas auth login\ncommand to authenticate with your\nAtlas\nlogin credentials and a one-time authentication token.\natlas auth\nlogin\nrequires manual login and verification of an authentication token,\nwhich is valid for 12 hours.\nAPI\nkeys are\noptional when connecting with\natlas auth login\n.\nAfter you run\natlas auth login\n, you can:\nConnect with minimum required settings\nand specify\nthe\n--projectId\nand\n--orgId\nflags with each command. This is the\nquickest way to get started for first-time login.\nSave your connection settings in a\nprofile\n.\nProfiles\nstore the project IDs, organization\nIDs, and, optionally,\nAPI\nkeys to use in future Atlas CLI\nsessions. To save time, you can specify a profile instead of using the\n--projectId\nand\n--orgId\nflags with each command.\nYou must\nconfigure API keys\nto\nauthenticate with this command.\nWhen you run the\natlas config init\ncommand, the Atlas CLI prompts you to provide\nyour\nAPI\nkeys and automatically creates a profile that\nstores the\nAPI\nkeys.\natlas config init\nis best for programmatic use because\nit doesn't require manual login or\ntoken verification.\nWhen you use connect with\natlas config init\n, you can:\nConnect with minimum required settings\nand\nspecify the\n--projectId\nand\n--orgId\nflags with each command. This\nis the quickest way to get started for first-time login.\nSave additional connection settings in a\nprofile\n.\nProfiles\nstore the project IDs, organization\nIDs, and, optionally,\nAPI\nkeys to use in future Atlas CLI\nsessions. To save time, you can specify a profile instead of using the\n--projectId\nand\n--orgId\nflags with each command.\nConnect With Minimum Required Settings\nSelect a use case and follow the steps to connect from the Atlas CLI with\nminimum required settings.\nComplete the Prerequisites\nNon-programmatic Use\nProgrammatic Use\nInstall the Atlas CLI\n.\nAdd your host's IP address to the\nIP access list\n.\nIf you authenticate with your\nAtlas\nuser credentials\nand your organization's owners enable\nIP access list for the Atlas UI for an organization\n,\nyour IP address must be added to the IP access list to run commands\nin this organization. To learn more, see\nRequire IP Access List for the Atlas UI\n.\nInstall the Atlas CLI\n.\nConfigure API keys\n.\nFollow These Steps\nSelect a use case and follow the procedure to quickly connect\nfrom the Atlas CLI.\nNon-programmatic Use\nProgrammatic Use\n1\nRun the authentication command.\nRun the\natlas auth login\ncommand in your terminal.\natlas auth login\nThe command opens a browser window and returns a one-time\nactivation code. This code expires after 10 minutes.\n2\nSign into\nAtlas\n.\nIf you aren't signed in already, sign into your\nAtlas\naccount\nin the browser.\n3\nEnter the authorization code.\nPaste your activation code into the browser and click\nConfirm Authorization\n.\n4\nReturn to the Atlas CLI and accept the default profile options.\nReturn to the terminal. If you connect successfully, you see a\nmessage:\nSuccessfully logged in as {Your Email Address}.\nAccept the default profile configuration by pressing\nEnter\nif the following options display:\nDefault Org ID\nDefault Project ID\nDefault Output Format\nDefault MongoDB Shell Path\nImportant\nIf you previously set up any profiles with\nAPI\nkeys for\nMongoCLI, your profiles\nmigrate automatically\n.\nSome migrated profiles might cause Atlas CLI commands to fail\nwhen authenticating\nwith\natlas auth login\n. The following message displays\nwhen you run\natlas auth login\nif there is a conflict:\nThere was an error fetching your organizations: Global user is\nfrom outside access listed subnets.\nTo fix the conflict, open the\nconfiguration file\n, remove the default profile, and\nrun\natlas auth login\nagain.\n5\nIssue commands using the\n--projectId\nand\n--orgId\nflags.\nWhen you run\nAtlas CLI commands\nfor\nthe duration of your Atlas CLI session, specify your Project ID and Org ID\nusing the\n--projectId\nand\n--orgId\nflags.\nExample\natlas alerts list --projectId 60b3c81153cf986293e2608b\n1\nRun the authentication command.\nRun the\natlas config init\ncommand in your terminal.\natlas config init\n2\nEnter your\nAPI\nkeys.\nEnter your public and private keys when prompted.\n3\nAccept the default profile options.\nAccept the remaining default profile options by pressing\nEnter\nwhen the following options display:\nDefault Org ID\nDefault Project ID\nDefault Output Format\nDefault MongoDB Shell Path\n4\nIssue commands using the\n--projectId\nand\n--orgId\nflags.\nWhen you run\nAtlas CLI commands\nfor\nthe duration of your Atlas CLI session, specify your Project ID and Org ID\nusing the\n--projectId\nand\n--orgId\nflags.\nExample\natlas alerts list --projectId 60b3c81153cf986293e2608b\nTake the Next Steps\nStart using the\nAtlas CLI commands\n.\nTo save connection settings by modifying the default profile or create a\ndifferent profile, see\nSave Connection Settings\n.\nBack\nVerify Packages\nNext\nSave Connection Settings",
    "url": "https://www.mongodb.com/docs/atlas/cli/current/connect-atlas-cli/",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31801.7491306
  },
  {
    "title": "Keyboard Shortcuts",
    "content": "Docs Home\n/\nCompass\nKeyboard Shortcuts\nCopy page\nKeyboard shortcuts enable you to easily navigate\nCompass\n.\nCategory\nDescription\nWindows\nMac\nMenu\nOnline help\nF1\nF1\nMenu\nQuit application\nCtrl\n+\nQ\nCmd\n+\nQ\nMenu\nShow settings\nCtrl\n+\n,\nCmd\n+\n,\nMenu\nHide window\nCtrl\n+\nH\nCmd\n+\nH\nMenu\nHide other windows\nCtrl\n+\nShift\n+\nH\nCmd\n+\nShift\n+\nH\nMenu\nOpen a new Compass window\nCtrl\n+\nN\nCmd\n+\nN\nMenu\nClose current window\nCtrl\n+\nShift\n+\nW\nCmd\n+\nShift\n+\nW\nMenu\nShare schema as JSON\nAlt\n+\nCtrl\n+\nS\nAlt\n+\nCmd\n+\nS\nMenu\nReload screen\nCtrl\n+\nShift\n+\nR\nCmd\n+\nShift\n+\nR\nMenu\nReload data\nCtrl\n+\nR\nCmd\n+\nR\nMenu\nToggle sidebar\nCtrl\n+\nShift\n+\nD\nCmd\n+\nShift\n+\nD\nMenu\nActual size\nCtrl\n+\n0\nCmd\n+\n0\nMenu\nZoom in\nCtrl\n+\n=\nCmd\n+\n=\nMenu\nZoom out\nCtrl\n+\n-\nCmd\n+\n-\nMenu\nToggle DevTools\nAlt\n+\nCtrl\n+\nI\nAlt\n+\nCmd\n+\nI\nMenu\nMinimize\nCtrl\n+\nM\nCmd\n+\nM\nGeneral\nUndo\nCtrl\n+\nZ\nCmd\n+\nZ\nGeneral\nRedo\nCtrl\n+\nShift\n+\nZ\nCmd\n+\nShift\n+\nZ\nGeneral\nCut\nCtrl\n+\nX\nCmd\n+\nX\nGeneral\nCopy\nCtrl\n+\nC\nCmd\n+\nC\nGeneral\nPaste\nCtrl\n+\nV\nCmd\n+\nV\nGeneral\nSelect all\nCtrl\n+\nA\nCmd\n+\nA\nGeneral\nFind\nCtrl\n+\nF\nCmd\n+\nF\nWorkspace\nNavigate to next tab\nCtrl\n+\nShift\n+\n]\nCmd\n+\nShift\n+\n]\nWorkspace\nNavigate to previous tab\nCtrl\n+\nShift\n+\n[\nCmd\n+\nShift\n+\n[\nWorkspace\nClose current tab\nCtrl\n+\nShift\n+\nW\nCmd\n+\nShift\n+\nW\nWorkspace\nOpen new tab\nCtrl\n+\nT\nCmd\n+\nT\nAggregation Focus Mode\nAdd a new stage after current one\nCtrl\n+\nShift\n+\nB\nCmd\n+\nShift\n+\nB\nAggregation Focus Mode\nAdd a new stage before current one\nCtrl\n+\nShift\n+\nA\nCmd\n+\nShift\n+\nA\nAggregation Focus Mode\nNavigate to next stage\nCtrl\n+\nShift\n+\n9\nCmd\n+\nShift\n+\n9\nAggregation Focus Mode\nNavigate to previous stage\nCtrl\n+\nShift\n+\n0\nCmd\n+\nShift\n+\n0\nPipeline text editor\nComment out code\nCtrl\n+\n/\nCmd\n+\n/\nAll text editors\nPrettify code\nCtrl\n+\nShift\n+\nB\nCtrl\n+\nShift\n+\nB\nQuery bar\nSubmit query\nEnter\n(from the query bar)\nEnter\n(from the query bar)\nWeb Shell\nDeletes the next character\nCtrl\n+\nD\nCtrl\n+\nD\nWeb Shell\nMoves the cursor to the end of the line\nCtrl\n+\nE\nCtrl\n+\nE\nWeb Shell\nMoves the cursor forward one character\nCtrl\n+\nF\nCtrl\n+\nF\nWeb Shell\nErases one character, similar to hitting backspace\nCtrl\n+\nH\nCmd\n+\nH\nWeb Shell\nClears the screen, similar to the clear command\nCtrl\n+\nL\nCmd\n+\nL\nWeb Shell\nSwap the last two characters before the cursor\nCtrl\n+\nT\nCtrl\n+\nT\nWeb Shell\nCycle backwards through command history\n↑\n↑\nWeb Shell\nCycle forwards through command history\n↓\n↓\nBack\nConnection Errors\nNext\nFAQ",
    "url": "https://www.mongodb.com/docs/compass/keyboard-shortcuts/",
    "source": "mongodb",
    "doc_type": "compass",
    "scraped_at": 31802.0276531
  },
  {
    "title": "Costs for Serverless Instances (Deprecated)",
    "content": "Docs Home\n/\nAtlas\n/\nManage Billing\nCosts for Serverless Instances (Deprecated)\nCopy page\nImportant\nAs of February 2025, you can create Flex clusters, and can no longer\ncreate\nM2\nand\nM5\nclusters or Serverless instances in the\nAtlas UI, Atlas CLI, Atlas Administration API,\nAtlas Kubernetes Operator\n, HashiCorp Terraform,\nor\nAtlas\nCloudFormation Resources.\nYou can still use existing Serverless instances.\nAtlas\nno longer supports\nM2\nand\nM5\nclusters.\nAtlas\ndeprecated Serverless instances. As of May 25, 2025,\nAtlas\nhas automatically migrated all existing\nM2\nand\nM5\nclusters to Flex clusters.\nFor Serverless instances, beginning May 5 2025,\nAtlas\nwill\ndetermine whether to migrate instances to Free clusters,\nFlex clusters, or Dedicated clusters according to your usage.\nTo see which tiers\nAtlas\nwill migrate your instances\nto, consult the\nAll Clusters\npage in the Atlas UI.\nServerless instances offer pay-per-operation pricing, meaning that\nyou only pay for the Processing Units consumed by your database\noperations and storage consumed by your data and indexes. You do not\nneed to specify a cluster size since serverless instances seamlessly\nscale to accommodate changes in workload traffic.\nServerless instances\nmay be more cost effective for applications with low or intermittent\ntraffic. To learn more about Serverless instances and use cases,\nsee\nChoose a Cluster Type\n.\nNote\nExpiration operations\ntriggered by\nTTL Indexes\nmight generate\nRPU\ns and\nWPU\ns . The volume of these additional processing\nunits typically incurs only a small fee relative to your overall\ncosts.\nUsage Cost Summary\nOperation pricing varies between cloud providers and geographic\nregions. All operations are billed\nper day\n.\nName\nDescription\nPrice\nRead Processing Unit (\nRPU\n)\nRead operations to the database.\nAtlas\ncalculates RPUs based on the number of read\noperations, document bytes\nread (in 4KB increments), and index bytes read (in 256\nbyte increments) per operation. RPUs are calculated on\na daily basis and start at 0 each day.\nThe price for each million\nRPU\ns decreases based on volume\nof reads in a day. Starting prices range from\n$0.09 to $0.22 per million\nRPU\ns.\n[\n1\n]\nSee\nRead Processing Unit Pricing\nfor price tier details.\nWrite Processing Unit (\nWPU\n)\nWrite operations to the database.\nAtlas\ncalculates WPUs based on document bytes and index\nbytes written (up to 1KB). If a document and index exceed 1KB,\nAtlas\ncovers each excess chunk of 1KB with an additional WPU.\nAtlas\ncalculates WPUs on a daily basis and WPUs start at 0\neach day.\nRange from $0.90 to $2.20 per million\nWPU\ns.\n[\n1\n]\nStorage\nLogical document and index storage. This value includes the\nnumber of bytes of all uncompressed BSON documents stored in all\ncollections, plus the bytes stored in their associated indexes.\nRange from $0.20 to $0.70 per\nGB\nper month.\nContinuous Backup\nPoint-in-Time backups triggered by write events.\nRange from $0.20 to $0.60 per\nGB\nper month.\nRestore from Backup\nThe time required to restore your serverless instance.\nIMPORTANT:\nData transfer as part of the backup and restore process is\ncharged separately.\nRange from $2.50 to $6.00 per restore hour.\nData Transfer\nData transfer to and from the database.\n[\n1\n]\nIf your data transfer costs are a significant portion of your\nbill, see\nHow to Reduce Data Transfer Costs\n.\nRegional: $0.01 per\nGB\nfor\nall\ncloud providers and\nregions.\nCross-Region: range from $0.02 to $0.20 per\nGB\n.\nPublic Internet: range from $0.09 to $0.20 per\nGB\n.\n[\n1\n]\n(\n1\n,\n2\n,\n3\n)\nRPU\nand\nWPU\nprices are presented\nper million\n, but you are\nonly charged for the amount you use. For example, if\nWPU\ns\ncost $1.25 per million in your region; using 500,000 would cost\n$0.63. The same is true for data transfer: you're only charged for\nthe amount you transfer.\nRead Processing Unit Pricing\nThe price for each million (M)\nRPU\ns depends on volume of reads\nthat day.\nRPU\ns start at 0 each day. Prices are in cascading\ntiers:\nTier\nDescription\nPrice\n0-50 M\nRPU\ns\nThe first 50 million\nRPU\ns in a day.\n$0.09 to $0.22/M\n50-550 M\nRPU\ns\nThe next 500 million\nRPU\ns in a day.\n$0.05 to $0.11/M\n550 M-20.55 B\nRPU\ns\nThe next 20 billion\nRPU\ns in a day.\n$0.01 to $0.03/M\n20.55+ B\nRPU\ns\nAll subsequent\nRPU\ns in a day.\nFree\nExample\nIf your application uses 560 million\nRPU\ns in a day, in a region\nwith prices of $0.10/M, $0.06/M, and $0.02/M by tier,\nAtlas\ncharges you $35.20:\n$0.10/M for the first 50 million ($5.00).\n$0.06/M for the next 500 million ($30.00).\n$0.02/M for the last 10 million ($0.20).\nIf that usage is typical for a day, your application costs\napproximately $1056 per month ($35.20 per day x 30 days).\nIn another example, if your application uses 0.5 million\nRPU\ns\nin a day in the same region,\nAtlas\ncharges you $0.05:\n$0.10/M for the first 0.5 million ($0.05).\nIf that usage is typical for a day, your application costs\napproximately $1.50 per month ($0.05 per day x 30 days).\nPayment Methods\nWarning\nAn unexpected, significant increase in Serverless instance usage\ncould result in an expensive invoice. Use\nbilling alerts\nto monitor usage.\nYou can pay for Serverless instances with:\nA\npayment method\nadded through the\nAtlas\nconsole,\nan\nAtlas\nsubscription\n, or\nAtlas\ncredits.\nBack\nCluster Configuration\nNext\nAtlas Flex Costs",
    "url": "https://www.mongodb.com/docs/atlas/billing/serverless-instance-costs/#std-label-serverless-instance-costs",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31802.4935229
  },
  {
    "title": "atlas clusters",
    "content": "Docs Home\n/\nMongoDB Atlas\n/\nAtlas CLI\n/\nCommands\natlas clusters\nCopy page\nManage clusters for your project.\nPublic Preview: The atlas api sub-command, automatically generated from the MongoDB Atlas Admin API, offers full coverage of the Admin API and is currently in Public Preview (please provide feedback at\nhttps://feedback.mongodb.com/forums/930808-atlas-cli\n).\nAdmin API capabilities have their own release lifecycle, which you can check via the provided API endpoint documentation link.\nThe clusters command provides access to your cluster configurations. You can create, edit, and delete clusters.\nOptions\nName\nType\nRequired\nDescription\n-h, --help\nfalse\nhelp for clusters\nInherited Options\nName\nType\nRequired\nDescription\n-P, --profile\nstring\nfalse\nName of the profile to use from your configuration file. To learn about profiles for the Atlas CLI, see\nhttps://dochub.mongodb.org/core/atlas-cli-save-connection-settings\n.\nRelated Commands\natlas clusters advancedSettings\n- Manage advanced configuration settings for your cluster.\natlas clusters availableRegions\n- Manage available regions for your project.\natlas clusters connectionStrings\n- Manage MongoDB cluster connection string.\natlas clusters create\n- Create a cluster for your project.\natlas clusters delete\n- Remove the specified cluster from your project.\natlas clusters describe\n- Return the details for the specified cluster for your project.\natlas clusters failover\n- Starts a failover test for the specified cluster in the specified project.\natlas clusters indexes\n- Manage cluster rolling indexes for your project.\natlas clusters list\n- Return all clusters for your project.\natlas clusters onlineArchives\n- Manage online archives for your cluster.\natlas clusters pause\n- Pause the specified running MongoDB cluster.\natlas clusters sampleData\n- Manage sample data for your cluster.\natlas clusters search\n- Manage Atlas Search for your cluster.\natlas clusters start\n- Start the specified paused MongoDB cluster.\natlas clusters update\n- Modify the settings of the specified cluster.\natlas clusters upgrade\n- Upgrade a shared cluster's tier, disk size, and/or MongoDB version.\natlas clusters watch\n- Watch the specified cluster in your project until it becomes available.\nBack\nlist\nNext\nadvancedSettings",
    "url": "https://www.mongodb.com/docs/atlas/cli/current/command/atlas-clusters/#std-label-atlas-clusters",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31802.7527703
  },
  {
    "title": "Thread and Fork Safety",
    "content": "Docs Home\n/\nLanguages\n/\nC++\n/\nC++ Driver\nThread and Fork Safety\nCopy page\nYou should always give each thread its own\nmongocxx::client\n.\nIn general each\nmongocxx::client\nobject AND all of its child objects,\nincluding\nmongocxx::client_session\n,\nmongocxx::database\n,\nmongocxx::collection\n,\nand\nmongocxx::cursor\n,\nshould be used by a single thread at a time\n. This\nis true even for clients acquired from a\nmongocxx::pool\n.\nEven if you create multiple child objects from a single\nclient\n, and\nsynchronize them individually, that is unsafe as they will concurrently\nmodify internal structures of the\nclient\n. The same is true if you copy a\nchild object.\nIncorrect Threading Example\nmongocxx::\ninstance instance{};\nmongocxx::\nuri uri{};\nmongocxx::\nclient c{uri};\nauto\ndb1\n= c[\n\"db1\"\n];\nauto\ndb2\n= c[\n\"db2\"\n];\nstd::\nmutex db1_mtx{};\nstd::\nmutex db2_mtx{};\nauto\nthreadfunc\n= []\n(mongocxx::database& db, std::\nmutex& mtx)\n{\nmtx.\nlock\n()\n;\ndb[\n\"col\"\n].\ninsert_one\n({})\n;\nmtx.\nunlock\n()\n;\n};\n// BAD!\nThese two databases\nare individually synchronized,\nbut they are\nderived from the same\n// client,\nso they can\nonly be accessed by\none thread at\na time\nstd\n::\nthread\nt1\n([&]\n()\n{ threadfunc\n(db1, db1_mtx)\n; threadfunc\n(db2, db2_mtx)\n; })\n;\nstd\n::\nthread\nt2\n([&]\n()\n{ threadfunc\n(db2, db2_mtx)\n; threadfunc\n(db1, db1_mtx)\n; })\n;\nt1.\njoin\n()\n;\nt2.\njoin\n()\n;\nIn the above example, even though the two databases are individually\nsynchronized, they are derived from the same client. There is shared state\ninside the library that is now being modified without synchronization. The\nsame problem occurs if\ndb2\nis a copy of\ndb1\n.\nAcceptable Threading Example\nmongocxx::\ninstance instance{};\nmongocxx::\nuri uri{};\nmongocxx::\nclient c1{uri};\nmongocxx::\nclient c2{uri};\nstd::\nmutex c1_mtx{};\nstd::\nmutex c2_mtx{};\nauto\nthreadfunc\n= []\n(std::string dbname, mongocxx::client& client, std::\nmutex& mtx)\n{\nmtx.\nlock\n()\n;\nclient[dbname][\n\"col\"\n].\ninsert_one\n({})\n;\nmtx.\nunlock\n()\n;\n};\n//\nThese two clients\nare individually synchronized,\nso it is\nsafe\nto share them\nbetween\n// threads.\nstd\n::\nthread\nt1\n([&]\n()\n{ threadfunc\n(\n\"db1\"\n, c1, c1_mtx)\n; threadfunc\n(\n\"db2\"\n, c2, c2_mtx)\n; })\n;\nstd\n::\nthread\nt2\n([&]\n()\n{ threadfunc\n(\n\"db2\"\n, c2, c2_mtx)\n; threadfunc\n(\n\"db1\"\n, c1, c1_mtx)\n; })\n;\nt1.\njoin\n()\n;\nt2.\njoin\n()\n;\nIdeal Threading Example\nmongocxx::\ninstance instance{};\nmongocxx::\npool pool{\nmongocxx::\nuri{}};\nauto\nthreadfunc\n= []\n(mongocxx::client& client, std::string dbname)\n{\nauto\ncol = client[dbname][\n\"col\"\n].\ninsert_one\n({})\n;\n};\n// Great!\nUsing the pool\nallows\nthe clients to\nbe\nsynchronized while sharing\nonly one\n// background monitoring thread.\nstd\n::\nthread\nt1\n([&]\n()\n{\nauto\nc = pool.acquire\n()\n;\nthreadfunc\n(*c,\n\"db1\"\n)\n;\nthreadfunc\n(*c,\n\"db2\"\n)\n;\n})\n;\nstd\n::\nthread\nt2\n([&]\n()\n{\nauto\nc = pool.acquire\n()\n;\nthreadfunc\n(*c,\n\"db2\"\n)\n;\nthreadfunc\n(*c,\n\"db1\"\n)\n;\n})\n;\nt1.\njoin\n()\n;\nt2.\njoin\n()\n;\nIn most programs, clients will be long lived for convenience and performance. In this contrived example,\nthere's quite a bit of overhead because we're doing so little work with each\nclient, but typically this is the best solution.\nFork Safety\nNeither a\nmongocxx::client\nor a\nmongocxx::pool\ncan be safely copied\nwhen forking. Because of this, any client or pool must be created\nafter\nforking, not before.\nBack\nInclude & Link the Driver\nNext\nAPI & ABI Versioning",
    "url": "https://www.mongodb.com/docs/languages/cpp/cpp-driver/current/thread-safety/",
    "source": "mongodb",
    "doc_type": "general",
    "scraped_at": 31802.9855958
  },
  {
    "title": "Read Data from MongoDB With Queries",
    "content": "Docs Home\n/\nGuides\nRead Data from MongoDB With Queries\nCopy page\nOverview\nIn the previous guide,\nRead Data in MongoDB\n, you retrieved all documents\nfrom the\nsample_guides.planets\ncollection without specifying any\ncriteria that the documents should meet.\nIn this guide, you will query the collection and retrieve documents that\nmatch specific\nequality\ncriteria, meaning the values of the specified\nfield or fields must match.\nTime required:\n15\nminutes\nWhat You'll Need\nA\nconnection string\nto your MongoDB deployment.\nSample datasets\nloaded into your cluster\n.\nAn\ninstalled MongoDB Driver\n.\nProcedure\n1\nConnect to your MongoDB instance.\nTip\nIn this code block there is a comment to replace the connection URI\nwith your own. Replace the URI string with your own\nAtlas connection string\n.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nTip\nThe following is an outline with the minimum code necessary to connect to MongoDB.\nYou'll make additions over the next few steps to read data.\nAt line 5, replace the URI string with your own\nAtlas connection string\n.\nCrudRead.cs\n1\n﻿\nusing\nMongoDB.Bson;\n2\nusing\nMongoDB.Driver;\n3\n4\n//\nReplace the uri\nstring with your MongoDB deployment's connection string.\n5\nvar\nuri =\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n6\n7\nvar\nclient =\nnew\nMongoClient(uri);\n8\n9\n//\ndatabase and collection\ncode goes here\n10\n//\nfind code goes\nhere\n11\n//\niterate code goes\nhere\n12\n13\n14\nTip\nThe following is an outline with the minimum code necessary to connect to MongoDB.\nYou'll make additions over the next few steps to read data.\nAt line 11, replace the URI string with your own\nAtlas connection string\n.\ncrudRead.go\n1\npackage\nmain\n2\n3\nimport\n(\n4\n\"context\"\n5\n6\n\"go.mongodb.org/mongo-driver/v2/mongo\"\n7\n\"go.mongodb.org/mongo-driver/v2/mongo/options\"\n8\n)\n9\n10\nfunc\nmain\n()\n{\n11\nuri\n:=\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n12\n13\nclient, err\n:=\nmongo.Connect(options.Client().ApplyURI(uri))\n14\nif\nerr !=\nnil\n{\n15\npanic\n(err)\n16\n}\n17\n18\ndefer\nfunc\n()\n{\n19\nif\nerr = client.Disconnect(context.TODO()); err !=\nnil\n{\n20\npanic\n(err)\n21\n}\n22\n}()\n23\n24\n//\ndatabase and colletion\ncode goes here\n25\n//\nfind code goes\nhere\n26\n//\niterate code goes\nhere\n27\n}\nTip\nThe following is an outline with the minimum code necessary to connect to MongoDB.\nYou'll make additions over the next few steps to read data.\nAt line 8, replace the URI string with your own\nAtlas connection string\n.\nCrudRead.java\n1\nimport\ncom.mongodb.client.*;\n2\nimport\ncom.mongodb.client.model.Filters.*;\n3\nimport\norg.bson.Document;\n4\nimport\norg.bson.conversions.Bson;\n5\n6\npublic\nclass\nCrudRead\n{\n7\npublic\nstatic\nvoid\nmain\n(String[] args)\n{\n8\nString\nuri\n=\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n9\n10\ntry\n(\nMongoClient\nmongoClient\n=\nMongoClients.create(uri)) {\n11\n//\ndatabase and collection\ncode goes here\n12\n//\nfind code goes\nhere\n13\n//\niterate code goes\nhere\n14\n}\n15\n}\n16\n}\nTip\nThe following is an outline with the minimum code necessary to connect to MongoDB.\nYou'll make additions over the next few steps to read data.\nAt line 4, replace the URI string with your own\nAtlas connection string\n.\ncrud-read.js\n1\nconst\n{\nMongoClient\n}\n=\nrequire\n(\n\"mongodb\"\n)\n;\n2\n//\nReplace the uri\nstring with your MongoDB deployment's connection string.\n3\nconst\nuri\n=\n4\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n5\nconst\nclient\n=\nnew\nMongoClient\n(\nuri)\n;\n6\nasync\nfunction\nrun\n(\n)\n{\n7\ntry\n{\n8\nawait\nclient.\nconnect\n(\n)\n;\n9\n//\ndatabase and collection\ncode goes here\n10\n//\nfind code goes\nhere\n11\n//\niterate code goes\nhere\n12\n}\nfinally\n{\n13\n//\nEnsures that the\nclient\nwill close when\nyou finish/error\n14\nawait\nclient.\nclose\n(\n)\n;\n15\n}\n16\n}\n17\nrun\n(\n).\ncatch\n(\nconsole\n.\ndir\n)\n;\nTip\nThe following is an outline with the minimum code necessary to connect to MongoDB.\nYou'll make additions over the next few steps to read data.\nAt line 4, replace the URI string with your own\nAtlas connection string\n.\ncrud_read.py\n1\nfrom\npymongo\nimport\nMongoClient\n2\n3\n#\nReplace the uri\nstring with your MongoDB deployment's connection string.\n4\nuri =\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n5\n6\nclient = MongoClient(uri)\n7\n8\n#\ndatabase and collection\ncode goes here\n9\n#\nfind code goes\nhere\n10\n#\niterate code goes\nhere\n11\n12\n#\nClose the connection\nto MongoDB when you're done.\n13\nclient.close()\nTip\nmongodb+srv\nMake sure you've installed PyMongo with the\nsrv\noption.\npython3 -m pip install \"pymongo[srv]\"\n2\nGet the database and collection.\nSwitch to the database and collection you want to query. In this case\nyou will use the\nsample_guides\ndatabase and\nplanets\ncollection.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nCrudRead.cs\n﻿\n//\ndatabase and collection\ncode goes here\nvar\ndb = client.GetDatabase(\n\"sample_guides\"\n);\nvar\ncoll = db.GetCollection<BsonDocument>(\n\"planets\"\n);\ncrudRead.go\n1\n//\ndatabase and colletion\ncode goes here\n2\ndb\n:=\nclient.Database(\n\"sample_guides\"\n)\n3\ncoll\n:=\ndb.Collection(\n\"planets\"\n)\nCrudRead.java\n1\n//\ndatabase and collection\ncode goes here\n2\nMongoDatabase\ndb\n=\nmongoClient.getDatabase(\n\"sample_guides\"\n);\n3\nMongoCollection<Document> coll = db.getCollection(\n\"planets\"\n);\ncrud-read.js\n//\ndatabase and collection\ncode goes here\nconst\ndb\n=\nclient.\ndb\n(\n\"sample_guides\"\n)\n;\nconst\ncoll\n=\ndb.\ncollection\n(\n\"planets\"\n)\n;\ncrud_read.py\n#\ndatabase and collection\ncode goes here\ndb = client.sample_guides\ncoll = db.planets\n3\nRetrieve specific documents in the\nplanets\ncollection.\nYou can retrieve specific documents from a collection by applying a query filter.\nA query filter is a document that contains the criteria you are searching for.\nThe following example illustrates using a query filter to retrieve documents\nfrom the\nplanets\ncollection that have a\nhasRings\nfield\nwith a value of\ntrue\n.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nCrudRead.cs\n﻿\n//\nfind code goes\nhere\nvar\ncursor =\nfrom\nplanet\nin\ncoll.AsQueryable()\nwhere\nplanet[\n\"hasRings\"\n] ==\ntrue\nselect\nplanet;\nTip\nBSON.D\nshould be used when sending documents to MongoDB,\nbecause\nBSON.D\nis ordered. This is important in more complex\noperations.\ncrudRead.go\n1\n//\nfind code goes\nhere\n2\nfilter\n:=\nbson.D{{\n\"hasRings\"\n,\ntrue\n}}\n3\ncursor, err\n:=\ncoll.Find(context.TODO(), filter)\n4\nif\nerr !=\nnil\n{\n5\npanic\n(err)\n6\n}\nThe MongoDB Java Sync Driver includes\nBuilders\nthat simplify the process of creating queries (and other operations).\nHere, you use the\nFilters.eq\nbuilder to construct the query document.\nCrudRead.java\n1\n//\nfind code goes\nhere\n2\nBson\nfilter\n=\neq(\n\"hasRings\"\n,\ntrue\n);\n3\nMongoCursor<Document> cursor = coll.find(filter).iterator();\ncrud-read.js\n//\nfind code goes\nhere\nconst\ncursor\n=\ncoll.\nfind\n(\n{\nhasRings\n:\ntrue\n})\n;\ncrud_read.py\n#\nfind code goes\nhere\ncursor = coll.find({\n\"hasRings\"\n:\nTrue\n})\n4\nIterate over the results.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nCrudRead.cs\n﻿\n//\niterate code goes\nhere\nforeach\n(\nvar\ndocument\nin\ncursor.ToEnumerable())\n{\nConsole.WriteLine(document);\n}\ncrudRead.go\n1\n//\niterate code goes\nhere\n2\nfor\ncursor.Next(context.TODO()) {\n3\nvar\nresult bson.M\n4\nif\nerr\n:=\ncursor.Decode(&result); err !=\nnil\n{\n5\npanic\n(err)\n6\n}\n7\nfmt.Println(result)\n8\n}\n9\nif\nerr\n:=\ncursor.Err(); err !=\nnil\n{\n10\npanic\n(err)\n11\n}\nCrudRead.java\n1\n//\niterate code goes\nhere\n2\ntry\n{\n3\nwhile\n(cursor.hasNext()) {\n4\nSystem.out.println(cursor.next().toJson());\n5\n}\n6\n}\nfinally\n{\n7\ncursor.close();\n8\n}\nIterate the results and print them to the console. Operations like\nthis are\nasychronous\nin the MongoDB Node.js\ndriver by default, meaning the Node.js runtime doesn't block other\noperations while waiting for them to finish execution.\nIn order to simplify the operation, you specify the\nawait\nkeyword, which\nwill\ncause the runtime to wait for the operation.\nThis is often easier than specifying a callback, or chaining\na promise.\nFor more information, see the\nPromise and Callbacks guide\n.\ncrud-read.js\n//\niterate code goes\nhere\nawait\ncursor.\nforEach\n(\nconsole\n.\nlog\n)\n;\ncrud_read.py\n#\niterate code goes\nhere\nfor\ndoc\nin\ncursor:\nprint\n(doc)\n5\nCheck your results.\nHere is the complete code followed by sample output.\nNote\nYour\nObjectId\nvalues will differ from those shown.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nHere is the complete code followed by sample output.\nCrudRead.cs\n1\n﻿\nusing\nMongoDB.Bson;\n2\nusing\nMongoDB.Driver;\n3\n4\n//\nReplace the uri\nstring with your MongoDB deployment's connection string.\n5\nvar\nuri =\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n6\n7\nvar\nclient =\nnew\nMongoClient(uri);\n8\n9\n//\ndatabase and collection\ncode goes here\n10\nvar\ndb = client.GetDatabase(\n\"sample_guides\"\n);\n11\nvar\ncoll = db.GetCollection<BsonDocument>(\n\"planets\"\n);\n12\n//\nfind code goes\nhere\n13\nvar\ncursor =\nfrom\nplanet\nin\ncoll.AsQueryable()\n14\nwhere\nplanet[\n\"hasRings\"\n] ==\ntrue\n15\nselect\nplanet;\n16\n//\niterate code goes\nhere\n17\nforeach\n(\nvar\ndocument\nin\ncursor)\n18\n{\n19\nConsole.WriteLine(document);\n20\n}\n21\n22\nHIDE OUTPUT\n{\n... 'name'\n:\n'Uranus'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Neptune'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Jupiter'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Saturn'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\nHere is the complete code followed by sample output. The output\ndocuments have been truncated here for display purposes.\ncrudRead.go\n1\npackage\nmain\n2\n3\nimport\n(\n4\n\"context\"\n5\n\"fmt\"\n6\n7\n\"go.mongodb.org/mongo-driver/v2/bson\"\n8\n\"go.mongodb.org/mongo-driver/v2/mongo\"\n9\n\"go.mongodb.org/mongo-driver/v2/mongo/options\"\n10\n)\n11\n12\nfunc\nmain\n()\n{\n13\nuri\n:=\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n14\n15\nclient, err\n:=\nmongo.Connect(options.Client().ApplyURI(uri))\n16\nif\nerr !=\nnil\n{\n17\npanic\n(err)\n18\n}\n19\n20\ndefer\nfunc\n()\n{\n21\nif\nerr = client.Disconnect(context.TODO()); err !=\nnil\n{\n22\npanic\n(err)\n23\n}\n24\n}()\n25\n26\n//\ndatabase and colletion\ncode goes here\n27\ndb\n:=\nclient.Database(\n\"sample_guides\"\n)\n28\ncoll\n:=\ndb.Collection(\n\"planets\"\n)\n29\n30\n//\nfind code goes\nhere\n31\nfilter\n:=\nbson.D{{\n\"hasRings\"\n,\ntrue\n}}\n32\ncursor, err\n:=\ncoll.Find(context.TODO(), filter)\n33\nif\nerr !=\nnil\n{\n34\npanic\n(err)\n35\n}\n36\n37\n//\niterate code goes\nhere\n38\nfor\ncursor.Next(context.TODO()) {\n39\nvar\nresult bson.M\n40\nif\nerr\n:=\ncursor.Decode(&result); err !=\nnil\n{\n41\npanic\n(err)\n42\n}\n43\nfmt.Println(result)\n44\n}\n45\nif\nerr\n:=\ncursor.Err(); err !=\nnil\n{\n46\npanic\n(err)\n47\n}\n48\n49\n}\nHIDE OUTPUT\nmap\n[\n... hasRings\n:\ntrue\nname\n:\nUranus ...\n]\n]\nmap\n[\n... hasRings\n:\ntrue\nname\n:\nNeptune ...\n]\n]\nmap\n[\n... hasRings\n:\ntrue\nname\n:\nJupiter ...\n]\n]\nmap\n[\n... hasRings\n:\ntrue\nname\n:\nSaturn ...\n]\n]\nHere is the complete code followed by sample output.\nCrudRead.java\n1\nimport\ncom.mongodb.client.*;\n2\nimport\ncom.mongodb.client.model.Filters.*;\n3\nimport\norg.bson.Document;\n4\nimport\norg.bson.conversions.Bson;\n5\n6\npublic\nclass\nCrudRead\n{\n7\npublic\nstatic\nvoid\nmain\n(String[] args)\n{\n8\nString\nuri\n=\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n9\n10\ntry\n(\nMongoClient\nmongoClient\n=\nMongoClients.create(uri)) {\n11\n//\ndatabase and collection\ncode goes here\n12\nMongoDatabase\ndb\n=\nmongoClient.getDatabase(\n\"sample_guides\"\n);\n13\nMongoCollection<Document> coll = db.getCollection(\n\"planets\"\n);\n14\n15\n//\nfind code goes\nhere\n16\nBson\nfilter\n=\neq(\n\"hasRings\"\n,\ntrue\n);\n17\nMongoCursor<Document> cursor = coll.find(filter).iterator();\n18\n19\n//\niterate code goes\nhere\n20\ntry\n{\n21\nwhile\n(cursor.hasNext()) {\n22\nSystem.out.println(cursor.next().toJson());\n23\n}\n24\n}\nfinally\n{\n25\ncursor.close();\n26\n}\n27\n}\n28\n}\n29\n}\nHIDE OUTPUT\n{\n... 'name'\n:\n'Uranus'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Neptune'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Jupiter'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Saturn'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\nHere is the complete code followed by sample output.\ncrud-read.js\n1\nconst\n{\nMongoClient\n}\n=\nrequire\n(\n\"mongodb\"\n)\n;\n2\n//\nReplace the uri\nstring with your MongoDB deployment's connection string.\n3\nconst\nuri\n=\n4\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n5\nconst\nclient\n=\nnew\nMongoClient\n(\nuri)\n;\n6\nasync\nfunction\nrun\n(\n)\n{\n7\ntry\n{\n8\nawait\nclient.\nconnect\n(\n)\n;\n9\n//\ndatabase and collection\ncode goes here\n10\nconst\ndb\n=\nclient.\ndb\n(\n\"sample_guides\"\n)\n;\n11\nconst\ncoll\n=\ndb.\ncollection\n(\n\"planets\"\n)\n;\n12\n13\n//\nfind code goes\nhere\n14\nconst\ncursor\n=\ncoll.\nfind\n(\n{\nhasRings\n:\ntrue\n})\n;\n15\n16\n//\niterate code goes\nhere\n17\nawait\ncursor.\nforEach\n(\nconsole\n.\nlog\n)\n;\n18\n}\nfinally\n{\n19\n//\nEnsures that the\nclient\nwill close when\nyou finish/error\n20\nawait\nclient.\nclose\n(\n)\n;\n21\n}\n22\n}\n23\nrun\n(\n).\ncatch\n(\nconsole\n.\ndir\n)\n;\nHIDE OUTPUT\n{\n... 'name'\n:\n'Uranus'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Neptune'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Jupiter'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Saturn'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\nHere is the complete code followed by sample output.\ncrud_read.py\n1\nfrom\npymongo\nimport\nMongoClient\n2\n3\n#\nReplace the uri\nstring with your MongoDB deployment's connection string.\n4\nuri =\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n5\n6\nclient = MongoClient(uri)\n7\n8\n#\ndatabase and collection\ncode goes here\n9\ndb = client.sample_guides\n10\ncoll = db.planets\n11\n#\nfind code goes\nhere\n12\ncursor = coll.find({\n\"hasRings\"\n:\nTrue\n})\n13\n#\niterate code goes\nhere\n14\nfor\ndoc\nin\ncursor:\n15\nprint\n(doc)\n16\n17\n#\nClose the connection\nto MongoDB when you're done.\n18\nclient.close()\nHIDE OUTPUT\n{\n... 'name'\n:\n'Uranus'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Neptune'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Jupiter'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n{\n... 'name'\n:\n'Saturn'\n,\n'hasRings'\n:\nTrue\n,\n...\n}\n6\nQuery using multiple criteria.\nYou can also query the collection using multiple criteria.\nThe following example illustrates using multiple criteria to retrieve\ndocuments from the\nplanets\ncollection that have a\nhasRings\nfield with a value of\nfalse\nand\nArgon(Ar)\nas an\nentry in the\nmainAtmosphere\nfield.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nHere is the complete code followed by sample output.\nCrudRead.cs\n1\n﻿\n//\nfind code goes\nhere\n2\nvar\ncursor =\nfrom\nplanet\nin\ncoll.AsQueryable()\n3\nwhere\nplanet[\n\"hasRings\"\n] ==\nfalse\n4\nwhere\nplanet[\n\"mainAtmosphere\"\n] ==\n\"Ar\"\n5\nselect\nplanet;\nHIDE OUTPUT\n{\n...\n,\n\"name\"\n:\n\"Mars\"\n,\n\"mainAtmosphere\"\n:\n[\n\"CO2\"\n,\n\"Ar\"\n,\n\"N\"\n]\n,\n...\n}\n{\n...\n,\n\"name\"\n:\n\"Earth\"\n,\n\"mainAtmosphere\"\n:\n[\n\"N\"\n,\n\"O2\"\n,\n\"Ar\"\n]\n,\n...\n}\nHere is the complete code followed by sample output. The output\ndocuments have been truncated here for display purposes.\ncrudRead.go\n1\n//\nfind code goes\nhere\n2\nfilter\n:=\nbson.D{\n3\n{\n\"$and\"\n,\n4\nbson.A{\n5\nbson.D{{\n\"hasRings\"\n,\nfalse\n}},\n6\nbson.D{{\n\"mainAtmosphere\"\n,\n\"Ar\"\n}},\n7\n},\n8\n},\n9\n}\n10\ncursor, err\n:=\ncoll.Find(context.TODO(), filter)\n11\nif\nerr !=\nnil\n{\n12\npanic\n(err)\n13\n}\nHIDE OUTPUT\nmap\n[\n... hasRings\n:\nfalse\nmainAtmosphere\n:\n[\nCO2 Ar N\n]\n...\n]\n]\nmap\n[\n... hasRings\n:\nfalse\nmainAtmosphere\n:\n[\nN O2 Ar\n]\n...\n]\n]\nHere is the complete code followed by sample output.\nCrudRead.java\n1\n//\nfind code goes\nhere\n2\nBson\nfilter\n=\nand(eq(\n\"hasRings\"\n,\nfalse\n), eq(\n\"mainAtmosphere\"\n,\n\"Ar\"\n));\n3\nMongoCursor<Document> cursor = coll.find(filter).iterator();\nHIDE OUTPUT\n{\n...\n,\n\"name\"\n:\n\"Mars\"\n,\n\"mainAtmosphere\"\n:\n[\n\"CO2\"\n,\n\"Ar\"\n,\n\"N\"\n]\n,\n...\n}\n{\n...\n,\n\"name\"\n:\n\"Earth\"\n,\n\"mainAtmosphere\"\n:\n[\n\"N\"\n,\n\"O2\"\n,\n\"Ar\"\n]\n,\n...\n}\nHere is the complete code followed by sample output.\ncrud-read.js\n1\n//\nfind code goes\nhere\n2\nconst\ncursor\n=\ncoll.\nfind\n(\n{\nhasRings\n:\nfalse\n,\nmainAtmosphere\n:\n\"Ar\"\n})\n;\nHIDE OUTPUT\n{\n...\n,\n\"name\"\n:\n\"Mars\"\n,\n\"mainAtmosphere\"\n:\n[\n\"CO2\"\n,\n\"Ar\"\n,\n\"N\"\n]\n,\n...\n}\n{\n...\n,\n\"name\"\n:\n\"Earth\"\n,\n\"mainAtmosphere\"\n:\n[\n\"N\"\n,\n\"O2\"\n,\n\"Ar\"\n]\n,\n...\n}\ncrud_read.py\n1\n#\nfind code goes\nhere\n2\ncursor = coll.find({\n\"hasRings\"\n:\nFalse\n,\n\"mainAtmosphere\"\n:\n\"Ar\"\n})\nHIDE OUTPUT\n{\n...\n,\n\"name\"\n:\n\"Mars\"\n,\n\"mainAtmosphere\"\n:\n[\n\"CO2\"\n,\n\"Ar\"\n,\n\"N\"\n]\n,\n...\n}\n{\n...\n,\n\"name\"\n:\n\"Earth\"\n,\n\"mainAtmosphere\"\n:\n[\n\"N\"\n,\n\"O2\"\n,\n\"Ar\"\n]\n,\n...\n}\nEven though the\nmainAtmosphere\nfield is an array, you can use\na strict equality query because MongoDB treats arrays as first-class\ntypes. During execution of the query, MongoDB compared each entry\nin the array to the value you specified, in this case\n\"Ar\"\n,\nto determine if the documents matched your criteria.\nSummary\nIf you have completed this guide, you have retrieved data from MongoDB\nusing specific equality criteria. This is useful when you know exactly\nwhat you're searching for, for example an item number, a username, or\nchemical element.\nIn the next guide, you'll learn how to read data from MongoDB using comparison\noperators to retrieve documents that match a broader set of criteria.\nSee Also\nSee the following resources for more in-depth information about the\nconcepts presented here:\nSpecify Equality Condition\nMatch an Embedded or Nested Array\nC#\nGo\nJava (Sync)\nNode.js\nPython\nThe\nMongoDB C# Driver\ndocumentation\nThe\nMongoDB Go Driver\ndocumentation\nThe\nMongoDB Java (Sync) Driver\ndocumentation\nThe\nMongoDB Node.js Driver\ndocumentation\nThe\nPyMongo\ndocumentation\nWhat's Next\nRead Data using Operators and Compound Queries\n20\nmins\nUse operators and compound queries to retrieve documents in MongoDB.\nStart Guide\nChapter\n2\nCRUD\nAdd a MongoDB Driver\nRead Data in MongoDB\nRead Data from MongoDB With Queries\nRead Data using Operators and Compound Queries\nInsert Data into MongoDB\nUpdate Data in MongoDB\nDelete Data from MongoDB",
    "url": "https://www.mongodb.com/docs/guides/crud/read_queries/",
    "source": "mongodb",
    "doc_type": "general",
    "scraped_at": 31803.2984052
  },
  {
    "title": "Restore MongoDB Deployments",
    "content": "Docs Home\n/\nOps Manager\n/\nBackup & Restore\nRestore MongoDB Deployments\nCopy page\nNote\nNow Available\nNew MongoDB Backup Restore Utility for Point-in-Time Restores.\nFor more information, refer to the individual restore procedures.\nUse these procedures to restore a MongoDB deployment using Backup\nartifacts.\nBack\nDisable\nNext\nOverview",
    "url": "https://www.mongodb.com/docs/ops-manager/current/tutorial/nav/backup-restore-deployments/",
    "source": "mongodb",
    "doc_type": "general",
    "scraped_at": 31803.7406932
  },
  {
    "title": "Backups for Serverless Instances (Deprecated)",
    "content": "Docs Home\n/\nAtlas\n/\nBackup, Restore, and Archive\n/\nBackup\nBackups for Serverless Instances (Deprecated)\nCopy page\nImportant\nAs of February 2025, you can create Flex clusters, and can no longer\ncreate\nM2\nand\nM5\nclusters or Serverless instances in the\nAtlas UI, Atlas CLI, Atlas Administration API,\nAtlas Kubernetes Operator\n, HashiCorp Terraform,\nor\nAtlas\nCloudFormation Resources.\nYou can still use existing Serverless instances.\nAtlas\nno longer supports\nM2\nand\nM5\nclusters.\nAtlas\ndeprecated Serverless instances. As of May 25, 2025,\nAtlas\nhas automatically migrated all existing\nM2\nand\nM5\nclusters to Flex clusters.\nFor Serverless instances, beginning May 5 2025,\nAtlas\nwill\ndetermine whether to migrate instances to Free clusters,\nFlex clusters, or Dedicated clusters according to your usage.\nTo see which tiers\nAtlas\nwill migrate your instances\nto, consult the\nAll Clusters\npage in the Atlas UI.\nAtlas\ntakes snapshots of Serverless instances using the native\nsnapshot capabilities of the Serverless instances's cloud service\nprovider.\nWarning\nIf you delete a serverless instance,\nAtlas\ndeletes all\nits associated backups.\nAtlas\noffers the following backup options for\nServerless instances:\nOption\nDescription\nServerless Continuous Backup\nServerless instances are\ndeprecated\n.\nYou can't create new Serverless instances, but you can still\nconfigure their backup.\nAtlas\ntakes incremental\nsnapshots\nof the data in your\nServerless instance every six hours and lets you restore the\ndata from a selected point in time within the last 72 hours.\nAtlas\nalso takes daily snapshots and retains these daily\nsnapshots for 35 days. To learn more, see\nCosts for Serverless Instances (Deprecated)\n.\nBasic Backup\nAtlas\ntakes incremental\nsnapshots\nof the data in your\nServerless instance every six hours and retains only the two\nmost recent snapshots. You can use this option for free.\nTo learn more, see\nConfigure Backup for a Serverless Instance (Deprecated)\n.\nLimitations\nYou can't disable backup of Serverless instances.\nYou can't download Serverless instance snapshots.\nCustom policies are not supported for Serverless instance\nsnapshots.\nAtlas\nalways takes snapshots every six hours.\nIf you require finer-grained backups, consider migrating to a\ndedicated cluster.\nAtlas\ndoesn't support on-demand snapshots for\nServerless instances.\nYou can't restore snapshots from Flex clusters,\ndedicated clusters, or from\nCloud Manager\nto Serverless instances.\nRequired Access\nYou must have\nProject Read Only\naccess to the project to view\nServerless instance snapshots.\nView Serverless Instance Snapshots\nAtlas\ndisplays existing snapshots on the\nSnapshots\npage.\nTo view your snapshots:\nAtlas UI\nAtlas Administration API\n1\nIn\nAtlas\n, go to the\nClusters\npage for your project.\nWARNING:\nNavigation Improvements In Progress\nWe're currently rolling out a new and improved navigation\nexperience. If the following steps don't match your view in the\nAtlas UI, see\nthe preview documentation\n.\nIf it's not already displayed, select the organization that\ncontains your desired project from the\nOrganizations\nmenu\nin the\nnavigation bar.\nIf it's not already displayed, select your desired project\nfrom the\nProjects\nmenu in the navigation bar.\nIf it's not already displayed, click\nClusters\nin the\nsidebar.\nThe\nClusters\npage displays.\n2\nGo to the\nBackup\npage for your cluster.\nClick your cluster's name.\nClick the\nBackup\ntab.\nIf the cluster has no\nBackup\ntab, then\nAtlas\nbackups are disabled for that cluster and no\nsnapshots are available. You can enable backups when you\nscale the cluster\n.\nThe\nBackup\npage displays.\nAtlas\ndisplays existing snapshots in the\nAll Snapshots\ntable. From this table, you can\nrestore\nyour existing snapshots.\nThe Atlas Administration API provides different endpoints for retrieving\none or all snapshots from a given serverless cluster.\nReturn One Snapshot for One Serverless Cluster\nReturn All Snapshots for One Serverless Cluster\nBack\nFlex Cluster\nNext\nOptions",
    "url": "https://www.mongodb.com/docs/atlas/backup/cloud-backup/serverless-backup/",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31804.1650798
  },
  {
    "title": "Compatibility",
    "content": "Docs Home\n/\nLanguages\n/\nC++\n/\nC++ Driver\nCompatibility\nCopy page\nMongoDB Compatibility\nThe following compatibility table specifies the recommended version or versions\nof the MongoDB C++ Driver for use with a specific version of MongoDB.\nThe first column lists the driver version.\nImportant\nMongoDB ensures compatibility between the MongoDB Server and the drivers\nfor three years after the server version's end of life (EOL) date. To learn\nmore about the MongoDB release and EOL dates, see\nMongoDB Software Lifecycle Schedules\n.\nCompatibility Table Legend\nIcon\nExplanation\n✓\nAll features are supported.\n⊛\nThe driver version will work with the MongoDB version, but not all\nnew MongoDB features are supported.\n✗\nThe driver version will not work with the MongoDB version. Attempting\nto connect to the MongoDB version will result in errors.\nNo mark\nThe driver version is not tested with the MongoDB version.\nC++ Driver Version\nMongoDB 8.0\nMongoDB 7.0\nMongoDB 6.0\n3.11 to 4.1\n✓\n✓\n✓\n3.8 to 3.10\n⊛\n✓\n✓\n3.7\n⊛\n⊛\n⊛\n3.6\n⊛\n⊛\n⊛\n3.4 to 3.5\n⊛\n⊛\n⊛\nLanguage Compatibility\nThe following compatibility table documents the C++ standards supported for use with each\nMongoDB C++ Driver version.\nThe first column lists the driver version.\nC++ Driver Version\nC++23\nC++20\nC++17\nC++14\nC++11\n3.8 to 4.1\n✓\n✓\n✓\n✓\n✓\n3.0 to 3.7\n✓\n✓\n✓\nlibmongoc Compatibility\nThe following compatibility table specifies the required version of libmongoc.\nThe first column lists the driver version.\nC++ Driver Version\nMinimum libmongoc Version\n4.1.x\n2.0.0\n4.0.x\n1.29.0\n3.11.x\n1.28.0\n3.9.x and 3.10.x\n1.25.0\n3.8.x\n1.24.0\n3.7.x\n1.22.1\n3.6.x\n1.17.0\n3.5.x\n1.15.0\n3.4.x\n1.13.0\n3.3.x\n1.10.0\n3.2.x\n1.9.2\n3.1.4\n1.7.0\n3.1.0 to 3.1.3\n1.5.0\n3.0.1 to 3.0.3\n1.3.4\n3.0.0\n1.3.1\nCompiler Compatibility\nThe C++ driver supports the following compilers:\nCompiler\nVersion\nGCC\nv8.1 or later\nClang\nv3.8 or later\nXcode\nv13.1 or later\nVisual Studio\n2015 Update 3 or later\nNote\nThe preceding compiler versions are the minimum versions required to\nbuild the C++ driver from source. These are not requirements\nto use prebuilt headers and libraries.\nFor more information on how to read the compatibility tables, see our guide on\nMongoDB Compatibility Tables.\nBack\nTesting\nNext\nIssues & Help",
    "url": "https://www.mongodb.com/docs/languages/cpp/cpp-driver/current/compatibility/",
    "source": "mongodb",
    "doc_type": "general",
    "scraped_at": 31804.6154457
  },
  {
    "title": "Read Data in MongoDB",
    "content": "Docs Home\n/\nGuides\nRead Data in MongoDB\nCopy page\nOverview\nIn this guide, you will learn how to retrieve data from MongoDB.\nTime required:\n10\nminutes\nWhat You'll Need\nA\nconnection string\nto your MongoDB deployment.\nSample datasets\nloaded into your cluster\n.\nAn\ninstalled MongoDB Driver\n.\nProcedure\n1\nConnect to your MongoDB instance.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nTip\nThe following is an outline with the minimum code necessary to connect to MongoDB.\nYou'll make additions over the next few steps to read data.\nAt line 5, replace the URI string with your own\nAtlas connection string\n.\nCrudRead.cs\n1\n﻿\nusing\nMongoDB.Bson;\n2\nusing\nMongoDB.Driver;\n3\n4\n//\nReplace the uri\nstring with your MongoDB deployment's connection string.\n5\nvar\nuri =\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n6\n7\nvar\nclient =\nnew\nMongoClient(uri);\n8\n9\n//\ndatabase and collection\ncode goes here\n10\n//\nfind code goes\nhere\n11\n//\niterate code goes\nhere\n12\n13\n14\nTip\nThe following is an outline with the minimum code necessary to connect to MongoDB.\nYou'll make additions over the next few steps to read data.\nAt line 11, replace the URI string with your own\nAtlas connection string\n.\ncrudRead.go\n1\npackage\nmain\n2\n3\nimport\n(\n4\n\"context\"\n5\n6\n\"go.mongodb.org/mongo-driver/v2/mongo\"\n7\n\"go.mongodb.org/mongo-driver/v2/mongo/options\"\n8\n)\n9\n10\nfunc\nmain\n()\n{\n11\nuri\n:=\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n12\n13\nclient, err\n:=\nmongo.Connect(options.Client().ApplyURI(uri))\n14\nif\nerr !=\nnil\n{\n15\npanic\n(err)\n16\n}\n17\n18\ndefer\nfunc\n()\n{\n19\nif\nerr = client.Disconnect(context.TODO()); err !=\nnil\n{\n20\npanic\n(err)\n21\n}\n22\n}()\n23\n24\n//\ndatabase and colletion\ncode goes here\n25\n//\nfind code goes\nhere\n26\n//\niterate code goes\nhere\n27\n}\nTip\nThe following is an outline with the minimum code necessary to connect to MongoDB.\nYou'll make additions over the next few steps to read data.\nAt line 8, replace the URI string with your own\nAtlas connection string\n.\nCrudRead.java\n1\nimport\ncom.mongodb.client.*;\n2\nimport\ncom.mongodb.client.model.Filters.*;\n3\nimport\norg.bson.Document;\n4\nimport\norg.bson.conversions.Bson;\n5\n6\npublic\nclass\nCrudRead\n{\n7\npublic\nstatic\nvoid\nmain\n(String[] args)\n{\n8\nString\nuri\n=\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n9\n10\ntry\n(\nMongoClient\nmongoClient\n=\nMongoClients.create(uri)) {\n11\n//\ndatabase and collection\ncode goes here\n12\n//\nfind code goes\nhere\n13\n//\niterate code goes\nhere\n14\n}\n15\n}\n16\n}\nTip\nThe following is an outline with the minimum code necessary to connect to MongoDB.\nYou'll make additions over the next few steps to read data.\nAt line 4, replace the URI string with your own\nAtlas connection string\n.\ncrud-read.js\n1\nconst\n{\nMongoClient\n}\n=\nrequire\n(\n\"mongodb\"\n)\n;\n2\n//\nReplace the uri\nstring with your MongoDB deployment's connection string.\n3\nconst\nuri\n=\n4\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n5\nconst\nclient\n=\nnew\nMongoClient\n(\nuri)\n;\n6\nasync\nfunction\nrun\n(\n)\n{\n7\ntry\n{\n8\nawait\nclient.\nconnect\n(\n)\n;\n9\n//\ndatabase and collection\ncode goes here\n10\n//\nfind code goes\nhere\n11\n//\niterate code goes\nhere\n12\n}\nfinally\n{\n13\n//\nEnsures that the\nclient\nwill close when\nyou finish/error\n14\nawait\nclient.\nclose\n(\n)\n;\n15\n}\n16\n}\n17\nrun\n(\n).\ncatch\n(\nconsole\n.\ndir\n)\n;\nTip\nThe following is an outline with the minimum code necessary to connect to MongoDB.\nYou'll make additions over the next few steps to read data.\nAt line 4, replace the URI string with your own\nAtlas connection string\n.\ncrud_read.py\n1\nfrom\npymongo\nimport\nMongoClient\n2\n3\n#\nReplace the uri\nstring with your MongoDB deployment's connection string.\n4\nuri =\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n5\n6\nclient = MongoClient(uri)\n7\n8\n#\ndatabase and collection\ncode goes here\n9\n#\nfind code goes\nhere\n10\n#\niterate code goes\nhere\n11\n12\n#\nClose the connection\nto MongoDB when you're done.\n13\nclient.close()\nTip\nmongodb+srv\nMake sure you've installed PyMongo with the\nsrv\noption.\npython3 -m pip install \"pymongo[srv]\"\nIn this code block there is a comment to replace the connection URI\nwith your own. Make sure to replace the URI string with your\nAtlas connection string\n.\n2\nGet the database and collection.\nSwitch to the database and collection you want to query. In this case\nyou will use the\nsample_guides\ndatabase and\nplanets\ncollection.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nCrudRead.cs\n﻿\n//\ndatabase and collection\ncode goes here\nvar\ndb = client.GetDatabase(\n\"sample_guides\"\n);\nvar\ncoll = db.GetCollection<BsonDocument>(\n\"planets\"\n);\ncrudRead.go\n1\n//\ndatabase and colletion\ncode goes here\n2\ndb\n:=\nclient.Database(\n\"sample_guides\"\n)\n3\ncoll\n:=\ndb.Collection(\n\"planets\"\n)\nCrudRead.java\n1\n//\ndatabase and collection\ncode goes here\n2\nMongoDatabase\ndb\n=\nmongoClient.getDatabase(\n\"sample_guides\"\n);\n3\nMongoCollection<Document> coll = db.getCollection(\n\"planets\"\n);\ncrud-read.js\n//\ndatabase and collection\ncode goes here\nconst\ndb\n=\nclient.\ndb\n(\n\"sample_guides\"\n)\n;\nconst\ncoll\n=\ndb.\ncollection\n(\n\"planets\"\n)\n;\ncrud_read.py\n#\ndatabase and collection\ncode goes here\ndb = client.sample_guides\ncoll = db.planets\n3\nRetrieve all documents in the\nplanets\ncollection.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nCrudRead.cs\n﻿\n//\nfind code goes\nhere\nvar\ncursor = coll.AsQueryable();\nUse the\nFind()\nmethod to retrieve all documents. In another\nguide, you'll learn how to use the same method to retrieve documents\nthat match specific criteria.\nTip\nThe empty\nbson.D{}\nis required to match all documents.\ncrudRead.go\n1\n//\nfind code goes\nhere\n2\ncursor, err\n:=\ncoll.Find(context.TODO(), bson.D{})\n3\nif\nerr !=\nnil\n{\n4\npanic\n(err)\n5\n}\nUse the\nfind()\nmethod to retrieve all documents. In another\nguide, you'll learn how to use the same method to retrieve documents\nthat match specific criteria.\nCrudRead.java\n1\n//\nfind code goes\nhere\n2\nMongoCursor<Document> cursor = coll.find().iterator();\nUse the\nfind()\nmethod to retrieve all documents. In another\nguide, you'll learn how to use the same method to retrieve documents\nthat match specific criteria.\ncrud-read.js\n//\nfind code goes\nhere\nconst\ncursor\n=\ncoll.\nfind\n(\n)\n;\nUse the\nfind()\nmethod to retrieve all documents. In another\nguide, you'll learn how to use the same method to retrieve documents\nthat match specific criteria.\ncrud_read.py\n#\nfind code goes\nhere\ncursor = coll.find()\n4\nIterate over the results.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nCrudRead.cs\n﻿\n//\niterate code goes\nhere\nforeach\n(\nvar\ndocument\nin\ncursor.ToEnumerable())\n{\nConsole.WriteLine(document);\n}\ncrudRead.go\n1\n//\niterate code goes\nhere\n2\nfor\ncursor.Next(context.TODO()) {\n3\nvar\nresult bson.M\n4\nif\nerr\n:=\ncursor.Decode(&result); err !=\nnil\n{\n5\npanic\n(err)\n6\n}\n7\nfmt.Println(result)\n8\n}\n9\nif\nerr\n:=\ncursor.Err(); err !=\nnil\n{\n10\npanic\n(err)\n11\n}\nCrudRead.java\n1\n//\niterate code goes\nhere\n2\ntry\n{\n3\nwhile\n(cursor.hasNext()) {\n4\nSystem.out.println(cursor.next().toJson());\n5\n}\n6\n}\nfinally\n{\n7\ncursor.close();\n8\n}\nIterate the results and print them to the console. Operations like\nthis are\nasychronous\nin the MongoDB Node.js\ndriver by default, meaning the Node.js runtime doesn't block other\noperations while waiting for them to finish execution.\nIn order to simplify the operation, you specify the\nawait\nkeyword, which\nwill\ncause the runtime to wait for the operation.\nThis is often easier than specifying a callback, or chaining\na promise.\nFor more information, see the\nPromise and Callbacks guide\n.\ncrud-read.js\n//\niterate code goes\nhere\nawait\ncursor.\nforEach\n(\nconsole\n.\nlog\n)\n;\ncrud_read.py\n#\niterate code goes\nhere\nfor\ndoc\nin\ncursor:\nprint\n(doc)\n5\nCheck your results.\nHere is the complete code followed by sample output.\nNote\nYour\nObjectId\nvalues will differ from those shown.\nC#\nGo\nJava (Sync)\nNode.js\nPython\nHere is the complete code followed by sample output.\nCrudRead.cs\n1\n﻿\nusing\nMongoDB.Bson;\n2\nusing\nMongoDB.Driver;\n3\n4\n//\nReplace the uri\nstring with your MongoDB deployment's connection string.\n5\nvar\nuri =\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n6\n7\nvar\nclient =\nnew\nMongoClient(uri);\n8\n9\n//\ndatabase and collection\ncode goes here\n10\nvar\ndb = client.GetDatabase(\n\"sample_guides\"\n);\n11\nvar\ncoll = db.GetCollection<BsonDocument>(\n\"planets\"\n);\n12\n//\nfind code goes\nhere\n13\nvar\ncursor = coll.AsQueryable();\n14\n//\niterate code goes\nhere\n15\nforeach\n(\nvar\ndocument\nin\ncursor)\n16\n{\n17\nConsole.WriteLine(document);\n18\n}\n19\n20\nHIDE OUTPUT\n{\n'_id'\n:\nObjectId('\n621\nff30d2a3e781873fcb65c')\n,\n'name'\n:\n'Mercury'\n,\n'orderFromSun'\n:\n1\n,\n'hasRings'\n:\nFalse\n,\n'mainAtmosphere'\n:\n[\n]\n,\n'surfaceTemperatureC'\n:\n{\n'min'\n:\n-173\n,\n'max'\n:\n427\n,\n'mean'\n:\n67\n}\n}\n,\n...\nHere is the complete code followed by sample output.\ncrudRead.go\n1\npackage\nmain\n2\n3\nimport\n(\n4\n\"context\"\n5\n\"fmt\"\n6\n7\n\"go.mongodb.org/mongo-driver/v2/bson\"\n8\n\"go.mongodb.org/mongo-driver/v2/mongo\"\n9\n\"go.mongodb.org/mongo-driver/v2/mongo/options\"\n10\n)\n11\n12\nfunc\nmain\n()\n{\n13\nuri\n:=\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n14\n15\nclient, err\n:=\nmongo.Connect(options.Client().ApplyURI(uri))\n16\nif\nerr !=\nnil\n{\n17\npanic\n(err)\n18\n}\n19\n20\ndefer\nfunc\n()\n{\n21\nif\nerr = client.Disconnect(context.TODO()); err !=\nnil\n{\n22\npanic\n(err)\n23\n}\n24\n}()\n25\n26\n//\ndatabase and colletion\ncode goes here\n27\ndb\n:=\nclient.Database(\n\"sample_guides\"\n)\n28\ncoll\n:=\ndb.Collection(\n\"planets\"\n)\n29\n30\n//\nfind code goes\nhere\n31\ncursor, err\n:=\ncoll.Find(context.TODO(), bson.D{})\n32\nif\nerr !=\nnil\n{\n33\npanic\n(err)\n34\n}\n35\n36\n//\niterate code goes\nhere\n37\nfor\ncursor.Next(context.TODO()) {\n38\nvar\nresult bson.M\n39\nif\nerr\n:=\ncursor.Decode(&result); err !=\nnil\n{\n40\npanic\n(err)\n41\n}\n42\nfmt.Println(result)\n43\n}\n44\nif\nerr\n:=\ncursor.Err(); err !=\nnil\n{\n45\npanic\n(err)\n46\n}\n47\n48\n}\nHIDE OUTPUT\nmap\n[\n_id\n:\nObjectID(\n\"621ff30d2a3e781873fcb65c\"\n) hasRings\n:\nfalse\nmainAtmosphere\n:\n[\n]\nname\n:\nMercury orderFromSun\n:\n1\nsurfaceTemperatureC\n:\nmap\n[\nmax\n:\n427\nmean\n:\n67\nmin\n:\n-173\n]\n]\n...\nHere is the complete code followed by sample output.\nCrudRead.java\n1\nimport\ncom.mongodb.client.*;\n2\nimport\ncom.mongodb.client.model.Filters.*;\n3\nimport\norg.bson.Document;\n4\nimport\norg.bson.conversions.Bson;\n5\n6\npublic\nclass\nCrudRead\n{\n7\npublic\nstatic\nvoid\nmain\n(String[] args)\n{\n8\nString\nuri\n=\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n9\n10\ntry\n(\nMongoClient\nmongoClient\n=\nMongoClients.create(uri)) {\n11\n//\ndatabase and collection\ncode goes here\n12\nMongoDatabase\ndb\n=\nmongoClient.getDatabase(\n\"sample_guides\"\n);\n13\nMongoCollection<Document> coll = db.getCollection(\n\"planets\"\n);\n14\n15\n//\nfind code goes\nhere\n16\nMongoCursor<Document> cursor = coll.find().iterator();\n17\n18\n//\niterate code goes\nhere\n19\ntry\n{\n20\nwhile\n(cursor.hasNext()) {\n21\nSystem.out.println(cursor.next().toJson());\n22\n}\n23\n}\nfinally\n{\n24\ncursor.close();\n25\n}\n26\n}\n27\n}\n28\n}\nHIDE OUTPUT\n{\n\"_id\"\n:\n{\n\"$oid\"\n:\n\"621ff30d2a3e781873fcb65c\"\n}\n,\n\"name\"\n:\n\"Mercury\"\n,\n\"orderFromSun\"\n:\n1\n,\n\"hasRings\"\n:\nfalse\n,\n\"mainAtmosphere\"\n:\n[\n]\n,\n\"surfaceTemperatureC\"\n:\n{\n\"min\"\n:\n-173\n,\n\"max\"\n:\n427\n,\n\"mean\"\n:\n67\n}\n}\n...\nHere is the complete code followed by sample output.\ncrud-read.js\n1\nconst\n{\nMongoClient\n}\n=\nrequire\n(\n\"mongodb\"\n)\n;\n2\n//\nReplace the uri\nstring with your MongoDB deployment's connection string.\n3\nconst\nuri\n=\n4\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n;\n5\nconst\nclient\n=\nnew\nMongoClient\n(\nuri)\n;\n6\nasync\nfunction\nrun\n(\n)\n{\n7\ntry\n{\n8\nawait\nclient.\nconnect\n(\n)\n;\n9\n//\ndatabase and collection\ncode goes here\n10\nconst\ndb\n=\nclient.\ndb\n(\n\"sample_guides\"\n)\n;\n11\nconst\ncoll\n=\ndb.\ncollection\n(\n\"planets\"\n)\n;\n12\n13\n//\nfind code goes\nhere\n14\nconst\ncursor\n=\ncoll.\nfind\n(\n)\n;\n15\n16\n//\niterate code goes\nhere\n17\nawait\ncursor.\nforEach\n(\nconsole\n.\nlog\n)\n;\n18\n}\nfinally\n{\n19\n//\nEnsures that the\nclient\nwill close when\nyou finish/error\n20\nawait\nclient.\nclose\n(\n)\n;\n21\n}\n22\n}\n23\nrun\n(\n).\ncatch\n(\nconsole\n.\ndir\n)\n;\nHIDE OUTPUT\n{\n'_id'\n:\nObjectId('\n621\nff30d2a3e781873fcb65c')\n,\n'name'\n:\n'Mercury'\n,\n'orderFromSun'\n:\n1\n,\n'hasRings'\n:\nFalse\n,\n'mainAtmosphere'\n:\n[\n]\n,\n'surfaceTemperatureC'\n:\n{\n'min'\n:\n-173\n,\n'max'\n:\n427\n,\n'mean'\n:\n67\n}\n}\n,\n...\nHere is the complete code followed by sample output.\ncrud_read.py\n1\nfrom\npymongo\nimport\nMongoClient\n2\n3\n#\nReplace the uri\nstring with your MongoDB deployment's connection string.\n4\nuri =\n\"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\"\n5\n6\nclient = MongoClient(uri)\n7\n8\n#\ndatabase and collection\ncode goes here\n9\ndb = client.sample_guides\n10\ncoll = db.planets\n11\n#\nfind code goes\nhere\n12\ncursor = coll.find({\n\"hasRings\"\n:\nTrue\n})\n13\n#\niterate code goes\nhere\n14\nfor\ndoc\nin\ncursor:\n15\nprint\n(doc)\n16\n17\n#\nClose the connection\nto MongoDB when you're done.\n18\nclient.close()\nHIDE OUTPUT\n{\n'_id'\n:\nObjectId('\n621\nff30d2a3e781873fcb65c')\n,\n'name'\n:\n'Mercury'\n,\n'orderFromSun'\n:\n1\n,\n'hasRings'\n:\nFalse\n,\n'mainAtmosphere'\n:\n[\n]\n,\n'surfaceTemperatureC'\n:\n{\n'min'\n:\n-173\n,\n'max'\n:\n427\n,\n'mean'\n:\n67\n}\n}\n,\n...\nSummary\nIf you successfully completed the procedure in this guide, you have\nretrieved data from MongoDB.\nIn the next guide, you'll learn how to retrieve data from MongoDB using criteria.\nSee Also\nC#\nGo\nJava (Sync)\nNode.js\nPython\nThe\nMongoDB C# Driver\ndocumentation.\nThe\nMongoDB Go Driver\ndocumentation.\nThe\nMongoDB Java (Sync) Driver\ndocumentation.\nThe\nMongoDB Node.js Driver\ndocumentation.\nThe\nPyMongo\ndocumentation.\nFor other CRUD guides:\nRead Data from MongoDB With Queries\nRead Data using Operators and Compound Queries\nInsert Data into MongoDB\nUpdate Data in MongoDB\nDelete Data from MongoDB\nWhat's Next\nRead Data from MongoDB With Queries\n15\nmins\nUse a query to specify which documents to retrieve documents in MongoDB.\nStart Guide\nChapter\n2\nCRUD\nAdd a MongoDB Driver\nRead Data in MongoDB\nRead Data from MongoDB With Queries\nRead Data using Operators and Compound Queries\nInsert Data into MongoDB\nUpdate Data in MongoDB\nDelete Data from MongoDB",
    "url": "https://www.mongodb.com/docs/guides/crud/read/",
    "source": "mongodb",
    "doc_type": "general",
    "scraped_at": 31804.9827784
  },
  {
    "title": "Glossary",
    "content": "Docs Home\n/\nDatabase Manual\n/\nReference\nGlossary\nCopy page\n$cmd\nA virtual\ncollection\nthat exposes MongoDB's\ndatabase commands\n.\nTo use database commands, see\nIssue Commands\n.\n_id\nA field required in every MongoDB\ndocument\n. The\n_id\nfield must have a unique value. You can\nthink of the\n_id\nfield as the document's\nprimary key\n.\nIf you create a new document without an\n_id\nfield, MongoDB\nautomatically creates the field and assigns a unique\nBSON\nObjectId\nto the field.\nabsolute system CPU utilization\nSystem CPU utilization relative to the full amount of CPU available\nto cloud instances that share CPU.\nWhen a cloud provider throttles CPU utilization for a cloud instance,\nthe instance's absolute system CPU utilization is equal to the\nbaseline CPU utilization\nassigned to this instance.\nWhen a cloud provider adds CPU above the baseline CPU, such as\nthrough a bursting mechanism, the sum of normalized kernel CPU\nutilization and user CPU utilization on an instance can exceed\nthe instance's baseline CPU. In this case, the sum of the normalized\nkernel CPU utilization and user CPU utilization is still less than\nthe full amount of CPU shared by cloud instances. See also\nrelative system CPU utilization\n,\nbaseline CPU utilization\n,\nand\nburstable instances\n.\naccumulator\nAn\nexpression\nin an\naggregation pipeline\nthat\nmaintains state between documents in the aggregation\npipeline\n. For a list of accumulator operations, see\n$group\n.\naction\nAn operation the user can perform on a resource. Actions and\nresources\ncombine to create\nprivileges\n. See\naction\n.\nadmin database\nA privileged database. Users\nmust have access to the\nadmin\ndatabase to run certain\nadministrative commands. For a list of administrative commands,\nsee\nAdministration Commands\n.\nAdvanced Persistent Threat\nIn security, an attacker who gains and maintains long-term access to the\nnetwork, disk and/or memory  and remains undetected for an extended\nperiod.\naggregation\nAn operation that reduces and summarizes large\nsets of data. MongoDB's\naggregate()\nand\nmapReduce()\nmethods are two\nexamples of aggregation operations. For more information, see\nAggregation Operations\n.\naggregation pipeline\nConsists of one or more stages that process documents. Aggregation\noperators calculate aggregate values without having to use\nmap-reduce\n. For a list of operators, see\nAggregation Reference\n.\nalert\nNotification sent by Atlas when your database operations or\nserver usage reach thresholds that affect cluster\nperformance. To learn what conditions you can set to trigger\nalerts, see\nReview Alert Conditions\n.\nTip\nResolve Alerts\nanalytics node\nSpecialized read-only node that can isolate queries which you do\nnot want to affect your operational workload. Analytics nodes are\nuseful for handling analytic data such as reporting queries\nexecuted by BI tools. You can host analytics nodes in dedicated\ngeographic regions to optimize read performance and reduce\nlatency.\nAPI\nCommunication protocol facilitating interaction between the\nclient and\nMongoDB Atlas\n. You can use the\nAtlas Administration API to\nautomate many of the tasks performed in the Atlas UI.\nTip\nAtlas Programmatic Access\nAtlas Administration API Specification\nApproximate Nearest Neighbor (ANN) search\nComputational technique used to quickly find points in a dataset\nthat are close to a given query point. Atlas Vector Search uses ANN search\nto find vector embeddings in the data that are closest to the\nvector embeddings in the query without scanning every\nvector.\nTip\nAtlas Search Overview\narbiter\nA\nreplica set\nmember that exists just to vote in\nelections\n. Arbiters do not replicate data. An\narbiter participates in elections for a\nprimary\nbut cannot\nbecome a primary. For more details, see\nReplica Set Arbiter\n.\nAtlas\nMongoDB Atlas\nis a cloud-hosted database-as-a-service.\nAtlas Search\nFine-grained text indexing enabling advanced text search on your\ndata without any additional required management. Atlas Search provides\noptions for several kinds of\ntext analyzers\n, score-based results ranking, and a rich\nquery language\n.\nTip\nAtlas Search Overview\nAtlas user\nAccount used to access the\nAtlas\napplication. You can grant\nAtlas\nusers access to\nAtlas\norganizations, projects, or both, with certain\npermissions defined by\nuser roles\n. A\nAtlas\nuser is\ndifferent than a\ndatabase user\n.\nAtlas\nusers\ndo not provide access to any MongoDB databases.\nTip\nConfigure Access to the Atlas UI\nAtlas user role\nSet of permissions granted to an\nAtlas user\n. You can grant\npermissions at the\norganization\nor\nproject\nlevel.\nTip\nAtlas User Roles\nAtlas Vector Search\nFeature in\nAtlas\nthat allows you to perform semantic search\non vector embeddings by comparing query vectors with indexed\nvectors to find the closest match.\natomic operation\nAn atomic operation is a write operation that either completes\nentirely or doesn't complete at all. For\ndistributed transactions\n, which\ninvolve writes to multiple documents, all writes to each document must\nsucceed for the transaction to succeed. Atomic operations cannot\npartially complete. See\nAtomicity and Transactions\n.\nauthentication\nVerification of the user identity. See\nAuthentication on Self-Managed Deployments\n.\nauthorization\nProvisioning of access to databases and operations. See\nRole-Based Access Control in Self-Managed Deployments\n.\nauto-scaling\nConfigurable option to have your cluster automatically increase\nor decrease its cluster tier, storage capacity, or both in\nresponse to cluster usage.\nTip\nConfigure Auto-Scaling\nautomatic encryption\nWhen using\nIn-Use Encryption\n, automatically performing\nencryption and decryption based on your preconfigured\nencryption schema. The Automatic Encryption Shared Library translates MongoDB\nQuery Language into the correct call, meaning you don't need to\nrewrite your application for specific encrypt and decrypt calls.\nB-tree\nA data structure commonly used by database management systems to\nstore indexes. MongoDB uses B-tree indexes.\nbackup\nCopy of your data that encapsulates the state of your cluster at\na given time. Backups provide a safety measure in the case of\ndata loss events.\nAtlas\nprovides fully-managed\nCloud Backups\n.\nbackup cursor\nA\ntailable cursor\nthat points to a list of backup files.\nBackup cursors are for internal use only.\nbalancer\nAn internal MongoDB process that runs in the context of a\nsharded cluster\nand manages the migration of\nchunks\n. Administrators must disable the balancer for all\nmaintenance operations on a sharded cluster. See\nSharded Cluster Balancer\n.\nbaseline CPU utilization\nFraction of the full amount of CPU available to cloud instances that\nshare CPU. A cloud provider assigns a certain amount of baseline CPU\nto each cloud instance, based on the instance's cluster tier.\nTypically, baseline CPU utilization falls between 20% and 50% of\nabsolute system CPU utilization\n. See also\nrelative system CPU utilization\nand\nburstable instances\n.\nbig-endian\nA byte order in which the most significant byte (big end) of a\nmultibyte data value is stored at the lowest memory address.\nclick to enlarge\nblocking sort\nA sort that must be performed in memory before the output is returned.\nIn-memory sorts may impact performance for large data sets. Use an\nindexed sort\nto avoid an in-memory sort.\nbounded collection scan\nA plan used by the\nquery optimizer\nthat\nexcludes documents with specific field value ranges. For\nexample, if a range of date field values is outside of a specified\ndate range, the documents in that range are excluded from the\nquery plan. See\nCollection Scan\n.\nBSON\nA serialization format used to store\ndocuments\nand make\nremote procedure calls in MongoDB. \"BSON\" is a combination of the words\n\"binary\" and \"JSON\". Think of BSON as a binary representation\nof JSON (JavaScript Object Notation) documents. See\nBSON Types\nand\nMongoDB Extended JSON (v2)\n.\nBSON types\nThe set of types supported by the\nBSON\nserialization\nformat. For a list of BSON types, see\nBSON Types\n.\nburstable instances\nCloud instance types that share a common physical CPU that, for some\ncloud providers, use a \"CPU credit\" model. When you use burstable instances,\nportions of shared CPU may either become available to each of the virtual\ninstances or may become unavailable, under different demands on the\ninstance resources. To learn more, see\nAWS burstable instances\n,\nAzure disk bursting\n, and\nGCP CPU Bursting\n. See also\nbaseline CPU utilization\n,\nabsolute system CPU utilization\n,\nand\nrelative system CPU utilization\n.\nCAP theorem\nGiven three properties of computing systems, consistency,\navailability, and partition tolerance, a distributed computing\nsystem can provide any two of these features, but never all\nthree.\ncapped collection\nA fixed-sized\ncollection\nthat automatically\noverwrites its oldest entries when the collection reaches its maximum size.\nThe MongoDB\noplog\nthat is used in\nreplication\nis a\ncapped collection. See\nCapped Collections\n.\ncardinality\nThe measure of the number of elements within a set of values.\nFor example, the set\nA = { 2, 4, 6 }\ncontains 3 elements,\nand has a cardinality of 3. See\nShard Key Cardinality\n.\ncartesian product\nThe result of combining two data sets where the combined set\ncontains every possible combination of values.\ncfq\nComplete Fairness Queueing (cfq) is an I/O operation scheduler\nthat allocates bandwidth for incoming request processes.\nchecksum\nA calculated value used to ensure data integrity.\nThe\nmd5\nalgorithm is sometimes used as a checksum.\nchunk\nA contiguous range of\nshard key\nvalues within a\nshard\n. Chunk ranges are inclusive of the lower boundary\nand exclusive of the upper boundary. MongoDB splits chunks when\nthey grow bigger than the configured chunk size. The default chunk\nsize is 128 megabytes. MongoDB migrates chunks when a shard\ncontains too many chunks of a collection relative to other shards.\nFor more details, see\nData Partitioning with Chunks\n,\nSharded Cluster Balancer\n, and\nManage Sharded Cluster Balancer\n.\nclient\nThe application layer that uses a database for data persistence\nand storage.\nDrivers\nprovide the interface\nlevel between the application layer and the database server.\nA client can also be a single thread or process.\nclient affinity\nA consistent client connection to a specified data source.\ncloud backups\nLocalized cluster\nbackup\nstorage using the native\nsnapshot\nfunctionality of the cluster's cloud service\nprovider.\nAtlas\nsupports Cloud Backups for clusters served on:\nAmazon Web Services (AWS)\nGoogle Cloud Platform (GCP)\nMicrosoft Azure\nTip\nBack Up Your Cluster\ncluster\nSet of nodes comprising a MongoDB\ndeployment\n. Clusters can\nbe\nreplica sets\nor\nsharded\ndeployments\n.\nTip\nsharded cluster\ncluster class\nConfigurable for M40+ clusters hosted on AWS.\nStorage class of your cluster. Your selected class affects cluster\nstorage performance and cluster costs. You can choose one of the\nfollowing classes:\nLow CPU\nGeneral\nLocal NVMe SSD\nTip\nCustomize Cluster Storage\nNVMe storage\ncluster tier\nDictates the memory, storage, vCPUs, and\nIOPS\nspecification for\neach data-bearing server in the cluster. Cluster storage size and\noverall performance increase as the cluster tier increases.\nTip\nSelect Cluster Tier\nAtlas Cluster Sizing and Tier Selection\ncluster-to-cluster sync\nSynchronizes data between\nsharded clusters\n. Also known as C2C sync.\nclustered collection\nA\ncollection\nthat stores documents ordered by a\nclustered index\nkey.\nSee\nClustered Collections\n.\nCMK\nAbbreviation of Customer Master Key, see\nCustomer Master Key\n.\ncollection\nA grouping of MongoDB\ndocuments\n. A collection\nis the equivalent of an\nRDBMS\ntable. A collection is\nin a single\ndatabase\n. Collections do not enforce a\nschema. Documents in a collection can have different fields.\nTypically, documents in a collection have a similar or related\npurpose. See\nNamespaces\n.\ncollection scan\nCollection scans are a query execution strategy where MongoDB must\ninspect every document in a collection to see if it matches the\nquery criteria. These queries are very inefficient and don't use\nindexes. See\nQuery Optimization\nfor details about\nquery execution strategies.\ncommit\nSaves data changes made after the start of the\nstartSession\ncommand. Operations within a\ntransaction\nare not permanent until they are committed\nwith the\ncommitTransaction\ncommand.\ncommit quorum\nDuring an\nindex build\nthe\ncommit quorum\nspecifies how many secondaries must be ready to commit their local\nindex build before the primary node performs the commit.\ncompound index\nAn\nindex\nconsisting of two or more keys. See\nCompound Indexes\n.\nconcurrency control\nConcurrency control ensures that database operations can be\nexecuted concurrently without compromising correctness.\nPessimistic concurrency control, such as that used in systems\nwith\nlocks\n, blocks any potentially\nconflicting operations even if they may not conflict.\nOptimistic concurrency control, the approach\nused by\nWiredTiger\n, delays\nchecking until after a conflict may have occurred, ending and\nretrying one of the operations in any\nwrite conflict\n.\nconfig database\nAn internal database with metadata for a\nsharded cluster\n.\nTypically, you don't modify the\nconfig\ndatabase. For more\ninformation about the\nconfig\ndatabase, see\nConfig Database\n.\nconfig server\nA\nmongod\ninstance that stores all the metadata\nassociated with a\nsharded cluster\n.\nSee\nConfig Servers\n.\nconfig shard\nA\nmongod\ninstance that stores all the metadata\nassociated with a\nsharded cluster\nand can also store\napplication data. See\nConfig Shards\n.\nconnection pool\nA cache of database connections maintained by the driver. The cached\nconnections are re-used when connections to the database are\nrequired, instead of opening new connections.\nconnection storm\nA scenario where a driver attempts to open more connections to a\ndeployment than that deployment can handle. When requests for new\nconnections fail, the driver requests to establish even more\nconnections in response to the deployment slowing down or failing\nto open new connections. These continuous requests can overload\nthe deployment and lead to outages.\ncontainer\nA collected set of software and its dependent libraries that are\npackaged together to make transferring between computing\nenvironments easier. Containers run as compartmentalized processes\non your operating system, and can be given their own resource\nconstraints. Common container technologies are Docker and\nKubernetes.\ncontention factor\nMultiple operations attempting to modify the same resource, such as a\ndocument field, cause conflicts that delay operations. The\ncontention factor is a setting used with Queryable Encryption to internally\npartition encrypted field/value pairs and optimize operations.\nSee\ncontention\n.\ncosine similarity\nMetric that uses the angle between two vectors to determine the\nsimilarity between those vectors. Cosine similarity is sensitive\nto vector orientation. You can use cosine similarity function when\nindexing your vector embeddings for Atlas Vector Search. If the vectors are\nnormalized to unit length, use\ndotProduct similarity\nfunction instead.\nCPU steal\nThe percentage by which the CPU usage exceeds the guaranteed baseline CPU\ncredit accumulation rate. CPU steal is relevant for cloud providers that\nrely on the credit model in their bursting strategy. CPU credits are units\nof CPU utilization that you accumulate. The credits accumulate at a\nconstant rate to provide a guaranteed level of performance. You can use\nthese credits for additional CPU performance. When the credit balance is\nexhausted, MongoDB only provides the guaranteed baseline of CPU\nperformance and displays the amount of excess as steal percent See also\nrelative system CPU utilization\n,\nbaseline CPU utilization\n,\nand\nburstable instances\n.\nCRUD\nAn acronym for the fundamental operations of a database: Create,\nRead, Update, and Delete. See\nMongoDB CRUD Operations\n.\nCSV\nA text data format with comma-separated values.\nCSV files can be used to exchange data between relational\ndatabases because CSV files have tabular data. You can\nimport CSV files using\nmongoimport\n.\ncursor\nA pointer to the result set of a\nquery\n. Clients can\niterate through a cursor to retrieve results. By default, cursors\nnot opened within a session automatically timeout after 10\nminutes of inactivity. Cursors opened in a session close with\nthe end or timeout of the session.\nSee\nCursors\n.\ncustom role\nCustom set of MongoDB\nprivilege actions\nand MongoDB\nroles\nthat you can save and assign to a\ndatabase user\n. Create custom roles when\nAtlas\n's\nbuilt-in roles\ndon't describe your desired set of\nprivileges.\nTip\nConfigure Custom Database Roles\nCustomer Master Key\nA key that encrypts your\nData Encryption Key\n.\nThe customer master key must be hosted in a remote key\nprovider.\ndaemon\nA background, non-interactive process.\ndata directory\nThe file system location where\nmongod\nstores data\nfiles.\ndbPath\nspecifies the data directory.\nData Encryption Key\nA key you use to encrypt the fields in your MongoDB\ndocuments. The\nencrypted\nData Encryption Key is stored in your\nKey Vault collection. The Data Encryption Key is\nencrypted by the\nCustomer Master Key\n.\nData Explorer\nTool within\nAtlas\nto view and interact with cluster data.\nYou can also use the Data Explorer to manage indexes and run\naggregation pipelines to process your data.\nTip\nInteract with Your Data\nData Federation\nMongoDB's solution for querying data stored in low-cost S3\nbuckets,\nAtlas\nclusters, and\nHTTP\nendpoints using the\nMongoDB Query Language. Analytics applications can use Atlas Data Federation to\nmake use of archived data for their data processing needs.\nTip\nAtlas Data Federation\ndata files\nStore document data and indexes. The\ndbPath\noption specifies the file system location for the data files.\ndata ingestion pipeline\nWorkflow for organizing and transforming data by using\nRAG\nand\nstoring it in a vector database such as\nAtlas\n.\ndata partition\nA distributed system architecture that splits data into ranges.\nSharding\nuses partitioning. See\nData Partitioning with Chunks\n.\ndata-center awareness\nA property that allows clients to address members in a system\nbased on their locations.\nReplica sets\nimplement data-center awareness using\ntagging\n. See\nData Center Awareness\n.\ndatabase\nA container for\ncollections\n.\nEach database has a set of files in the file\nsystem. One MongoDB server typically has multiple\ndatabases.\ndatabase command\nA MongoDB operation, other than an insert, update, remove, or\nquery. For a list of database commands, see\nDatabase Commands\n. To use database commands, see\nIssue Commands\n.\ndatabase exfiltration\nDatabase exfiltration refers to an authorized party taking data from\na secured system and either sharing it with an unauthorized party or\nstoring it on an unsecured system. This may be malicious or accidental.\ndatabase profiler\nA tool that, when enabled, keeps a record on all long-running\noperations in a database's\nsystem.profile\ncollection. The\nprofiler is most often used to diagnose slow queries. See\nDatabase Profiler\n.\ndatabase user\nCredentials used to authenticate a client to access a MongoDB\ncluster. You can assign\nprivileges\nto a database user to determine that\nuser's access level to a cluster. Database users\nare different from\nAtlas users\n. Database\nusers have access to MongoDB deployments, not the\nAtlas\napplication.\nTip\nConfigure Database Users\ndbpath\nThe location of MongoDB's data file storage. See\ndbPath\n.\nDDL (Data Definition Language)\nDDL includes commands that create and modify collections and\nindexes.\ndead letter queue\nA dead letter queue is a collection within an\nAtlas\ndatabase that stores documents that throw errors during\ningestion.\ndedicated cluster\nCluster category containing clusters of tier\nM10\nand greater.\nTier\nRecommended environments\nM10\nand\nM20\nDevelopment\nLow-traffic production\nM30\nand greater\nProduction\ndedicated config server\nA\nmongod\ninstance that only stores all the metadata associated\nwith a\nsharded cluster\n.\nDEK\nData Encryption Key. For more details, see\nData Encryption Key\n.\ndelayed member\nA\nreplica set\nmember that cannot become primary and\napplies operations at a specified delay. The delay is useful for\nprotecting data from human error (unintentionally deleted\ndatabases) or updates that have unforeseen effects on the\nproduction database. See\nDelayed Replica Set Members\n.\ndense vectors\nNumeric representation of data where most or all of the dimensions\ncontain non-zero values. Atlas Vector Search uses dense vectors, which are\npacked with more data, to capture more complex relationships.\ndeployment\nA group of MongoDB servers containing your data.\nAtlas\n-managed clusters are clusters\n(\nreplica sets\nor\nsharded clusters\n).\ndimensions\nNumber of components or elements that make up the features or\nattributes of data in multi-dimensional space. Atlas Vector Search supports up\nto\n4096\ndimensions at index-time and query-time.\ndocument\nA record in a MongoDB\ncollection\nand the basic unit of\ndata in MongoDB. Documents are analogous to\nJSON\nobjects\nbut exist in the database in a more type-rich format known as\nBSON\n. See\nDocuments\n.\ndot notation\nMongoDB uses the dot notation to access the elements of an array\nand to access the fields of an embedded document. See\nDot Notation\n.\ndotProduct similarity\nMeasures similarity between two vectors in multi-dimensional\nspace and returns a scalar value, which is positive when the\nvectors point in roughly the same direction, negative when the\nvectors point in opposite direction, and zero when the vectors\nhave no similarity. Atlas Vector Search supports using\ndotproduct\nsimilarity function when searching for nearest neighbors. We\nrecommend this similarity function instead of cosine similarity if\nthe vectors are normalized to unit length.\ndraining\nThe process of removing or \"shedding\"\nchunks\nfrom\none\nshard\nto another. Administrators must drain shards\nbefore removing them from the cluster. See\nRemove Shards from a Sharded Cluster\n.\ndriver\nA client library for interacting with MongoDB in a particular\ncomputer language. See\ndriver\n.\ndurable\nA write operation is durable when it persists after a shutdown (or\ncrash) and restart of one or more server processes. For a single\nmongod\nserver, a write operation is considered\ndurable when it has been written to the server's\njournal\nfile. For a\nreplica set\n, a write operation\nis considered durable after the write operation achieves\ndurability on a majority of voting nodes and written to a majority\nof voting nodes' journals.\nelectable node\nNode which is eligible to become the\nprimary\nmember of\nyour\nreplica set\n.\nAtlas\nprioritizes nodes in the\nhighest priority region\nfor primary eligibility during\nelections. To ensure reliable elections, the total number of\nelectable nodes across an entire region must be 3, 5, or 7.\nelection\nThe process where members of a\nreplica set\nselect a\nprimary\non startup and in the event of a failure. See\nReplica Set Elections\n.\nembedding\nRepresentation of data such as text, images, audio, video,\nand so on as an array of numbers, which can be interpreted as\ncoordinates in multi-dimensional space.\nAtlas\nsupports storing\nembeddings in an\nAtlas\ncluster and Atlas Vector Search supports indexing\nand querying vector embeddings of up to\n4096\ndimensions.\nencryption key\nRandom string of bits generated specifically to encrypt and\ndecrypt data.\nAtlas\nProject Owners\ncan\nconfigure an additional layer of encryption on their data in\naddition to the default\nencryption at rest\nthat\nAtlas\nprovides. Project\nowners can use their\nAtlas\n-compatible customer key management\nprovider with the MongoDB\nencrypted storage engine\n.\nAtlas\nsupports the following customer key management providers\nwhen configuring Encryption at Rest:\nAmazon Web Services Key Management Service\nAzure Key Vault\nGoogle Cloud Platform Key Management Service\nTip\nEncryption at Rest using Customer Key Management\nencryption schema\nIn\nQueryable Encryption\n, the\nJSON schema\nthat defines which fields are queryable and which query types are\npermitted on those fields.\nendianness\nIn computing, endianness refers to the order in which bytes are\narranged. This ordering can refer to transmission over a\ncommunication medium or more commonly how the bytes are ordered\nin computer memory, based on their significance and position.\nFor details, see\nbig-endian\nand\nlittle-endian\n.\nenvelope encryption\nAn encryption procedure where data is encrypted using a\nData Encryption Key\nand the data encryption key is\nencrypted by another key called the\nCustomer Master Key\n.\nThe encrypted keys are stored as\nBSON\ndocuments in a\nMongoDB collection called the KeyVault.\neuclidean similarity\nFormula to calculate the similarity by using the distance between\ntwo vectors in multi-dimensional space. Euclidean distance is\nsensitive to the magnitude of the vectors. Atlas Vector Search supports using\neuclidean\nsimilarity function for indexing vectors and when\nsearching for nearest neighbors.\neventual consistency\nA property of a distributed system that allows changes to the\nsystem to propagate gradually. In a database system, this means\nthat readable members aren't required to have the latest\nupdates.\nexplicit encryption\nWhen using\nIn-Use Encryption\n, explicitly specifying the\nencryption or decryption operation, keyID, and\nquery type (for Queryable Encryption) or algorithm (for Client-Side Field Level Encryption) when working\nwith encrypted data. Compare to\nautomatic encryption\n.\nexpression\nA component of a query that resolves to a value. Expressions are\nstateless, meaning they return a value without mutating any of the\nvalues used to build the expression.\nIn the MongoDB Query Language, you can build expressions from the\nfollowing components:\nComponent\nExample\nConstants\n3\nOperators\n$add\nField path expressions\n\"$<path.to.field>\"\nFor example,\n{ $add: [ 3, \"$inventory.total\" ] }\nis an expression\nconsisting of the\n$add\noperator and two input expressions:\nThe constant\n3\nThe\nfield path expression\n\"$inventory.total\"\nThe expression returns the result of adding 3 to the value at path\ninventory.total\nof the input document.\nfailover\nThe process that allows a\nsecondary\nmember of a\nreplica set\nto become\nprimary\nin the event of a\nfailure. See\nAutomatic Failover\n.\nfield\nA name-value pair in a\ndocument\n. A document has\nzero or more fields. Fields are analogous to columns in relational\ndatabases. See\nDocument Structure\n.\nfield path\nPath to a field in a document. To specify a field path, use a\nstring that prefixes the field name with a dollar sign (\n$\n).\nfirewall\nA system level network filter that restricts access based on\nIP addresses and other parameters. Firewalls are part of a\nsecure network. See\nFirewalls\n.\nfree tier\nFree-to-use cluster tier that provides a small-scale development\nenvironment to host your data. Free clusters never expire,\nand provide access to a\nsubset\nof Atlas\nfeatures and functionality. Free clusters might also be\nreferred to by their instance size,\nM0\n.\nTip\nGet Started with Atlas\nAtlas M0 (Free Cluster) Limits\nfsync\nA system call that flushes all dirty, in-memory pages to storage.\nAs applications write data, MongoDB records the data in the\nstorage layer.\nTo provide\ndurable\ndata,\nWiredTiger\nuses\ncheckpoints\n. For more\ndetails, see\nJournaling and the WiredTiger Storage Engine\n.\ngeohash\nA geohash value is a binary representation of the location on a\ncoordinate grid. See\nGeohash Values\n.\nGeoJSON\nA\ngeospatial\ndata interchange format based on JavaScript\nObject Notation (\nJSON\n). GeoJSON is used in\ngeospatial queries\n. For\nsupported GeoJSON objects, see\nGeospatial Data\n.\nFor the GeoJSON format specification, see\nhttps://tools.ietf.org/html/rfc7946#section-3.1\n.\ngeospatial\nRelating to geographical location. See\nGeospatial Queries\n.\nglobal cluster\nClusters with defined geographic zones to support location-aware\nread and write operations for globally distributed application\ninstances and clients. You can enable global sharding on clusters\nof tier\nM30\nand greater.\nTip\nManage Global Clusters\nCreate a Global Cluster\nglobal write zone\nGeographic zone representing a subset of your global cluster\ndistribution. Each\nglobal cluster\nsupports up to 9\ndistinct global write zones. Each zone consists of one\nhighest priority region\nand one or more\nelectable\n,\nread-only\n, or\nanalytics\nregions.\nThe available geographic regions depend on the selected cloud\nservice provider.\nGridFS\nA convention for storing large files in a MongoDB database. All of\nthe official MongoDB drivers support the GridFS convention, as does the\nmongofiles\nprogram. See\nGridFS for Self-Managed Deployments\n.\ngroup\nSee\nproject\n.\ngroup ID\nSee\nproject ID\n.\nhashed shard key\nA type of\nshard key\nthat uses a hash of the value\nin the shard key field to distribute documents among members of\nthe\nsharded cluster\n. See\nHashed Indexes\n.\nhealth manager\nA health manager runs health checks on a\nhealth manager facet\nat a specified\nintensity level\n. The health manager checks are run at\nspecified time intervals. A health manager can be configured to\nmove a failing\nmongos\nout of a cluster\nautomatically.\nhealth manager facet\nA set of features that a\nhealth manager\ncan be configured to run health checks for. For\nexample, you can configure a health manager to monitor and\nmanage DNS or LDAP cluster health issues automatically. See\nHealth Manager Facets\nfor details.\nhidden member\nA\nreplica set\nmember that cannot become\nprimary\nand are invisible to client applications. See\nHidden Replica Set Members\n.\nhierarchical bavigable small worlds graphs\nAlgorithm for performing efficient nearest neighbor search in\nmulti-dimensional space. Atlas Vector Search performs\nANN\nsearch with\nHierarchical Navigable Small Worlds\n.\nhigh availability\nHigh availability indicates a system designed for durability,\nredundancy, and automatic failover. Applications\nsupported by the system can operate without\ndowntime for a long time period. MongoDB\nreplica sets\nsupport\nhigh availability when deployed according to the\nbest practices\n.\nFor guidance on replica set deployment architecture, see\nReplica Set Deployment Architectures\n.\nhighest priority region\nRegion in a\nmulti-region cluster\nwhich\nAtlas\nprioritizes for\nprimary\neligibility during elections.\nTip\nConfigure High Availability and Workload Isolation\nhybrid search\nMethod of combining different search methods, such as a full-text\nand a semantic search, to take advantage of their respective\nstrengths. The results are combined by using a technique such as\nReciprocal Rank Fusion (RRF).\nidempotent\nAn operation produces the same result with the\nsame input when run multiple times.\nimpact\nEstimated performance improvement of an index that\nPerformance Advisor\nsuggests.\nTip\nReview Index Ranking\nin-memory sort\nA sort that must be performed in memory before the output is\nreturned. In-memory sorts may impact performance for large data sets. Use an\nindexed sort\nto avoid an in-memory sort.\nSee\nSort and Index Use\nfor more information on indexed sort\noperations.\nIn-Use Encryption\nEncryption that secures data when transmitted, stored, and\nprocessed, and enables supported queries on that encrypted data.\nMongoDB provides two approaches to In-Use Encryption:\nQueryable Encryption\nand\nClient-Side Field Level Encryption\n.\nindex\nA data structure that optimizes queries. See\nIndexes\n.\nindex bounds\nThe range of index values that MongoDB searches when using an\nindex to run a query. To learn more, see\nMultikey Index Bounds\n.\nindexed sort\nA sort where an index provides the sorted result. Sort operations that\nuse an index often have better performance than an\nin-memory sort\n.\nSee\nUse Indexed to Sort Query Results\nfor\nmore information.\ninit script\nA shell script used by a Linux platform's\ninit system\nto start, restart, or stop a\ndaemon\nprocess. If you installed MongoDB using a package manager, an init\nscript is provided for your system as part of the\ninstallation. See the respective\nInstallation Guide\nfor your operating\nsystem.\ninit system\nThe init system is the first process started on a Linux platform\nafter the kernel starts, and manages all other processes on the\nsystem. The init system uses an\ninit script\nto start,\nrestart, or stop a\ndaemon\nprocess, such as\nmongod\nor\nmongos\n. Recent Linux\nversions typically use the\nsystemd\ninit system and the\nsystemctl\ncommand. Older Linux versions typically use the\nSystem V\ninit system and the\nservice\ncommand. See\nthe Installation Guide for your operating system.\ninitial sync\nThe\nreplica set\noperation that replicates data from an\nexisting replica set member to a new replica set member. See\nInitial Sync\n.\nintent lock\nA\nlock\non a resource that indicates the lock holder\nwill read from (intent shared) or write to (intent\nexclusive) the resource using\nconcurrency control\nat\na finer granularity than that of the resource with the intent\nlock. Intent locks allow concurrent readers and writers of a\nresource. See\nWhat type of locking does MongoDB use?\n.\ninterface endpoint\nAWS\nVPC\nendpoint with a private IP address\nthat sends traffic to the\nAtlas\nprivate endpoint service over\nAWS\nPrivateLink.\nTip\nLearn About Private Endpoints in Atlas\ninterrupt point\nA point in an operation when it can\nsafely end. MongoDB only ends an operation\nat designated interrupt points. See\nTerminate Running Operations\n.\nIP access list\nList of\nIP\naddresses and\nCIDR\nblocks with access to clusters\nwithin an\nAtlas\nproject\n. For client connections over the\npublic Internet,\nAtlas\nallows connections to a cluster only\nfrom entries in the corresponding project's IP access list.\nThe access list may have up to 200 entries.\nAtlas\nalso allows client connections over nonpublic networking,\nsuch\nnetwork peering connections\nor private\nendpoints. These types of connections work irrespective of the\nIP access list. To learn more, see\nSet Up a Network Peering Connection\nand\nLearn About Private Endpoints in Atlas\n.\nIPv6\nA revision to the IP (Internet Protocol) standard with a\nlarge address space to support Internet hosts.\nISODate\nThe international date format used by\nmongosh\nto display dates. The format is\nYYYY-MM-DD HH:MM.SS.millis\n.\nJavaScript\nA scripting language.\nmongosh\n, the legacy\nmongo\nshell, and certain server\nfunctions use a JavaScript interpreter. See\nServer-side JavaScript\nfor more information.\njournal\nA sequential, binary transaction log used to bring the database\ninto a valid state in the event of a hard shutdown.\nJournaling writes data first to the journal and then to the core\ndata files. MongoDB enables journaling by default for 64-bit\nbuilds of MongoDB version 2.0 and newer. Journal files are\npre-allocated and exist as files in the\ndata directory\n.\nSee\nJournaling\n.\nJSON\nJavaScript Object Notation. A plain text format\nfor expressing structured data with support in many programming\nlanguages. For more information, see\nhttp://www.json.org\n.\nCertain MongoDB tools render an approximation of MongoDB\nBSON\ndocuments in JSON format. See\nMongoDB Extended JSON (v2)\n.\nJSON document\nA\nJSON\ndocument is a collection of fields and values in a\nstructured format. For sample JSON documents, see\nhttp://json.org/example.html\n.\nJSONP\nJSON\nwith padding. Refers to a method of injecting JSON\ninto applications.\nPresents potential security concerns\n.\njumbo chunk\nA\nchunk\nthat grows beyond the\nspecified chunk size\nand cannot split into smaller chunks. For\nmore details, see\nIndivisible/Jumbo Chunks\n.\nK-nearest neighbor search\nGiven a set of points\nP\nwith a defined similarity function\nS\n,\nfor a query point\nq\n, finds the set of\nk\npoints in\nP\nwith the\nbest values of\nS*(*p\n,\nq\n). Atlas Vector Search\nENN\nsearch returns the\nexact top\nk\npoints and\nANN\nsearch returns\nk\npoints that are\nsimilar to\nq\n, but not necessarily the\nk\nmost similar to\nq\n.\nkey material\nThe random string of bits used by an encryption algorithm to\nencrypt and decrypt data.\nkey vault collection\nA MongoDB collection that stores the encrypted\nData Encryption Keys\nas\nBSON\ndocuments.\nLDAP\nCross-platform protocol used to authenticate users and authorize\nthem to access data on a cluster. You can use\nAtlas\nto manage\nuser authentication and authorization from all MongoDB clients\nusing your own\nLDAP\nserver over\nTLS\n. A single\nLDAPS\nconfiguration applies to all clusters in an\nAtlas\nproject.\nleast privilege\nAn authorization policy that grants a user only the access\nthat is essential to that user's work.\nlegacy coordinate pairs\nThe format used for\ngeospatial\ndata before MongoDB\nversion 2.4. This format stores geospatial data as points on a\nplanar coordinate system (for example,\n[ x, y ]\n). See\nGeospatial Queries\n.\nLineString\nA LineString is an array of two or more positions. A\nclosed LineString with four or more positions is called a\nLinearRing, as described in the GeoJSON LineString specification:\nhttps://tools.ietf.org/html/rfc7946#section-3.1.4\n. To use a\nLineString in MongoDB, see\nGeoJSON Objects\n.\nlink-token\nString that contains the information necessary to connect from\nCloud Manager or Ops Manager\nto\nAtlas\nduring a live migration from a\nCloud Manager or Ops Manager\ndeployment\nto a cluster in\nAtlas\n.\nWhen you are ready to live migrate data from a\nCloud Manager or Ops Manager\ndeployment,\nyou generate a link-token in\nAtlas\nand then enter it in your\nCloud Manager or Ops Manager\norganization's settings. You use the same link-token to\nmigrate each deployment in your\nCloud Manager or Ops Manager\norganization sequentially,\none at a time. You can generate multiple link-tokens in\nAtlas\n.\nUse one unique link-token for each\nCloud Manager or Ops Manager\norganization.\nlittle-endian\nA byte order in which the least significant byte (little end)\nof a multibyte data value is stored at the lowest memory address.\nclick to enlarge\nlive migration\nProcess to seamlessly move an existing source replica set or\nsharded cluster to\nAtlas\n. During the live migration process,\nAtlas\nkeeps the target cluster in sync with the remote source\nuntil you cut your applications over to the\nAtlas\ncluster.\nAtlas\noffers two modes of live migration:\nPush live migration, known in the\nAtlas\nuser interface as\nLive Migration from Ops Manager or Cloud Manager\n,\nwhere\nAtlas\npushes\na deployment from\nCloud Manager or Ops Manager\nto\nAtlas\n.\nPull live migration, known in the\nAtlas\nuser interface as\nGeneral Live Migration\n, where\nAtlas\npulls\na deployment from a cloud or on-premise deployment to\nAtlas\n.\nTip\nLegacy Live Migration (Pull) of Replica Sets to Atlas\nlock\nMongoDB uses locks to ensure that\nconcurrency\ndoes not affect correctness. MongoDB uses\nread locks\n,\nwrite locks\nand\nintent locks\n. For more information, see\nWhat type of locking does MongoDB use?\n.\nlog files\nContain server events, such as incoming connections, commands run,\nand issues encountered. For more details, see\nLog Messages\n.\nLVM\nLogical volume manager. LVM is a program that abstracts disk\nimages from physical devices and provides a number of raw disk\nmanipulation and snapshot capabilities useful for system\nmanagement. For information on LVM and MongoDB, see\nBack Up and Restore Using LVM on Linux\n.\nmaintenance window\nDay and time of the week when\nAtlas\nshould start weekly\nmaintenance on your cluster. You can set your maintenance window\nin your\nProject Settings\n.\nImportant\nMaintenance Window Considerations\nUrgent Maintenance Activities\nUrgent maintenance activities such as security patches cannot\nwait for your chosen window.\nAtlas\nwill start those\nmaintenance activities when needed.\nOngoing Maintenance Operations\nOnce maintenance is scheduled for your cluster, you cannot change\nyour maintenance window until the current maintenance efforts have\ncompleted.\nMaintenance Requires Replica Set Elections\nAtlas\nperforms maintenance the same way as the maintenance\nprocedure described in the\nMongoDB Manual\n. This\nprocedure requires at least one\nreplica set election\nduring the maintenance window per replica set.\nMaintenance Starts As Close to the Hour As Possible\nMaintenance always begins as close to the scheduled hour as\npossible, but in-progress cluster updates or unexpected system\nissues could delay the start time.\nmap-reduce\nAn aggregation process that has a \"map\"\nphase that selects the data and a \"reduce\" phase that transforms the\ndata. In MongoDB, you can run arbitrary aggregations over data\nusing map-reduce. For the map-reduce implementation, see\nMap-Reduce\n. For all approaches to aggregation,\nsee\nAggregation Operations\n.\nmapping type\nA structure in programming languages that associate keys with\nvalues. Keys may contain embedded pairs of keys and values\n(for example, dictionaries, hashes, maps, and associative arrays).\nThe properties of these structures depend on the language\nspecification and implementation. Typically, the order of keys in\nmapping types is arbitrary and not guaranteed.\nmd5\nA hashing algorithm that calculates a\nchecksum\nfor the\nsupplied data. The algorithm returns a unique value\nto identify the data. MongoDB uses md5 to identify chunks of data\nfor\nGridFS\n. See\nfilemd5 (database command)\n.\nmean\nAverage of a set of numbers.\nmedian\nIn a dataset, the median is the percentile value where 50% of the\ndata falls at or below that value.\nmember\nAn individual\nmongod\nprocess. A\nreplica set\nhas\nmultiple members. A member is also known as a\nnode\n.\nmetadata collection\nIn\nQueryable Encryption\n, the internal collections\nMongoDB uses to enable querying on encrypted fields. See\nMetadata Collections\n.\nMIME\nMultipurpose Internet Mail Extensions. A standard set of type and\nencoding definitions used to declare the encoding and type of data\nin multiple data storage, transmission, and email contexts. The\nmongofiles\ntool provides an option to specify a MIME\ntype to describe a file inserted into\nGridFS\nstorage.\nmode\nNumber that occurs most frequently in a set of numbers.\nmongo\nThe legacy MongoDB shell. The\nmongo\nprocess starts\nthe legacy shell as a\ndaemon\nconnected to either a\nmongod\nor\nmongos\ninstance. The shell\nhas a JavaScript interface.\nStarting in MongoDB v5.0,\nmongo\nis deprecated and\nmongosh\nreplaces\nmongo\nas the\nclient shell. See\nmongosh\n.\nmongod\nThe MongoDB database server. The\nmongod\nprocess\nstarts the MongoDB server as a\ndaemon\n. The MongoDB server\nmanages data requests and background operations. See\nmongod\n.\nMongoDB Charts\nVisualization tool for your\nAtlas\ndata. You can launch MongoDB\nCharts from your\nAtlas\ncluster and view your data with the\nCharts application to begin visualizing your data.\nTip\nMongoDB Charts\nmongos\nThe MongoDB sharded cluster query router. The\nmongos\nprocess starts the MongoDB router as a\ndaemon\n. The MongoDB router acts as an interface\nbetween an application and a MongoDB\nsharded cluster\nand\nhandles all routing and load balancing across the cluster. See\nmongos\nInstances\n.\nmongosh\nMongoDB Shell.\nmongosh\nprovides a shell\ninterface to either a\nmongod\nor a\nmongos\ninstance.\nStarting in MongoDB v5.0,\nmongosh\nreplaces\nmongo\nas the preferred shell.\nmulti-region cluster\nAtlas\ncluster\nspanning multiple geographic regions.\nMulti-region clusters can increase availability and improve\nperformance by routing application queries to the most appropriate\ngeographic regions.\nMulti-region clusters must contain\nelectable nodes\n.\nMulti-region clusters may contain\nread-only nodes\nand\nanalytics nodes\n.\nnamespace\nA namespace is a combination of the database name and\nthe name of the collection or index:\n<database-name>.<collection-or-index-name>\n. All documents\nbelong to a namespace. See\nNamespaces\n.\nNamespace Insights\nAtlas\ntool that monitors collection-level\nquery latency\n. You can view query latency metrics and\nstatistics for certain hosts and operation types. Manage pinned\nnamespaces and choose up to five namespaces to show in the\ncorresponding query latency charts.\nTip\nMonitor Collection-Level Query Latency with Namespace Insights\nnatural order\nThe order\nrecordIds\nare created and stored in the\nWiredTiger\nindex. The default sort\norder for a\ncollection scan\nrun on a single instance is\nnatural order.\nIn replica sets, natural order is not guaranteed to be consistent\nand can differ between members.\nIn sharded collections, natural order is not defined. However,\nusing\n$natural\nstill forces each shard to perform a\ncollection scan.\nFor details, see\n$natural\nand\nReturn in Natural Order\n.\nnetwork partition\nA network failure that separates a distributed system into\npartitions such that nodes in one partition cannot communicate\nwith the nodes in the other partition.\nSometimes, partitions are partial or asymmetric. An example\npartial partition is the a division of the nodes of a network\ninto three sets, where members of the first set cannot\ncommunicate with members of the second set, and the reverse, but\nall nodes can communicate with members of the third set.\nIn an\nasymmetric partition, communication may be possible only when it\noriginates with certain nodes. For example, nodes on one side of\nthe partition can communicate with the other side only if they\noriginate the communications channel.\nnetwork peering connection\nProcess by which two Internet networks connect and exchange\ntraffic. You can directly peer your\nVPC\nwith the\nAtlas\nVPC\ncreated for your MongoDB clusters. Using network peering,\nyour application servers can directly connect to\nAtlas\nwhile\nremaining isolated from public networks.\nTip\nSet Up a Network Peering Connection\nnode\nAn individual\nmongod\nprocess. A\nreplica set\nhas\nmultiple nodes. A node is also known as a\nmember\n.\nnoop\nNo Operation (noop), is an I/O operation scheduler that allocates\nI/O bandwidth for incoming processes based on a first in, first out\nqueue.\nNVMe\nNVMe (Non-Volatile Memory Express) is a protocol for accessing\nhigh-speed storage media.\nNVMe storage\nAvailable for M40+ clusters hosted on AWS\nFor applications hosted on\nAWS\nwhich require low-latency and\nhigh-throughput IO, you can use the NVMe\ncluster class\n.\nThe NVMe cluster class leverages a unique data protocol to\ngreatly improve data access speeds.\nNVMe clusters use a\nhidden secondary node\nconsisting of a provisioned\nvolume with high throughput and\nIOPS\nto facilitate backup.\nTip\nCustomize Cluster Storage\nobject identifier\nSee\nObjectId\n.\nObjectId\nA 12-byte\nBSON\ntype that is unique\nwithin a\ncollection\n. The ObjectId is generated using the\ntimestamp, computer ID, process ID, and a local process incremental\ncounter. MongoDB uses ObjectId values as the default values for\n_id\nfields.\noperation log\nSee\noplog\n.\noperation metadata\nInformation about the execution of processes rather than their content,\nsuch as the number and time of insert, update, and delete operations.\noperation rejection filter\nA rejected\nquery shape\n. For more\ndetails, see\nBlock Slow Queries with Operation Rejection Filters\n.\noperation time\nSee\noptime\n.\noperational node\nAny\nelectable node\nor a\nread-only node\nin your\nAtlas\ncluster.\noperator\nA keyword beginning with a\n$\nused to express an update,\ncomplex query, or data transformation. For example,\n$gt\nis the\nquery language's \"greater than\" operator. For available operators,\nsee\nOperators\n.\noplog\nA\ncapped collection\nthat stores an ordered history of\nlogical writes to a MongoDB database. The oplog is the\nbasic mechanism enabling\nreplication\nin MongoDB.\nSee\nReplica Set Oplog\n.\noplog buffer collection\nA temporary collection created during\nresharding\noperations that stores\noplog\nentries from a\ndonor shard.\nOplog buffer collections ensure that recipient shards\ncan access oplog entries when they get deleted from the donor shard.\nOplog buffer collections are removed when resharding is complete.\noplog hole\nA temporary gap in the oplog because the oplog writes aren't in\nsequence. Replica set\nprimaries\napply\noplog entries in parallel as a batch operation. As a result,\ntemporary gaps in the oplog can occur from entries that aren't\nyet written from a batch.\noplog window\noplog\nentries are time-stamped. The oplog window is the time\ndifference between the newest and the oldest timestamps in the\noplog\n. If a secondary node loses connection with the primary, it\ncan only use\nreplication\nto sync up again if the\nconnection is restored within the oplog window.\noptime\nA reference to a position in the replication\noplog\n. The optime\nvalue is a document that contains:\nts\n, the\nTimestamp\nof\nthe operation.\nt\n, the\nterm\nin which the\noperation was originally generated on the primary.\nordered query plan\nA query plan that returns results in the order consistent with the\nsort()\norder. See\nQuery Plans\n.\norganization\nLogical grouping of\nAtlas\nprojects\n. You can\nleverage an organization to manage billing, users, and security\nsettings for the projects it contains.\nBilling happens at the organization level while preserving visibility\ninto usage in each project.\nYou can view all projects within an organization.\nYou can use teams to bulk assign organization users to projects within the\norganization.\nTip\nOrganizations\norganization ID\nUnique 24-digit hexadecimal string used to identify your\nAtlas\norganization\n. The\nReturn All\nOrganizations\nendpoint returns the ID\nof all organizations that the authenticated user executing the\nAPI\ncall can access.\norphaned cursor\nA cursor that is not correctly closed or iterated over\nin your application code. Orphaned cursors can cause performance\nissues in your MongoDB deployment.\norphaned document\nIn a sharded cluster, orphaned documents are those documents on a\nshard that also exist in chunks on other shards. This is caused by\na failed migration or an incomplete migration cleanup because of\nan atypical shutdown.\nOrphaned documents are cleaned up automatically after a chunk migration\ncompletes. You no longer need to run\ncleanupOrphaned\nto\ndelete orphaned documents.\npassive member\nA member of a\nreplica set\nthat cannot become primary\nbecause its\nmembers[n].priority\nis\n0\n. See\nPriority 0 Replica Set Members\n.\nper-CPU cache\nA type of cache that locally stores memory for a specific CPU core.\nPer-CPU caches are used by the new version of TCMalloc, which is\nintroduced in MongoDB 8.0.\nper-thread cache\nA type of cache that locally stores memory for each application thread.\nPer-thread caches are used by the legacy version of TCMalloc, which is\nused in MongoDB 7.0 and earlier.\npercentile\nIn a dataset, a percentile is a value where that percentage\nof the data is at or below the specified value. For details, see\nCalculation Considerations\n.\nPerformance Advisor\nAtlas\ntool that monitors slow queries executed on your\ncluster and suggests indexes to improve query performance. Each\nindex that the Performance Advisor suggests include an\nimpact\nscore indicating the potential performance\nimprovement that index would bring.\nTip\nMonitor and Improve Slow Queries with the Performance Advisor\nPID\nA process identifier. UNIX-like systems assign a unique-integer\nPID to each running process. You can use a PID to inspect a\nrunning process and send signals to it. See\n/proc\nFile System\n.\npipe\nA communication channel in UNIX-like systems allowing independent\nprocesses to send and receive data. In the UNIX shell, piped\noperations allow users to direct the output of one command into\nthe input of another.\npipeline\nA series of operations in an\naggregation\n.\nSee\nAggregation Pipeline\n.\nplan cache query shape\nA combination of query predicate, sort,\nprojection\n, and\ncollation\n. The plan cache\nquery shape allows MongoDB to identify equivalent queries and\nanalyze their performance.\nFor the query predicate, only the predicate structure and field\nnames are used. The values in the query predicate aren't used. For\nexample, a query predicate\n{ type: 'food' }\nis equivalent to\n{ type: 'drink' }\n.\nTo identify slow queries with the same plan cache query shape,\neach plan cache query shape has a hexadecimal\nplanCacheShapeHash\nvalue. For more information, see\nplanCacheShapeHash and planCacheKey\n.\nStarting in MongoDB 8.0, the existing\nqueryHash\nfield is duplicated\nin a new field named\nplanCacheShapeHash\n. If you're using an earlier\nMongoDB version, you'll only see the\nqueryHash\nfield. Future MongoDB\nversions will remove the deprecated\nqueryHash\nfield, and you'll need\nto use the\nplanCacheShapeHash\nfield instead.\npoint\nA single coordinate pair as described in the GeoJSON Point\nspecification:\nhttps://tools.ietf.org/html/rfc7946#section-3.1.2\n. To\nuse a Point in MongoDB, see\nGeoJSON Objects\n.\npolygon\nAn array of\nLinearRing\ncoordinate arrays, as\ndescribed in the GeoJSON Polygon specification:\nhttps://tools.ietf.org/html/rfc7946#section-3.1.6\n. For Polygons\nwith multiple rings, the first must be the exterior ring and\nany others must be interior rings or holes.\nMongoDB does not permit the exterior ring to self-intersect.\nInterior rings must be fully contained within the outer loop and\ncannot intersect or overlap with each other. See\nGeoJSON Objects\n.\npost-image document\nA document after it was inserted, replaced, or updated. See\nChange Streams with Document Pre- and Post-Images\n.\npowerOf2Sizes\nA setting for each collection that allocates space for each\ndocument\nto maximize storage reuse and reduce\nfragmentation.\npowerOf2Sizes\nis the default for\nTTL\nCollections\n. To change collection settings, see\ncollMod\n.\npre-image document\nA document before it was replaced, updated, or deleted. See\nChange Streams with Document Pre- and Post-Images\n.\npre-splitting\nAn operation performed before inserting data that divides the\nrange of possible shard key values into chunks to facilitate easy\ninsertion and high write throughput. In some cases pre-splitting\nexpedites the initial distribution of documents in\nsharded cluster\nby manually dividing the collection rather than waiting\nfor the MongoDB\nbalancer\nto do so. See\nCreate Ranges in a Sharded Cluster\n.\nprefix compression\nReduces memory and disk consumption by storing any identical index\nkey prefixes only once, per page of memory. See:\nCompression\nfor more about WiredTiger's\ncompression behavior.\nprimary\nIn a\nreplica set\n, the primary is the member that\nreceives all write operations. See\nPrimary\n.\nprimary key\nA record's unique immutable identifier. In\nRDBMS\nsoftware, the primary\nkey is typically an integer stored in each row's\nid\nfield.\nIn MongoDB, the\n_id\nfield stores a document's primary\nkey, which is typically a BSON\nObjectId\n.\nprimary shard\nEach database in a sharded cluster has a primary shard. It is the\ndefault shard for all unsharded collections in the database. See\nPrimary Shard\n.\npriority\nA configurable value that helps determine which members in\na\nreplica set\nare most likely to become\nprimary\n.\nSee\nmembers[n].priority\n.\nprivilege\nA combination of specified\nresource\nand\nactions\npermitted on the resource. See\nprivilege\n.\nproject\nLogical grouping of\nclusters\n. You can have multiple clusters\nwithin a single project and multiple projects within a single\norganization\n.\nNote\nProject is synonymous with\ngroup\n.\nproject ID\nUnique 24-digit hexadecimal string used to identify your\nAtlas\nproject\n. The\nGet All Projects\nAPI\nendpoint returns the ID of all\nprojects that the authenticated user executing the API call can\naccess.\nNote\nProject ID is synonymous with group ID.\nprojection\nA document supplied to a\nquery\nthat specifies the fields\nMongoDB returns in the result set. For more information about projections,\nsee\nProject Fields to Return from Query\nand\nProjection Operators\n.\nquantization\nMethod of compressing the value of individual dimensions in a\nvector into a smaller range to reduce resource consumption and\nimprove speed. Atlas Vector Search supports indexing and querying quantized\nvectors.\nquery\nA read request. MongoDB uses a\nJSON\nform of query language\nthat includes\nquery operators\nwith\nnames that begin with a\n$\ncharacter. In\nmongosh\n, you can run queries using the\ndb.collection.find()\nand\ndb.collection.findOne()\nmethods. See\nQuery Documents\n.\nquery framework\nA combination of the\nquery optimizer\nand query execution engine\nthat processes an operation.\nquery operator\nA keyword beginning with\n$\nin a query. For example,\n$gt\nis the \"greater than\" operator. For a list of\nquery operators, see\nquery operators\n.\nquery optimizer\nA process that generates query plans. For each query, the\noptimizer generates a plan that matches the query to the index\nthat returns the results as efficiently as possible. The\noptimizer reuses the query plan each time the query runs. If a\ncollection changes significantly, the optimizer creates a new\nquery plan. See\nQuery Plans\n.\nquery plan\nMost efficient execution plan chosen by the query planner. For\nmore details, see\nQuery Plans\n.\nquery predicate\nAn expression that returns a boolean indicating whether a document\nmatches the specified query. For example,\n{ name: { $eq: \"Alice\"\n} }\n, which returns documents that have a field\n\"name\"\nwhose\nvalue is the string\n\"Alice\"\n.\nQuery predicates can contain child expressions and operators for\nmore complex matching. To see available query operators, see\nQuery and Projection Operators\n.\nQuery Profiler\nAtlas\ntool that diagnoses and monitors performance\nissues in your cluster. The Query Profiler can expose long-running\nqueries and their performance statistics. You can filter the data\nreturned by the Query Profiler to hone in on specific namespaces\nand operation types.\nquery shape\nA\nquery shape\nis a set of specifications that group similar queries.\nFor details, see\nQuery Shapes\n.\nrange\nA contiguous range of\nshard key\nvalues within a\nchunk. Data ranges include the lower boundary and\nexclude the upper boundary. MongoDB migrates data when a\nshard contains\ntoo much data of a collection\nrelative to other shards.\nSee\nData Partitioning with Chunks\nand\nSharded Cluster Balancer\n.\nRDBMS\nRelational Database Management System. A database management\nsystem based on the relational model, typically using\nSQL\nas the query language.\nread concern\nSpecifies a level of isolation for read operations. For example,\nyou can use read concern to only read data that has propagated to\na majority of nodes in a\nreplica set\n. See\nRead Concern\n.\nread lock\nA shared\nlock\non a resource such as a collection or\ndatabase that, while held, allows concurrent readers but no\nwriters. See\nWhat type of locking does MongoDB use?\n.\nread preference\nA setting that determines how clients direct read operations.\nRead preference affects all replica sets, including shard replica\nsets. By default, MongoDB directs reads to\nprimaries\n. However, you may also direct reads to secondaries for\neventually consistent\nreads. See\nRead Preference\n.\nread-only node\nReplica set in a dedicated geographic region that supplements your\nelectable node\nregions. You can use read-only nodes to\nlocalize data where it is most frequently read to improve\nperformance.\nReal-Time Performance Panel\nAtlas\nmonitoring service that displays current network\ntraffic, database operations on your clusters, and hardware\nstatistics about your host machines. Use the\nRTPP\nto visually\nevaluate query execution times, monitor network activity, and\ndiscover potential replication lag on secondary members of\nreplica sets.\nTip\nMonitor Real-Time Performance\nrecall\nMeasures the fraction of true nearest neighbors that were returned\nby an\nANN\nsearch. This measure reflects how close the algorithm\napproximates the results of\nENN\nsearch. The notation\nRecall@k\nrefers to the measurement of how many of the true nearest\nneighbors were present in the top\nk\nresults returned by Atlas Vector Search.\nrecovering\nA\nreplica set\nmember status indicating that a member\nis not ready to begin activities of a secondary or primary.\nRecovering members are unavailable for reads.\nrelative system CPU utilization\nThe CPU utilization relative to the amount of baseline CPU assigned to\na cloud instance. You can calculate relative system CPU utilization\nby dividing the\nabsolute system CPU utilization\nby the amount\nof baseline CPU assigned to a cloud instance.\nMongoDB caps relative system CPU utilization at 100%. When a cloud provider\nthrottles CPU utilization for a cloud instance, or bursts CPU utilization\nfor an instance above the baseline amount of CPU available to that\ninstance, the relative system CPU value is 100%.\nSee also\nabsolute system CPU utilization\n, and\nburstable instances\n.\nreplica set\nGroup of MongoDB servers that maintain the same data set. Replica\nsets provide redundancy, high availability, and are the basis for\nall production deployments.\nreplication\nA feature allowing multiple database servers to share the same\ndata. Replication ensures data redundancy and enables load\nbalancing. See\nReplication\n.\nreplication lag\nThe time period between the last operation in the\nprimary's\noplog\nand the last operation\napplied to a particular\nsecondary\n. You typically want\nreplication lag as short as possible. See\nReplication\nLag\n.\nresident memory\nThe subset of an application's memory currently stored in\nphysical RAM. Resident memory is a subset of\nvirtual memory\n,\nwhich includes memory mapped to physical RAM and to storage.\nresource\nA database, collection, set of collections, or cluster. A\nprivilege\npermits\nactions\non a specified\nresource. See\nresource\n.\nrole\nA set of privileges that permit\nactions\non\nspecified\nresources\n. Roles assigned to a user\ndetermine the user's access to resources and operations. See\nSecurity\n.\nrollback\nA process that reverts write operations to ensure the consistency\nof all replica set members. See\nRollbacks During Replica Set Failover\n.\nrolling restart\nProcess that restarts all nodes in the cluster in sequence. To\nmaintain cluster availability,\nAtlas\nrestarts one node at a\ntime starting with a\nsecondary\nnode.\nAtlas\nalways maintains a primary node until the rolling\nrestart completes.\nscalar quantization\nScalar quantization involves selecting the minimum and maximum values\nacross all indexed vectors within a segment for each dimension,\nand producing equally sized bins between them. The mappings for\neach of these dimensions to the bins yields the new quantized\nvalues. Atlas Vector Search supports automatic scalar quantization for your\nfloat32 vectors, and ingestion and indexing of your scalar\nquantized vectors from embedding providers.\nsecondary\nA\nreplica set\nmember that replicates the contents of the\nmaster database. Secondary members may run read requests, but\nonly the\nprimary\nmembers can run write operations. See\nSecondaries\n.\nsecondary index\nA database\nindex\nthat improves query performance by\nminimizing the amount of work that the query engine must perform\nto run a query. See\nIndexes\n.\nsecondary member\nSee\nsecondary\n. Also known as a secondary node.\nseed list\nA seed list is used by drivers and clients (like\nmongosh\n) for initial discovery of the replica\nset configuration. Seed lists can be provided as a list of\nhost:port\npairs (see\nStandard Connection String Format\nor through DNS entries.) For more information,\nsee\nSRV Connection Format\n.\nself-managed\nA MongoDB instance that is set up and maintained by an\nindividual or organization, and not an external management or\nthird-party services (such as MongoDB Atlas).\nsemantic search\nSearch for values that have a similar meaning to query. Semantic\nsearch captures the natural relationship between words or phrases\neven when there is no lexical overlap. Semantic search and vector\nsearch are often used interchangeably. Atlas Vector Search supports semantic\nsearch on vector data stored in\nAtlas\nclusters.\nset name\nThe arbitrary name given to a replica set. All members of a\nreplica set must have the same name specified with the\nreplSetName\nsetting or the\n--replSet\noption.\nshard\nA single\nmongod\ninstance or\nreplica set\nthat stores part of a\nsharded cluster's\ntotal data set. Typically, in a production deployment, ensure all\nshards are part of replica sets. See\nShards\n.\nshard key\nThe field MongoDB uses to distribute documents among members of a\nsharded cluster\n. See\nShard Keys\n.\nsharded cluster\nThe set of nodes comprising a\nsharded\nMongoDB\ndeployment. A sharded cluster consists of config servers,\nshards, and one or more\nmongos\nrouting processes. See\nSharded Cluster Components\n.\nsharding\nA database architecture that partitions data by key ranges and\ndistributes the data among two or more database instances.\nSharding enables horizontal scaling. See\nSharding\n.\nshared cluster\nCluster category containing\nM0\n(\nfree tier\n) tier clusters.\nShared clusters are generally used for development and small\nproduction workloads.\nTip\nAtlas M0 (Free Cluster) Limits\nshell helper\nA method in\nmongosh\nthat has a concise\nsyntax for a\ndatabase command\n. Shell helpers\nimprove the interactive experience. See\nmongosh\nMethods\n.\nsimilarity function\nMeasures the similarity between two vectors. Atlas Vector Search supports\neuclidean\n,\ncosine\n, and\ndotProduct\nsimilarity functions.\nsingle-master replication\nA\nreplication\ntopology where only a single database\ninstance accepts writes. Single-master replication ensures\nconsistency and is the replication topology used by MongoDB.\nSee\nReplica Set Primary\n.\nsnappy\nA compression/decompression library to balance\nefficient computation requirements with reasonable compression rates.\nSnappy is the default compression\nlibrary for MongoDB's use of\nWiredTiger\n. See\nSnappy\nand the\nWiredTiger compression\ndocumentation\nfor more information.\nsnapshot\nA\nsnapshot\nis a copy of the data in a\nmongod\ninstance at a\nspecific point in time. You can retrieve snapshot metadata for the whole cluster\nor replica set, or for a single config server in a cluster.\nsoftIRQ\nThe CPU utilization metric that reflects a portion of CPU that a cloud\ninstance currently uses to process software interrupt requests.\nOn some cloud providers, this metric is useful for tracking CPU\nutilization on\nburstable instances\n.\nsort key\nThe value compared against when sorting fields. To learn how\nMongoDB determines the sort key for non-numeric fields, see\nComparison/Sort Order\n.\nsplit\nThe division between\nchunks\nin a\nsharded cluster\n. See\nData Partitioning with Chunks\n.\nSQL\nStructured Query Language (SQL) is used for interaction with\nrelational databases.\nSSD\nSolid State Disk. High-performance storage that uses solid\nstate electronics for persistence instead of rotating platters\nand movable read/write heads used by mechanical hard drives.\nstale read\nA stale read refers to when a transaction reads old (stale) data that has\nbeen modified by another transaction but not yet committed to the\ndatabase.\nstandalone\nAn instance of\nmongod\nthat runs as a single server and not as part of a\nreplica set\n. To convert a standalone instance to a\nreplica set, see\nConvert a Standalone Self-Managed mongod to a Replica Set\n.\nNote\nA standalone instance is\nnot\na replica set with only one\nmember.\nstash collection\nA temporary collection created on the recipient shard for\neach donor shard during\nresharding\noperations.\nStash collections temporarily hold documents that cannot be\nimmediately inserted due to operation conflicts. For\nexample, if a document's shard key has been updated, it now belongs\nto a different shard, and the order of operations applied to this document\ncan be ambiguous. The recipient stores these documents in a\nstash collection until it can apply operations\nin the correct order.\nstep down\nThe\nprimary\nmember of the replica set removes\nitself as primary and becomes a\nsecondary\nmember.\nIf a replica set loses contact with the primary, the\nsecondaries elect a new primary.  When the old primary\nlearns of the election, it steps down and rejoins the\nreplica set as a secondary.\nIf the user runs the\nreplSetStepDown\ncommand, the primary steps down, forcing the replica set\nto elect a new primary.\nstorage engine\nThe part of a database that is responsible for managing how data\nis stored and accessed, both in memory and on disk. Different\nstorage engines perform better for specific workloads. See\nStorage Engines for Self-Managed Deployments\nfor specific details on the built-in\nstorage engines in MongoDB.\nstorage order\nSee\nnatural order\n.\nstrict consistency\nA property of a distributed system requiring that all members\ncontain the latest changes to the system. In a database\nsystem, this means that any system that can provide data must\ncontain the latest writes.\nSubject Alternative Name\nSubject Alternative Name (SAN) is an extension of the X.509\ncertificate which allows an array of values such as IP addresses\nand domain names that specify the resources a single security\ncertificate may secure.\nsync\nThe\nreplica set\noperation where members replicate data\nfrom the\nprimary\n. Sync first occurs when MongoDB creates\nor restores a member, which is called\ninitial sync\n. Sync\nthen occurs continually to keep the member updated with changes to\nthe replica set's data. See\nReplica Set Data Synchronization\n.\nsyslog\nOn UNIX-like systems, a logging process that provides a uniform\nstandard for servers and processes to submit logging information.\nMongoDB provides an option to send output to the host's syslog\nsystem. See\nsyslogFacility\n.\ntag\nA label applied to a replica set member and used by\nclients to issue data-center-aware operations. For more information\non using tags with replica sets, see\nRead Preference Tag Set Lists\n.\nNote\nSharded cluster\nzones\nreplace\ntags\n.\ntag set\nA document containing zero or more\ntags\n.\ntailable cursor\nFor a\ncapped collection\n, a tailable cursor is a cursor that\nremains open after the client exhausts the results in the initial\ncursor. As clients insert new documents into the capped collection,\nthe tailable cursor continues to retrieve documents.\nteam\nGroup of\nAtlas users\nin the same organization. You can use teams to grant access to\nthe same group of Atlas users across multiple\nprojects\n. All users in the team share the same\nproject access.\nterm\nFor the members of a replica set, a monotonically increasing\nnumber that corresponds to an election attempt.\ntime series collection\nA\ncollection\nthat efficiently stores\nsequences of measurements over a period of time. See\nTime Series\n.\ntopology\nThe state of a deployment of MongoDB instances. Includes:\nType of deployment (standalone, replica set, or sharded cluster).\nAvailability of servers.\nRole of each server (\nprimary\n,\nsecondary\n,\nconfig server\n, or\nmongos\n).\ntransaction\nGroup of read or write operations. For details, see\nTransactions\n.\ntransaction coordinator\nA component of MongoDB that manages\ntransactions\nin a\nreplica set\nor a\nsharded cluster\n. It coordinates the execution and completion of\nmulti-document transactions across nodes and allows a complex\noperation to be treated as an\natomic operation\n.\nTSV\nA text-based data format consisting of tab-separated values.\nThis format is commonly used to exchange data between relational\ndatabases because the format is suited to tabular data. You can\nimport TSV files using\nmongoimport\n.\nTTL\nTime-to-live (TTL) is an expiration time or\nperiod for a given piece of information to remain in a cache or\nother temporary storage before the system deletes it or ages it\nout. MongoDB has a TTL collection feature. See\nExpire Data from Collections by Setting TTL\n.\nunbounded array\nAn array that consistently grows larger over time. If a document\nfield value is an unbounded array, the array may negatively impact\nperformance. In general, design your schema to avoid unbounded\narrays.\nunique index\nAn index that enforces uniqueness for a particular field in\na single collection. See\nUnique Indexes\n.\nunix epoch\nJanuary 1st, 1970 at 00:00:00 UTC. Commonly used in expressing time,\nwhere the number of seconds or milliseconds since this point is counted.\nunordered query plan\nA query plan that returns results in an order inconsistent with the\nsort()\norder.\nSee\nQuery Plans\n.\nupsert\nAn option for update operations. For example:\ndb.collection.updateOne()\n,\ndb.collection.findAndModify()\n. If upsert is\ntrue\n,\nthe update operation either:\nupdates the document(s) matched by the query.\nor if no documents match, inserts a new document. The new\ndocument has the field values specified in the update operation.\nFor more information about upserts, see\nInsert a New Document if No Match Exists (\nUpsert\n)\n.\nvector database\nSystem that stores vector embeddings and associated metadata, and\nenables nearest neighbor search on the stored vector embeddings.\nYou can use\nAtlas\nas your vector database and Atlas Vector Search to\nperform vector search on the stored vector embeddings. You can use\nvector database to implement\nRAG\n.\nvector index\nData structure that efficiently processes nearest neighbor search\nqueries. Atlas Vector Search supports creating indexes of type\nvector\nto\nindex fields for running\n$vectorSearch\nqueries.\nvector search\nMethod of performing\nk\nnearest neighbor search over a set of\nvectors stored in a vector index. Atlas Vector Search supports\nANN\nand\nENN\nsearch for\nk\nnearest neighbors.\nvirtual memory\nAn application's working memory, typically residing on both\ndisk and in physical RAM.\nWGS84\nThe default reference system and geodetic datum that MongoDB uses\nto calculate geometry over an Earth-like sphere for geospatial\nqueries on\nGeoJSON\nobjects. See the\n\"EPSG:4326: WGS 84\" specification:\nhttp://spatialreference.org/ref/epsg/4326/\n.\nwindow operator\nReturns values from a span of documents from a collection. See\nwindow operators\n.\nworking set\nThe data that MongoDB uses most often.\nwrite concern\nSpecifies whether a write operation has succeeded. Write concern\nallows your application to detect insertion errors or unavailable\nmongod\ninstances. For\nreplica sets\n, you can configure write concern to confirm replication to a\nspecified number of members. See\nWrite Concern\n.\nwrite conflict\nA situation where two concurrent operations, at least one of which\nis a write, try to use a resource that violates the\nconstraints for a storage engine that uses optimistic\nconcurrency control\n. MongoDB automatically ends and\nretries one of the conflicting write operations.\nwrite lock\nAn exclusive\nlock\non a resource such as a collection\nor database. When a process writes to a resource, it takes\nan exclusive write lock to prevent other processes from writing\nto or reading from that resource. For more information on\nlocks, see\nFAQ: Concurrency\n.\nzlib\nA data compression library that provides higher compression rates\nat the cost of more CPU, compared to MongoDB's use of\nsnappy\n. You can configure\nWiredTiger\nto use zlib as its compression library. See\nhttp://www.zlib.net\nand the\nWiredTiger compression documentation\nfor more\ninformation.\nzone\nA grouping of documents based on ranges of\nshard key\nvalues\nfor a given sharded collection. Each shard in the sharded cluster can\nbe in one or more zones. In a balanced cluster, MongoDB\ndirects reads and writes for a zone only to those shards\ninside that zone. See the\nZones\nmanual page for more\ninformation.\nzstd\nA data compression library that provides higher compression rates\nand lower CPU usage when compared to\nzlib\n.\nBack\nError Codes\nNext\nLog Messages",
    "url": "https://www.mongodb.com/docs/manual/reference/glossary/#std-term-snapshot",
    "source": "mongodb",
    "doc_type": "manual",
    "scraped_at": 31805.4864069
  },
  {
    "title": "Atlas Flex Costs",
    "content": "Docs Home\n/\nAtlas\n/\nManage Billing\nAtlas Flex Costs\nCopy page\nAtlas\nFlex clusters give you access to modern database\nfeatures including\nAtlas Search\n, Atlas Vector Search,\nAtlas Stream Processing\n, triggers, and more.\nTo learn more, see\nCluster Types\n.\nFlex clusters are capped at $30 per month. You pay a monthly base price,\nwhich includes:\n5GB storage\n100 operations/sec\nunlimited data transfer\nFlex clusters scale with your usage. If you use additional operations/sec,\nMongoDB charges you based on the following pricing tiers.\nUsage Cost Summary\nAs a user of an Atlas Flex cluster, you pay between $8 and $30 for\n30 days of usage. As for all\nAtlas\nclusters, the Flex cluster\ntier follows a pay-as-you-go model billed hourly.\nThe following table outlines the pricing for Flex clusters. Note that\nthere is a volume discount as your usage increases.\nTier (ops/second)\nMarginal Monthly Cost\nEffective Discount\nTotal Monthly Cost\nTotal Hourly Cost\n0 - 100 (Base)\n$8.00\n0%\n$8.00\n$0.0110\n100 - 200\n$7.00\n12.5%\n$15.00\n$0.0205\n200 - 300\n$6.00\n25%\n$21.00\n$0.0288\n300 - 400\n$5.00\n37.5%\n$26.00\n$0.0356\n400 - 500\n$4.00\n50%\n$30.00\n$0.0411\nExamples\nScenario 1 - Monthly Usage\nYour workload runs less than 100 ops/sec for 20 days, and then bursts\nup to 250 ops/second for 5 days. Then, for another 3 days, the workload bursts\nto 500 ops/second.\nUsage\nCost\nYour workload runs less than 100 ops/sec for 20 days.\n$0.011/hour * 24 hours * 20 days = $5.28\nThe workload bursts up to 250 ops/second for 5 days.\n$0.0288/hour * 24 hours * 5 days = $3.46\nThe workload bursts to 500 ops/second for another 3 days.\n$0.0411/hour * 24 hours * 3 days = $2.96\nTotal Usage Charge\n$5.28 + $3.46 + $2.96 = $11.70\nScenario 2 - One Day Use\nYour workload runs less than 100 ops/sec for 5 hours. It bursts up to\n410 ops/second for 10 hours, followed by 9 hours at 150 ops/second.\nAfter these 24 hours, you delete the cluster.\nUsage\nCost\nYour workload runs less than 100 ops/sec for 5 hours.\n$0.011 hourly base cost * 5 hours = $0.055\nThe workload bursts up to 410 ops/second for 10 hours.\n$0.0411/hour * 10 hours = $0.41\nThe workload drops to 150 ops/second for another 9 hours.\n$0.0205/hour * 9 hours = $0.18\nYou delete the cluster.\n(no charge)\nTotal Usage Charge\n$0.055 + $0.41 + $0.18 = $0.65\nBack\nCosts for Serverless Instances (deprecated)\nNext\nData Federation",
    "url": "https://www.mongodb.com/docs/atlas/billing/atlas-flex-costs/",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31806.0548387
  },
  {
    "title": "Manage Databases in Compass",
    "content": "Docs Home\n/\nCompass\n/\nInteract with Your Data\nManage Databases in\nCompass\nCopy page\nA database is a container for\ncollections\n.\nEach database gets its own set of files on the host file system.\nA single MongoDB server typically has multiple databases.\nDatabases Tab\nThe\nDatabases\ntab lists the existing databases for your\nMongoDB deployment. To access the\nDatabases\ntab, click the\ndeployment name in the\nConnections Sidebar\n.\nclick to enlarge\nFrom this view, you can click a database name in the sidebar to view its\ncollections. Alternatively, you can view database collections by clicking the\ndesired database in the left-hand navigation.\nYou can also\ncreate\nor\ndrop databases\nfrom this view.\nCreate a Database\n1\nOpen the\nCreate Database\ndialog.\nIn the\nConnections Sidebar\n, click the\nicon to the right of the connection name to bring up the\nCreate Database\ndialog.\n2\nEnter database and first collection information.\nIn the dialog, enter the name of the database to create and its\nfirst collection. Both the database name and the collection name are\nrequired.\nIf you want to create a\ncapped collection\n,\nselect the\nCapped Collection\ncheckbox and enter the maximum bytes.\nIf you want to use\ncustom collation\non the collection,\nselect the\nUse Custom Collation\ncheckbox and select the\ndesired collation settings.\nIf your deployment is connected using\nIn-Use Encryption\n, you can\nuse\nQueryable Encryption\non the newly\ncreated collection. Check the\nQueryable Encryption\noption\nand indicate the following encryption properties:\nEncrypted Fields\n.\n(Optional)\nKMS Provider\n.\n(Optional)\nKey Encryption Key\n.\n3\nClick\nCreate Database\nto create the database and its first collection.\nDrop a Database\n1\nClick the trash can icon for the database.\nFrom the\nDatabases\ntab, to delete a\ndatabase, click on the trash can icon for that database. A\nconfirmation dialog appears.\n2\nConfirm the database to delete.\nIn the dialog, enter the name of the database to delete.\n3\nClick\nDrop Database\nto delete the database.\nLimitations\nCreating and dropping databases is not permitted in\nMongoDB Compass Readonly Edition\n.\nCreating databases is not permitted if you are connected to a\nData Lake\n.\nBack\nInteract with Your Data\nNext\nManage Collections",
    "url": "https://www.mongodb.com/docs/compass/databases/#std-label-compass-create-database",
    "source": "mongodb",
    "doc_type": "compass",
    "scraped_at": 31806.8274917
  },
  {
    "title": "Manage Subscriptions",
    "content": "Docs Home\n/\nAtlas\n/\nManage Billing\nManage Subscriptions\nCopy page\nA MongoDB subscription offers flexible payment terms and advanced support\noptions. Subscriptions allow for billing in different currencies, billing\nin arrears, billing monthly or for annual invoicing  with\nACH\npayments\nor wire transfers. Subscriptions provide order forms\n[\n1\n]\nand\nan Enterprise Customer Service Agreement (CSA).\nSubscription Types\nSubscription\nDescription\nElastic subscription\nYou receive a monthly invoice for your usage.\nFlex Commitment subscription\nYou commit to paying for a number of credits for a term, such as\nannual, and receive a monthly invoice for your usage. Unused credits\nare charged in the last month of the subscription's term.\nMonthly Commitment subscription\nYou commit to paying for a number of credits for a term, such as\nannual, and receive a monthly invoice for the greater of your usage\nor your prorated monthly commitment. You can apply unused credits\nat a later date during the term to cover overages.\nMarketplace subscription\nYou receive invoices for your\nAtlas\nusage through\nthe marketplace from which you subscribed.\nPrepaid subscription\nYou prepay for credits, usable for a term you define upon\nsubscribing, such as annual.\nPurchase a Subscription\nTo purchase a subscription, contact\nMongoDB Sales\n.\nWhen you purchase a subscription, you are asked to provide a\nbilling contact\nemail address. Your billing contact receives invoices.\nNote\nMongoDB recommends inviting your billing contact to your\nsubscription-paying organization.\nActivate a Subscription\nNote\nYou must be an\nOrganization Owner\nor\nOrganization Billing Admin\nto activate a subscription.\nWhen you purchase a subscription from\nMongoDB Sales\n,\nMongoDB sends you an email containing an activation code.\nTo apply this subscription to your\nAtlas\norganization:\n1\nIn\nAtlas\n, go to the\nBilling\npage for your organization.\nWARNING:\nNavigation Improvements In Progress\nWe're currently rolling out a new and improved navigation\nexperience. If the following steps don't match your view in the\nAtlas UI, see\nthe preview documentation\n.\nIf it's not already displayed, select your desired organization\nfrom the\nOrganizations\nmenu in the\nnavigation bar.\nDo one of the following steps:\nClick\nBilling\nin the navigation bar.\nClick\nBilling\nin the sidebar.\nThe\nBilling\npage displays.\n2\nGo to the\nOverview\ntab.\nIf it isn't already displayed, click the\nOverview\ntab.\n3\nClick the\nApply Code\nbutton.\n4\nEnter your\nActivation Code\n.\n5\nClick\nSubmit\n.\nWith an active subscription,\nAtlas\ndisplays a confirmation icon\nand message in the\nPayment Method\ncard.\nclick to enlarge\nNote\nIf you do not add your subscription activation code within the\nfirst 30 days of use, MongoDB charges the credit card associated\nwith the account. Credit card charges are non-refundable.\nIf you lose your activation code,\nrequest support\n.\nSubscription Expiration\nIf your subscription expires:\nAtlas\nuses your configured payment method to pay any remaining\ncharges.\nAtlas\ndowngrades any\nsupport plan\nthat you've\npurchased with your subscription.\nIf you have no payment method added to your organization,\nAtlas\nprompts you to\nset a payment method\nwhen you\nmake billing changes through the Atlas UI or Admin API. If your\npayment method declines, your account will be at risk of suspension.\nOnce you\nactivate\nyour next subscription,\nAtlas\nbills you for any unpaid charges. If your subscription becomes\ndelayed, contact your\nMongoDB Sales\nrepresentative.\nTo stop incurring any future charges, you must\nterminate\nyour deployments.\nView Available Subscription Credits\nTo view active and upcoming subscription credits:\n1\nIn\nAtlas\n, go to the\nBilling\npage for your organization.\nWARNING:\nNavigation Improvements In Progress\nWe're currently rolling out a new and improved navigation\nexperience. If the following steps don't match your view in the\nAtlas UI, see\nthe preview documentation\n.\nIf it's not already displayed, select your desired organization\nfrom the\nOrganizations\nmenu in the\nnavigation bar.\nDo one of the following steps:\nClick\nBilling\nin the navigation bar.\nClick\nBilling\nin the sidebar.\nThe\nBilling\npage displays.\n2\nView the available credits.\nIf it isn't already selected, click the\nOverview\ntab.\nReview the\nAvailable Credits\ntable.\nclick to enlarge\nThe\nAvailable Credits\ntable displays:\nAn\nElastic Billing Enabled\nindicator if your organization\nhas\nElastic Billing\nenabled.\nYour active available credits. Active credit items display a progress\nbar to show your usage to date of that total pool of credits.\nElastic Billing\nsubscriptions, represented as infinite available credit. An active\nElastic Billing subscription means you're charged for\nAtlas\nusage as you go.\nUpcoming available credits that you have not yet started to spend,\ndisplayed in gray.\nAvailable credits are drawn from the top item to the bottom in the order\nthey appear on the table.\nTo view your expired credits, see the\nHistory\ntable located\nbelow the\nAvailable Credits\ntable.\nMonthly Commitment Subscription Credits\nIf your subscription includes a commitment for a term (e.g. annual),\nyou can view your month's spending against your commitment on your\norganization's\nAvailable Credits\ntable.\nclick to enlarge\nAt the end of the month, if your usage is lower than your prorated\nmonthly commitment, you are charged for the prorated commitment.\nUnused credits can be applied at a later date during the term to cover\noverages.\nIf you use all your monthly commitment subscription credits before your\nsubscription expires and your subscription has\nElastic Billing\nenabled,\nAtlas\nbills\nyou for usage beyond your subscription credits. When your Monthly Commitment Subscription transitions to Elastic Billing:\nYou receive two tax invoices for the same billing period\nYour cloud invoice displays an\nAtlas\nElastic Invoice and your\nfinal\nAtlas\nMonthly Commit payment in the\nPayment Details\nTo review your used credits and confirm if your subscription has entered Elastic Billing:\n1\nReview your used credits.\nDo one or both of the following steps:\nIn\nAtlas\n, go to the\nBilling\npage for your organization.\nWARNING:\nNavigation Improvements In Progress\nWe're currently rolling out a new and improved navigation\nexperience. If the following steps don't match your view in the\nAtlas UI, see\nthe preview documentation\n.\nIf it's not already displayed, select your desired organization\nfrom the\nOrganizations\nmenu in the\nnavigation bar.\nDo one of the following steps:\nClick\nBilling\nin the navigation bar.\nClick\nBilling\nin the sidebar.\nThe\nBilling\npage\ndisplays.\nDetails appear on the\nOverview\ntab.\nClick\nActivity Feed\nin the sidebar.\nThe\nOrganization Activity Feed\npage\ndisplays.\nAtlas\ncreates the following event:\nActive Invoicing Period initiated. MongoDB will invoice your organization based on your monthly usage.\n2\nConfirm whether your subscription has entered Elastic Billing.\nElastic Billing Subscription\nIf you have an active Elastic Billing subscription, you are charged for\nyour usage as you go.\nNote\nIf Elastic Billing is enabled, an indicator displays at the top of\nthe\nAvailable Credits\ntable.\nThe Elastic Billing indicator does not necessarily mean your Elastic\nBilling subscription is active, only that it is enabled. If you run\nout of credits, your organization will automatically transition to\nElastic Billing.\nclick to enlarge\nThe\nUsed\ncolumn displays your elastic usage to date.\nGoogle Cloud Monthly Commitment Billing\nWith a Google Cloud Monthly Commitment subscription, you have a minimum monthly\ncommitment you must pay Google each month. Your minimum monthly amount due\nis your total commitment value divided by the number of months\nin your commitment. Whether you use less or more than your monthly\ncommitment, you are billed at least this amount.\nNote\nAfter your cumulative subscription usage exceeds your total commitment value,\nAtlas\ntransitions your account to elastic billing.\nWith elastic billing,\nAtlas\nbills you the full value of your usage each month\nfor the remainder of your commitment. Your Google Cloud invoice reflects\nyour elastic billing charges in addition to your monthly minimum commitment.\nExample\nFor a $12,000 total annual commitment,\nafter you accrue $12,000 of usage, Google Cloud bills the monthly commitment\nof $1,000 plus any additional usage as reported by\nAtlas\n.\nIf your usage for a given\nmonth after you exhaust your total annual commitment is $4,000,\nGoogle Cloud charges $5,000 for that month\n($1,000 monthly minimum + $4,000 elastic billing).\nView Subscription Charges\nIn each one of your invoices, the\nSummary By Project\nand\nSummary By Service\ntables list your usage in terms of\nAtlas\ncredits.\nIn addition, you can examine the\nPayment and Usage Details\nfor your subscription.\nTo view your monthly commitment subscription charges in the\nPayment Details\nsection of your invoice:\n1\nIn\nAtlas\n, go to the\nBilling\npage for your organization.\nWARNING:\nNavigation Improvements In Progress\nWe're currently rolling out a new and improved navigation\nexperience. If the following steps don't match your view in the\nAtlas UI, see\nthe preview documentation\n.\nIf it's not already displayed, select your desired organization\nfrom the\nOrganizations\nmenu in the\nnavigation bar.\nDo one of the following steps:\nClick\nBilling\nin the navigation bar.\nClick\nBilling\nin the sidebar.\nThe\nBilling\npage displays.\n2\nOpen the\nInvoices\ntab.\n3\nSelect the invoice and click the link under\nInvoice Date\nor\nInvoice Period\n.\n4\nScroll down to the\nPayment Details\nsection of your invoice.\nThe\nPayment Details\ntable lists payment\nmethods in the left-most column and shows all the related payment\ninformation for each payment method.\n5\nLocate the\nAtlas monthly commitment\nin the\nPayment method\ncolumn.\n6\nLocate the\nTotal\ncolumn for the\nAtlas monthly commitment\nrow.\nThe\nTotal\ncolumn shows your total subscription cost\nfor that invoice period.\n7\nIn the\nActions\ncolumn, click\nView Details\n.\nThe pop-up informational card opens that shows the details of your\nmonthly commitment for your subscription. This card compares your\nsubscription commitment to your usage during the invoice period.\nYou are charged the greater of your monthly commit or your usage.\nUnused credits are applied to cover overages during the term of\nyour subscription.\nPay Subscription Charges\nWith a subscription, you receive two invoices each month:\nA\ncloud invoice\ndetailing your MongoDB Cloud usage, viewable\nin\nAtlas\non your organization's\nBilling\npage.\nA\ntax invoice\nemailed to your\nbilling contact\nwith MongoDB bank\ndetails and an amount due.\nNote\nIf you purchase a MongoDB subscription from a marketplace, you\ndo not receive a tax invoice. Instead, you receive an invoice\nwith an amount due through that marketplace.\nYour cloud invoice represents MongoDB Cloud usage in\nUSD\nand does not\naccount for currency conversions or other adjustments. The tax invoice\nemailed to your billing contact provides the amount to pay.\nIf your subscription covers the billing period, pay the invoice emailed\nto your billing contact. If your subscription does not cover the\nbilling period, your payment method on file is charged.\nIf you are a YayPay customer, you can pay your monthly commitment or\nelastic invoices directly from the\nPayment Details\nsection\nof your invoice page. To learn more, see\nView and Pay Your Current\nInvoice\n.\nSee also\nHow can I pay for an\nAtlas\nsubscription with a credit card?\n.\nUpdate your Billing Contact Information\nTo update your billing contact information, email a request to\nar@mongodb.com\n. Please include your company's name, your Organization\nID, and your new billing contact information.\nYou can find your Organization ID in the Atlas UI, under your\norganization's\nGeneral Settings\n.\n[\n1\n]\nAn order form represents your specific subscription agreement with\nMongoDB. MongoDB sends a new order form to your billing contact each\nsubscription cycle.\nBack\nInvoice Breakdown\nNext\nBilling Optimization",
    "url": "https://www.mongodb.com/docs/atlas/billing/subscriptions/#std-label-atlas-subscriptions",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31807.4214269
  },
  {
    "title": "Reference",
    "content": "Docs Home\n/\nOps Manager\nReference\nCopy page\nMongoDB Compatibility Matrix\nMongoDB versions compatible with\nOps Manager\nfeatures.\nSupported Browsers\nBrowsers that\nOps Manager\nsupports.\nGlossary\nCommon\nOps Manager\nterms and concepts with their definitions.\nOps Manager\nConfiguration Settings\nAvailable\nOps Manager\nconfiguration file parameters and values.\nAdvanced Options for MongoDB Deployments\nExplanation of the advanced deployment options for replica sets and\nsharded clusters.\nAutomation Configuration\nAvailable settings in the Automation configuration file used to\ndetermine the desired state of the MongoDB deployment.\nMongoDB Settings and Automation Support\nSupported and unsupported options for a MongoDB process as specified\nin the automation configuration file.\nDatabase Commands Used by Monitoring\nA reference sheet for the monitoring service.\nAlert Event Types\nAn inventory of all alert events generated in\nOps Manager\n.\nAudit Events\nAn inventory of all audit events reported in the activity feed.\nKubernetes Operator Object Specification\nDescribes the options for the\nKubernetes\npod specifications to create\nMongoDB databases.\nKubernetes Operator Known Issues\nKnown issues with the MongoDB Enterprise Kubernetes Operator.\nBack\nSecurity\nNext\nMongoDB Compatibility",
    "url": "https://www.mongodb.com/docs/ops-manager/current/reference/",
    "source": "mongodb",
    "doc_type": "general",
    "scraped_at": 31807.9894857
  },
  {
    "title": "$limit (aggregation)",
    "content": "Docs Home\n/\nDatabase Manual\n/\nAggregation Operations\n/\nReference\n/\nStages\n$limit (aggregation)\nCopy page\nDefinition\n$limit\nLimits the number of documents passed to the next stage in the\npipeline\n.\nCompatibility\nYou can use\n$limit\nfor deployments hosted in the following\nenvironments:\nMongoDB Atlas\n: The fully\nmanaged service for MongoDB deployments in the cloud\nMongoDB Enterprise\n: The\nsubscription-based, self-managed version of MongoDB\nMongoDB Community\n: The\nsource-available, free-to-use, and self-managed version of MongoDB\nSyntax\nThe\n$limit\nstage has the following prototype form:\n{\n$limit\n:\n<\npositive\n64-bit\ninteger\n>\n}\n$limit\ntakes a positive integer that specifies the\nmaximum number of documents to pass along.\nNote\nStarting in MongoDB 5.0, the\n$limit\npipeline aggregation\nhas a 64-bit integer limit. Values passed to the pipeline which\nexceed this limit will return a invalid argument error.\nBehavior\nUsing $limit with Sorted Results\nIf using the\n$limit\nstage with any of:\nthe\n$sort\naggregation stage,\nthe\nsort()\nmethod, or\nthe\nsort\nfield to the\nfindAndModify\ncommand or the\nfindAndModify()\nshell method,\nbe sure to include at least one field in your sort that contains\nunique values, before passing results to the\n$limit\nstage.\nSorting on fields that contain duplicate values may return an\ninconsistent sort order for those duplicate fields over multiple\nexecutions, especially when the collection is actively receiving writes.\nThe easiest way to guarantee sort consistency is to include the\n_id\nfield in your sort query.\nSee the following for more information on each:\nConsistent sorting with $sort (aggregation)\nConsistent sorting with the sort() shell method\nConsistent sorting with the findAndModify command\nConsistent sorting with the findAndModify() shell method\nExamples\nMongoDB Shell\nC#\nNode.js\nConsider the following example:\ndb.\narticle\n.\naggregate\n(\n[\n{\n$limit\n:\n5\n}\n])\n;\nThis operation returns only the first 5 documents passed to it\nby the pipeline.\n$limit\nhas no effect on the content\nof the documents it passes.\nTo use the MongoDB .NET/C# driver to add a\n$limit\nstage to an aggregation\npipeline, call the\nLimit()\nmethod on a\nPipelineDefinition\nobject.\nThe following example creates a pipeline stage that\nlimits the number of returned documents to\n10\n:\nvar\npipeline =\nnew\nEmptyPipelineDefinition<BsonDocument>()\n.Limit(\n10\n);\nTo use the MongoDB Node.js driver to add a\n$limit\nstage to an aggregation\npipeline, use the\n$limit\noperator in a pipeline object.\nThe following example creates a pipeline stage that\nlimits the number of returned documents to\n10\n. The\nexample then runs the aggregation pipeline:\nconst\npipeline\n=\n[\n{\n$limit\n:\n10\n}]\n;\nconst\ncursor\n=\ncollection.\naggregate\n(\npipeline)\n;\nreturn\ncursor\n;\nNote\nWhen a\n$sort\nprecedes a\n$limit\nand there are no\nintervening stages that modify the number of documents, the optimizer can\ncoalesce the\n$limit\ninto the\n$sort\n. This allows\nthe\n$sort\noperation to only\nmaintain the top\nn\nresults as it progresses, where\nn\nis the\nspecified limit, and ensures that MongoDB only needs to store\nn\nitems in memory.\nThis optimization still applies when\nallowDiskUse\nis\ntrue\nand\nthe\nn\nitems exceed the\naggregation memory limit\n.\nLearn More\nTo learn how to use\n$limit\nin a full example, see the\nFilter Data\ntutorial.\nBack\n$indexStats\nNext\n$listClusterCatalog",
    "url": "https://www.mongodb.com/docs/manual/reference/operator/aggregation/limit/",
    "source": "mongodb",
    "doc_type": "manual",
    "scraped_at": 31808.2560786
  },
  {
    "title": "Migrate to the Atlas CLI",
    "content": "Docs Home\n/\nMongoDB Atlas\n/\nAtlas CLI\n/\nConnect\nMigrate to the Atlas CLI\nCopy page\nNote\nmongocli atlas\ncommands for MongoCLI are now deprecated. Follow\nthe steps on this page to migrate to the Atlas CLI and run\nAtlas CLI commands\ninstead of\nmongocli atlas\ncommands.\nIf you have already configured MongoDB CLI, you can migrate your settings to the\nAtlas CLI easily. Migrating to the Atlas CLI preserves existing MongoDB CLI\nprofiles and saved\nAPI\nkeys. If you haven't already configured\nMongoDB CLI, you can proceed to\nInstall or Update the Atlas CLI\n.\nComplete the\nfollowing steps to migrate from the MongoDB CLI to the Atlas CLI:\n1\nInstall the Atlas CLI.\nInstall or Update the Atlas CLI\n. If you have an existing\nmongocli.toml\nconfiguration file from\nthe MongoDB CLI,\nAtlas CLI automatically\ncreates a new configuration file\n,\nconfig.toml\n. The Atlas CLI populates\nconfig.toml\nwith\nthe profile content and\nAPI\nkeys from\nmongocli.toml\n.\n2\nUpdate any automation scripts.\nIf you use any automation scripts that rely on\nmongocli.toml\nor\nuse MongoDB CLI\nAtlas\ncommands, update the scripts to point to the\nnew configuration file\nand use\nAtlas CLI commands\ninstead.\nNote\nAtlas CLI supports both MongoDB CLI environment variables and\nAtlas CLI environment variables\n,\nso changing existing environment variables is optional. However,\nyou can use either MongoDB CLI\nenvironment variables or\nAtlas CLI environment variables\n,\nbut not both together. If you plan to use\nAtlas CLI environment variables\nin the future,\nchange your existing MongoDB CLI environment variables.\n3\nChoose an authentication method and connect to\nAtlas\n.\nYou can authenticate using either\natlas auth login\nor\natlas\nconfig init\n. To learn more, see\nConnect from the Atlas CLI\n.\n4\nUse the new\nAtlas CLI commands\n.\nUse\nAtlas CLI commands\n(starting\nwith\natlas\n) wherever you previously used MongoDB CLI\nAtlas\ncommands (starting with\nmongocli atlas\n). You can use profiles\nthat Atlas CLI migrates\nfrom the MongoCLI by\nusing the --profile flag\n.\nBack\nSave Connection Settings\nNext\nCommands",
    "url": "https://www.mongodb.com/docs/atlas/cli/current/migrate-to-atlas-cli/#std-label-migrate-to-atlas-cli",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31808.8398811
  },
  {
    "title": "Aggregation Pipeline",
    "content": "Docs Home\n/\nDatabase Manual\n/\nAggregation Operations\nAggregation Pipeline\nCopy page\nAn aggregation pipeline consists of one or more\nstages\nthat process documents:\nEach stage performs an operation on the input documents.\nFor example, a stage can filter documents, group documents, and\ncalculate values.\nThe documents that are output from a stage are passed to the next\nstage.\nAn aggregation pipeline can return results for groups of documents.\nFor example, return the total, average, maximum, and minimum values.\nYou can update documents with an aggregation pipeline if you use the stages\nshown in\nUpdates with Aggregation Pipeline\n.\nNote\nAggregation pipelines run with the\ndb.collection.aggregate()\nmethod do not modify documents in\na collection, unless the pipeline contains a\n$merge\nor\n$out\nstage.\nYou can\nrun aggregation pipelines in the UI\nfor deployments hosted in\nMongoDB Atlas\n.\nWhen you run aggregation pipelines on MongoDB Atlas deployments in the\nMongoDB Atlas UI, you can preview the results at each stage.\nComplete Aggregation Pipeline Examples\nThe\nComplete Aggregation Pipeline Tutorials\nsection contains complete\ntutorials that provide detailed explanations of common aggregation tasks\nin a step-by-step format. The tutorials include examples for MongoDB\nShell and each of the\nofficial MongoDB drivers\n.\nAdditional Aggregation Pipeline Stage Details\nAn aggregation pipeline consists of one or more\nstages\nthat process documents:\nA stage does not have to output one document for every input\ndocument. For example, some stages may produce new documents or\nfilter out documents.\nThe same stage can appear multiple times in the pipeline with these\nstage exceptions:\n$out\n,\n$merge\n, and\n$geoNear\n.\nTo calculate averages and perform other calculations in a stage, use\naggregation expressions\nthat specify\naggregation operators\n. You\nwill learn more about aggregation expressions in the next section.\nFor all aggregation stages, see\nAggregation Stages\n.\nAggregation Expressions and Operators\nSome aggregation pipeline stages accept\nexpressions\n. Operators calculate values based on input expressions.\nIn the MongoDB Query Language, you can build expressions from the\nfollowing components:\nComponent\nExample\nConstants\n3\nOperators\n$add\nField path expressions\n\"$<path.to.field>\"\nFor example,\n{ $add: [ 3, \"$inventory.total\" ] }\nis an expression\nconsisting of the\n$add\noperator and two input expressions:\nThe constant\n3\nThe\nfield path expression\n\"$inventory.total\"\nThe expression returns the result of adding 3 to the value at path\ninventory.total\nof the input document.\nField Paths\nField path\nexpressions are used to access fields in\ninput documents. To specify a field path, prefix the field name or the\ndotted field path\n(if the field is in an\nembedded document) with a dollar sign\n$\n. For example,\n\"$user\"\nto\nspecify the field path for the\nuser\nfield or\n\"$user.name\"\nto\nspecify the field path to the embedded\n\"user.name\"\nfield.\n\"$<field>\"\nis equivalent to\n\"$$CURRENT.<field>\"\nwhere the\nCURRENT\nis a system variable that defaults to the root of\nthe current object, unless stated otherwise in specific stages.\nFor more information and examples, see\nField Paths\n.\nRun an Aggregation Pipeline\nTo run an aggregation pipeline, use:\ndb.collection.aggregate()\nor\naggregate\nUpdate Documents Using an Aggregation Pipeline\nTo update documents with an aggregation pipeline, use:\nCommand\nmongosh\nMethods\nfindAndModify\ndb.collection.findOneAndUpdate()\ndb.collection.findAndModify()\nupdate\ndb.collection.updateOne()\ndb.collection.updateMany()\nBulk.find.update()\nBulk.find.updateOne()\nBulk.find.upsert()\nOther Considerations\nAggregation Pipeline Limitations\nAn aggregation pipeline has limitations on the value types and the\nresult size. See\nAggregation Pipeline Limits\n.\nAggregation Pipelines and Sharded Collections\nAn aggregation pipeline supports operations on sharded collections.\nSee\nAggregation Pipeline and Sharded Collections\n.\nAggregation Pipelines as an Alternative to Map-Reduce\nStarting in MongoDB 5.0,\nmap-reduce\nis\ndeprecated:\nInstead of\nmap-reduce\n, you should use an\naggregation pipeline\n. Aggregation\npipelines provide better performance and usability than map-reduce.\nYou can rewrite map-reduce operations using\naggregation pipeline\nstages\n, such as\n$group\n,\n$merge\n, and others.\nFor map-reduce operations that require custom functionality, you can\nuse the\n$accumulator\nand\n$function\naggregation\noperators. You can use those\noperators to define custom aggregation expressions in JavaScript.\nFor examples of aggregation pipeline alternatives to map-reduce, see:\nMap-Reduce to Aggregation Pipeline\nMap-Reduce Examples\nLearn More\nTo learn more about aggregation pipelines, see:\nExpression Operators\nAggregation Stages\nBack\nAggregation Operations\nNext\nField Paths",
    "url": "https://www.mongodb.com/docs/manual/core/aggregation-pipeline/",
    "source": "mongodb",
    "doc_type": "manual",
    "scraped_at": 31809.111735
  },
  {
    "title": "Cluster Types",
    "content": "Docs Home\n/\nAtlas\n/\nCreate & Connect to Clusters\nCluster Types\nCopy page\nMongoDB Atlas\ncan deploy multiple tiers of cloud databases:\nDedicated clusters for high-throughput\nproduction applications.\nFlex clusters for development purposes and small-scale applications.\nYou create new cloud databases through the Atlas UI, Atlas Administration API,\nor Atlas CLI.\nYou can also create a\nlocal\nAtlas\ndeployment with the\nAtlas CLI.\nChoose a Deployment Type\nDedicated Clusters\nCreate a Dedicated cluster\nif you want to:\nChoose a specific database configuration based on your workload requirements.\nDefine database\nscaling behavior\n.\nRun high-throughput production workloads.\nAlways have capacity available.\nYou can:\nSet the\ncluster tier\n.\nUse advanced capabilities such as\nsharding\n.\nDistribute your data to\nmultiple regions and cloud providers\n.\nScale your cluster\non-demand.\nEnable\nautoscaling\n.\nMongoDB bills clusters based on the deployment configuration and\ncluster tier\n.\nFlex Clusters\nFlex clusters are low-cost cluster types suitable for teams that are\nlearning MongoDB or developing small proof-of-concept applications.\nYou can begin your project with an Atlas Flex cluster and upgrade to\na production-ready Dedicated cluster tier at a future time.\nImportant\nAs of February 2025, you can create Flex clusters, and can no longer\ncreate\nM2\nand\nM5\nclusters or Serverless instances in the\nAtlas UI, Atlas CLI, Atlas Administration API,\nAtlas Kubernetes Operator\n, HashiCorp Terraform,\nor\nAtlas\nCloudFormation Resources.\nYou can still use existing Serverless instances.\nAtlas\nno longer supports\nM2\nand\nM5\nclusters.\nAtlas\ndeprecated Serverless instances. As of May 25, 2025,\nAtlas\nhas automatically migrated all existing\nM2\nand\nM5\nclusters to Flex clusters.\nFor Serverless instances, beginning May 5 2025,\nAtlas\nwill\ndetermine whether to migrate instances to Free clusters,\nFlex clusters, or Dedicated clusters according to your usage.\nTo see which tiers\nAtlas\nwill migrate your instances\nto, consult the\nAll Clusters\npage in the Atlas UI.\nCreate a Flex cluster\nif you want\nto:\nGet started quickly with minimal database configuration and low costs.\nHave your database scale automatically and dynamically to meet your workload.\nRun infrequent or sparse workloads.\nDevelop or test in a cloud environment.\nGlobal Clusters\nCreate a global cluster\nif you want to support location-aware read and write operations.\nLocation-aware read and write operations are ideal for globally-distributed\napplication instances and clients.\nGlobal Clusters\nare a highly-curated implementation\nof a sharded cluster that offer:\nLow-latency read and write operations for globally distributed\nclients.\nUptime protection during partial or full regional outages.\nLocation-aware data storage in specific geographic regions.\nWorkload isolation based on cluster member types.\nYou can enable\nGlobal Writes\nin\nAtlas\nwhen deploying an\nM30\nor\ngreater sharded cluster. For replica sets,\nscale\nthe cluster to at least an\nM30\ntier and enable\nGlobal Writes\n. All shard nodes deploy with the\nselected cluster.\nImportant\nYou can't disable\nGlobal Writes\nfor a cluster once it is deployed.\nWhen\nGlobal Writes\nare enabled, Global Clusters do not support dedicated search nodes.\nLocal Deployments\nCreate a local deployment\nto try\nAtlas\nfeatures on a single-node replica set hosted on your local\ncomputer.\nFeature Support and Comparison\nThe following table indicates whether Dedicated clusters or\nFlex clusters support the listed configuration or capability in\nMongoDB Atlas\n.\nNote\nMongoDB plans to add support for more configurations and\ncapabilities on Atlas Flex over time. To see\nthe current Atlas Flex limitations and learn\nabout planned support, see\nAtlas Flex Limitations\n.\nFor the latest product updates, see the\nAtlas Changelog\n.\nConfigurations\nConfiguration\nDedicated Clusters\nFlex Clusters\nRapid releases\nAWS\nregions\nSelect regions only\nGoogle Cloud\nregions\nSelect regions only\nMicrosoft Azure\nregions\nSelect regions only\nMulti-region deployments\nMulti-cloud deployments\nSharded\ndeployments\nGlobal clusters\nIP access list\nNetwork peering\nPrivate endpoints\nAdvanced enterprise security features (including\nLDAP\nand\ndatabase auditing\n)\nCapabilities\nCapability\nDedicated Clusters\nFlex Clusters\nUse the\nAtlas\nAPI\nMonitor\nmetrics\nConfigure alerts\non available metrics or billing\nConfigurable\nbackups\nSnapshots\nPerform\npoint-in-time or automated restores\nfrom backup snapshots\nUse the\nAtlas UI\n(Find, Indexes, Schema\nAdvisor and Aggregation Pipeline Builder)\nGet on-demand\nindex\nand\nschema\nsuggestions\nLoad sample data\nUse\nAtlas Triggers\nUse\nAtlas Search\nUse\nOnline Archive\nRun\nfederated queries\nUse\nBI Connector\nUse\nMongoDB Charts\nTake the Next Steps\nOnce you select a cluster type, you can\nCreate a cluster\n.\nBack\nCreate & Connect to Clusters\nNext\nCreate a Cluster",
    "url": "https://www.mongodb.com/docs/atlas/create-database-deployment/#std-label-atlas-choose-flex",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31810.028555
  },
  {
    "title": "Atlas Search Overview",
    "content": "Docs Home\n/\nAtlas\nAtlas Search Overview\nCopy page\nWhat is\nAtlas Search\n?\nAtlas Search\nis an embedded full-text search that gives you a seamless,\nscalable experience for building relevance-based app features\nand eliminates the need to run a separate search system\nalongside your database.\nYou can use\nAtlas Search\nfor fine-grained text indexing and querying of data on your\ncluster.\nAtlas Search\nprovides several kinds of\ntext analyzers\n,\na rich\nquery language\nto create complex search logic,\ncustomizable score-based results ranking, and advanced search features for your\napplications like autocomplete, pagination, and faceting.\nGet Started with\nAtlas Search\nUse Cases\nAtlas Search\nsupports diverse use cases including the following:\nSearch-as-you-type:\nTo predict words with increasing accuracy as users\nenter characters in your application's search field, you can use the\nAtlas Search\nautocomplete\noperator to predict and return results for\npartial words. To learn more, see\nHow to Run Autocomplete and Partial Match\nAtlas Search\nQueries\n.\nFaceted Search:\nTo enable users of your application to narrow down\nsearch results through the use of filters, you can use\nthe\nAtlas Search\nfacet\nCollector\ncollector to build facets that group\nresults by values or ranges in the faceted fields. To learn more, see\nHow to Use Facets with\nAtlas Search\n.\nPaginated Results:\nTo group pages of results and implement functions like\n\"Next Page\" and \"Previous Page\", you can use the\nAtlas Search\nsearchSequenceToken\nwith\nsearchAfter\nand\nsearchBefore\noptions to traverse pages in-order and jump across pages. To learn\nmore, see\nHow to Paginate the Results\n.\nKey Concepts\nThe following concepts form the basis of\nAtlas Search\nand are essential to\noptimize your application.\nWhat are search queries?\nSearch queries consult a search index to return a set of results. Search\nqueries are different than traditional database queries, as they are\nintended to meet more general information needs. Where a database query\nmust follow a strict syntax, search queries can be used for simple text\nmatching, but can also look for similar phrases, number or date ranges,\nor use regular expressions or wildcards.\nAtlas Search\nqueries take the form of an\naggregation pipeline stage\n.\nAtlas Search\nprovides\n$search\nand\n$searchMeta\nstages, which can be used in conjunction with\nother\naggregation pipeline stages\nin your query\npipeline.\nAtlas Search\nprovides query\noperators\nand\ncollectors\nthat you can use inside these\naggregation pipeline stages.\nTo learn more, see\nQueries and Indexes\n.\nWhat is a search index?\nIn the context of search, an\nindex\nis a data structure that\ncategorizes data in an easily searchable format. Search indexes enable\nfaster retrieval of documents that contain a given term without having\nto scan the entire collection. While both\nAtlas Search\nindexes and\nMongoDB Indexes\nmake data retrieval faster, note\nthat they are not the same. Like the index in the back of a book, a\nsearch index is a mapping between terms and the documents that contain\nthose terms. Search indexes also contain other relevant metadata, such\nas the positions of terms in documents.\nYou can create an\nAtlas Search\nindex on a single field or on multiple fields by\nusing\nstatic mappings\n. Alternatively,\nyou can enable\ndynamic mappings\nto\nautomatically index all the dynamically indexable fields in your\ndocuments. You can create\nAtlas Search\nindexes on polymorphic data and embedded\ndocuments, or for specific use-cases like search-as-you-type or faceted\nsearch.\nTo learn more, see\nManage\nAtlas Search\nIndexes\n.\nWhat are search analyzers and tokens?\nWhen creating a search index, data must first be transformed into a\nsequence of\ntokens\nor\nterms\n. An\nanalyzer\nfacilitates this process\nthrough steps including:\nTokenization\n: Breaking up words in a string into indexable tokens.\nFor example, dividing a sentence by whitespace and punctuation.\nNormalization\n: Organizing data so that it is consistently\nrepresented and easier to analyze. For example, transforming text to\nlower case or removing unwanted words called\nstop words\n.\nStemming\n: Reducing words to their root form. For example, ignoring\nsuffixes, prefixes, and plural word forms.\nThe specifics of tokenization are language-specific and can require\nmaking additional choices. Which analyzer to use depends on your data\nand application.\nAtlas Search\nprovides some built-in\nanalyzers\n. You can\nalso create your own\ncustom analyzer\n. You can\nspecify alternate analyzers using\nmulti\nanalyzer.\nTo learn more, see\nProcess Data with Analyzers\n.\nWhat is a search score?\nEach document in the query results receives a relevancy score that\nenables query results to be returned in order from the highest relevance\nto the lowest. In the simplest form of scoring, documents score higher\nif the query term appears frequently in a document and lower if the\nquery term appears across many documents in the collection. Scoring can\nalso be customized. Tailoring search to a specific domain often means\ncustomizing the relevance-based default score by boosting, decaying, or\nmodifying it in other ways.\nTo learn more, see\nScore Documents\n.\nNext Steps\nFor a hands-on experience creating\nAtlas Search\nindexes and running\nAtlas Search\nqueries against sample data, try the\nAtlas Search\nQuick Start\n.",
    "url": "https://mongodb.com/docs/atlas/atlas-search",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31811.2766698
  },
  {
    "title": "Load Data into Atlas",
    "content": "Docs Home\n/\nAtlas\n/\nGet Started\nLoad Data into Atlas\nCopy page\nEstimated completion time: 5 minutes\nAtlas\nprovides sample data you can load into your\nAtlas\nclusters. You can use this data to quickly get started\nexperimenting with data in MongoDB and using tools such as the\nAtlas UI\nand\nMongoDB Charts\n.\nFor descriptions of each of the available datasets, see\nAvailable Sample Datasets\n. Each dataset page contains\ninformation on the databases, collections, and indexes in the dataset.\nYou can also generate synthetic data that aligns to your real data's\nschema. To learn more, see\nGenerate Synthetic Data\n.\nTo import your own data, see\nMigrate or Import Data\n.\nLoad Sample Data\nRequired Access\nTo load sample data, you must have\nProject Owner\naccess to the project.\nUsers with\nOrganization Owner\naccess must add themselves to the\nproject as a\nProject Owner\n.\nPrerequisites\nTo utilize the sample data provided by\nAtlas\n, you must create\nan\nAtlas\ncluster to load data into. To learn more,\nsee\nCluster Types\n.\nProcedure\nYou can load sample data into your\nAtlas\ncluster from the Atlas CLI or the\nAtlas UI\n.\nSelect the appropriate tab based on how you would like to load\nsample data:\nAtlas CLI\nAtlas UI\nTo load sample data into the specified cluster using the\nAtlas CLI, run the following command:\natlas clusters sampleData load <clusterName> [options]\nTo learn more about the command syntax and parameters, see the\nAtlas CLI documentation for\natlas clusters sampleData load\n.\nAfter you run the command to load sample data, you can use\nthe following Atlas CLI commands to monitor the status of the\nsample data load job:\nTo return the details for the specified sample data load job using the Atlas CLI, run the following command:\natlas clusters sampleData describe <id> [options]\nTo watch the specified sample data job in your cluster until it completes using the Atlas CLI, run the following command:\natlas clusters sampleData watch <id> [options]\nTo learn more about the syntax and parameters for the previous commands, see the Atlas CLI documentation for\natlas clusters sampleData describe\nand\natlas clusters sampleData watch\n.\nTip\nSee: Related Links\nInstall the Atlas CLI\nConnect to the Atlas CLI\nClusters View\nCollections View\nTo load sample data into your cluster from\nthe\nClusters\nview:\n1\nIn\nAtlas\n, go to the\nClusters\npage for your project.\nWARNING:\nNavigation Improvements In Progress\nWe're currently rolling out a new and improved navigation\nexperience. If the following steps don't match your view in the\nAtlas UI, see\nthe preview documentation\n.\nIf it's not already displayed, select the organization that\ncontains your desired project from the\nOrganizations\nmenu\nin the\nnavigation bar.\nIf it's not already displayed, select your desired project\nfrom the\nProjects\nmenu in the navigation bar.\nIf it's not already displayed, click\nClusters\nin the\nsidebar.\nThe\nClusters\npage displays.\n2\nOpen the\nLoad Sample Dataset\ndialog box.\nLocate the cluster where you want to load\nsample data.\nClick the\nEllipses (...)\nbutton for your\ncluster.\nClick\nLoad Sample Dataset\n.\nThe\nLoad sample data\ndialog box opens.\nIn the dialog box, choose which datasets to load from the drop-down menu.\nFor details on the collections and documents included in these\ndatasets, see\nAvailable Sample Datasets\n.\nTo load all available sample datasets, click\nSelect All\n.\n3\nClick\nLoad sample data\nto confirm.\nThe dialog box closes and\nAtlas\nbegins loading your sample\ndataset into your cluster.\n4\nGo to the\nCollections\npage.\nClick the\nBrowse Collections\nbutton for your cluster.\nThe\nData Explorer\ndisplays.\n5\nView your sample data.\nYou should see the databases that you loaded in your\ncluster in the\nCollections\nview.\n6\n(Optional) Run a query on the sample data.\nFor example, if you loaded the\nsample_restaurants\ndataset:\nIn the left navigation of the\nCollections\npage, select\nthe\nsample_restaurants\ndatabase and then the\nrestaurants\ncollection.\nTo find all restaurants located in Queens, copy the following\nquery filter document\ninto the\nFilter\nsearch bar.\n{\nborough\n:\n\"Queens\"\n}\nClick\nApply\n.\nAtlas\nshows documents where the\nborough\nfield corresponds to\nQueens\n.\nTo learn more, see\nQuery Documents\n.\nImportant\nYou can load sample data through the\nCollection View\nonly if you have\nData Explorer\nenabled\nand you don't have\nany data already in your collection.\nTo load sample data into your cluster from the\nCollections View\n:\n1\nIn\nAtlas\n, go to the\nClusters\npage for your project.\nWARNING:\nNavigation Improvements In Progress\nWe're currently rolling out a new and improved navigation\nexperience. If the following steps don't match your view in the\nAtlas UI, see\nthe preview documentation\n.\nIf it's not already displayed, select the organization that\ncontains your desired project from the\nOrganizations\nmenu\nin the\nnavigation bar.\nIf it's not already displayed, select your desired project\nfrom the\nProjects\nmenu in the navigation bar.\nIf it's not already displayed, click\nClusters\nin the\nsidebar.\nThe\nClusters\npage displays.\n2\nGo to the\nCollections\npage.\nClick the\nBrowse Collections\nbutton for your cluster.\nThe\nData Explorer\ndisplays.\n3\nClick\nLoad a Sample Dataset\n.\nThe\nLoad sample data\ndialog box opens.\n4\nIn the dialog box, select which datasets to load from the drop-down menu.\nFor details on the collections and documents included in these\ndatasets, see\nAvailable Sample Datasets\n.\nTo load all available sample datasets, click\nSelect All\n.\n5\nClick\nLoad sample data\nto confirm.\nOnce the load completes, the view refreshes to show your sample\ndata.\nYou should see the databases that you loaded in your\ncluster.\n6\n(Optional) Run a query on the sample data.\nFor example, if you loaded the\nsample_restaurants\ndataset:\nIn the left navigation of the\nCollections\npage, select\nthe\nsample_restaurants\ndatabase and then the\nrestaurants\ncollection.\nTo find all restaurants located in Queens, copy the following\nquery filter document\ninto the\nFilter\nsearch bar.\n{\nborough\n:\n\"Queens\"\n}\nClick\nApply\n.\nAtlas\nshows documents where the\nborough\nfield is equal to\nQueens\n.\nTo learn more, see\nQuery Documents\n.\nAvailable Sample Datasets\nThe following table shows the sample datasets available for\nAtlas\nclusters. Click a sample dataset to learn more about it.\nFor instructions on loading this sample data into your\nAtlas\ncluster, see\nLoad Sample Data\n.\nDataset Name\nDescription\nSample AirBnB Listings Dataset\nContains details on\nAirBnB\nlistings.\nSample Analytics Dataset\nContains training data for a mock financial services\napplication.\nSample Geospatial Dataset\nContains shipwreck data.\nSample Guides Dataset\nContains planet data.\nSample Mflix Dataset\nContains movie data. Includes\nvector embeddings\n.\nSample Restaurants Dataset\nContains restaurant data.\nSample Supply Store Dataset\nContains data from a mock office supply store.\nSample Training Dataset\nContains MongoDB training services dataset.\nSample Weather Dataset\nContains detailed weather reports.\nSample Data Namespaces\nWhen you load the sample data,\nAtlas\ncreates the following\nnamespaces on your cluster:\nWarning\nIf any of these namespaces already exist on your\ncluster when you attempt to load the sample data,\nthe operation will fail and no sample data will be loaded into your\ncluster.\nDatabase\nCollection\nsample_airbnb\nlistingsAndReviews\nsample_analytics\naccounts\nsample_analytics\ncustomers\nsample_analytics\ntransactions\nsample_geospatial\nshipwrecks\nsample_guides\nplanets\nsample_mflix\ncomments\nsample_mflix\nembedded_movies\nsample_mflix\nmovies\nsample_mflix\ntheaters\nsample_mflix\nusers\nsample_supplies\nsales\nsample_training\ncompanies\nsample_training\ngrades\nsample_training\ninspections\nsample_training\nposts\nsample_training\nroutes\nsample_training\ntrips\nsample_training\nzips\nsample_weatherdata\ndata\nTutorials Using Sample Data\nAtlas\nTutorials\nThe\nGet Started with Atlas\ntutorial walks through setting up\nan\nAtlas\ncluster and populating that cluster with sample\ndata.\nMongoDB Charts Tutorials\nThe following\nMongoDB Charts\ntutorials guide you through\nvisualizing sample data provided by\nAtlas\n:\nVisualizing Order Data\nVisualize the\nSample Supply Store Dataset\n, which contains sales order data\nfrom a mock office supply company.\nVisualizing Movie Details\nVisualize the\nSample Mflix Dataset\n, which contains data on movies and\nmovie theaters.\nTip\nTo visualize data in\nMongoDB Charts\nfrom the Atlas UI, click\nVisualize Your Data\nwhen viewing a specific database\nor collection.\nCharts\nloads the data source and you can\nstart building a chart in the\nCharts\nview. For detailed\nsteps, see\nBuild Charts\n.\nMongoDB Courses that Use Sample Data\nInstructor-led Training\nGet quickly ramped on MongoDB with comprehensive private training\nprograms for developers and operations teams.\nBack\nInsert and View a Document\nNext\nAirBnB Listings",
    "url": "https://www.mongodb.com/docs/atlas/sample-data/#std-label-sample-data",
    "source": "mongodb",
    "doc_type": "atlas",
    "scraped_at": 31811.6019766
  },
  {
    "title": "Use Promises with Asynchronous JavaScript",
    "content": "Docs Home\n/\nLanguages\n/\nJavascript\n/\nNode.js Driver\nUse Promises with Asynchronous JavaScript\nCopy page\nOverview\nThe Node.js driver uses the asynchronous JavaScript API to communicate with\nyour MongoDB cluster.\nAsynchronous JavaScript allows you to execute operations without waiting for\nthe processing thread to become free. This helps prevent your application\nfrom becoming unresponsive when\nexecuting long-running operations. For more information about asynchronous\nJavaScript, see the MDN web documentation on\nAsynchronous JavaScript\n.\nThis section describes\nPromises\nthat you can use with the Node.js driver to\naccess the results of your method calls to your MongoDB cluster.\nPromises\nA Promise is an object returned by the asynchronous method call that allows\nyou to access information on the eventual success or failure of the operation\nthat they wrap. The Promise is in the\nPending\nstate if the operation is\nstill running,\nFulfilled\nif the operation completed successfully, and\nRejected\nif the operation threw an exception. For more information on\nPromises and related terminology, see the MDN documentation on\nPromises\n.\nMost driver methods that communicate with your MongoDB cluster, such as\nfindOneAndUpdate()\nand\ncountDocuments()\n, return Promise\nobjects and already contain logic to handle the success or failure of the\noperation.\nYou can define your own logic that executes once the Promise reaches the\nFulfilled\nor\nRejected\nstate by appending the\nthen()\nmethod.\nThe first parameter of\nthen()\nis the method that gets called when the\nPromise reaches the\nFulfilled\nstate and the optional second parameter is\nthe method that gets called when it reaches the\nRejected\nstate. The\nthen()\nmethod returns a Promise to which you can append more\nthen()\nmethods.\nWhen you append one or more\nthen()\nmethods to a Promise, each call passes\nits execution result to the next one. This pattern is called\nPromise chaining\n. The following code snippet shows an example of Promise\nchaining by appending a single\nthen()\nmethod.\ncollection\n.\nupdateOne\n(\n{\nname\n:\n\"Mount McKinley\"\n}\n,\n{\n$set\n:\n{\nmeters\n:\n6190\n} })\n.\nthen\n(\nres\n=>\nconsole\n.\nlog\n(\n`Updated\n${res.result.n}\ndocuments`\n)\n,\nerr\n=>\nconsole\n.\nerror\n(\n`Something went wrong:\n${err}\n`\n)\n,\n)\n;\nTo handle only Promise transitions to the\nRejected\nstate, use the\ncatch()\nmethod\nrather than passing a\nnull\nfirst parameter to\nthen()\n. The\ncatch()\nmethod\naccepts a single callback that is executed when the Promise transitions to the\nRejected\nstate.\nThe\ncatch()\nmethod is often appended at the end of a Promise chain to\nhandle any exceptions thrown. The following code snippet demonstrates appending\na\ncatch()\nmethod to the end of a Promise chain.\ndeleteOne\n(\n{\nname\n:\n\"Mount Doom\"\n})\n.\nthen\n(\nresult\n=>\n{\nif\n(\nresult.\ndeletedCount\n!\n==\n1\n)\n{\nthrow\n\"Could not find Mount Doom!\"\n;\n}\nreturn\nnew\nPromise\n(\n(\nresolve, reject\n) =>\n{\n...\n})\n;\n})\n.\nthen\n(\nresult\n=>\nconsole\n.\nlog\n(\n`Vanquished\n${result.quantity}\nNazgul`\n))\n.\ncatch\n(\nerr\n=>\nconsole\n.\nerror\n(\n`Fatal error occurred:\n${err}\n`\n))\n;\nNote\nCertain methods in the driver such as\nfind()\nreturn a\nCursor\ninstead of a Promise. To determine what type each method returns, see\nthe\nNode.js API documentation\n.\nAwait\nIf you are using\nasync\nfunctions, you can use the\nawait\noperator on\na Promise to pause further execution until the Promise reaches either the\nFulfilled\nor\nRejected\nstate and returns. Since the\nawait\noperator\nwaits for the resolution of the Promise, you can use it in place of\nPromise chaining to sequentially execute your logic. The following code\nsnippet uses\nawait\nto execute the same logic as the first Promise\nchaining example.\nasync\nfunction\nrun\n(\n)\n{\n...\ntry\n{\nres\n=\nawait\nmyColl.\nupdateOne\n(\n{\nname\n:\n\"Mount McKinley\"\n}\n,\n{\n$set\n:\n{\nmeters\n:\n6190\n} }\n,\n)\n;\nconsole\n.\nlog\n(\n`Updated\n${res.result.n}\ndocuments`\n)\n;\n}\ncatch\n(\nerr)\n{\nconsole\n.\nerror\n(\n`Something went wrong:\n${err}\n`\n)\n;\n}\n}\nFor more information, see the MDN documentation on\nawait\n.\nOperational Considerations\nOne common mistake when using\nasync\nmethods is to forget to use\nawait\noperator on Promises to get the value of the result rather than the Promise\nobject. Consider the following example in which we iterate over a cursor\nusing\nhasNext()\n, which returns a Promise that resolves to a boolean that\nindicates whether more results exist, and\nnext()\nwhich returns a\nPromise that resolves to the next entry the cursor is pointing to.\nasync\nfunction\nrun\n(\n)\n{\n...\n// WARNING:\nthis snippet may\ncause an infinite loop\nconst\ncursor\n=\nmyColl.\nfind\n(\n)\n;\nwhile\n(\ncursor.\nhasNext\n(\n))\n{\nconsole\n.\nlog\n(\ncursor.\nnext\n(\n))\n;\n}\n}\nSince the call to\nhasNext()\nreturns a\nPromise\n, the conditional\nstatement returns\ntrue\nregardless of the value that it resolves to.\nIf we alter the code to\nawait\nthe call to\nnext()\nonly, as demonstrated\nin the following code snippet, it throws the following error:\nMongoError: Cursor is closed\n.\nasync\nfunction\nrun\n(\n)\n{\n...\n// WARNING:\nthis snippet throws\na MongoError\nconst\ncursor\n=\nmyColl.\nfind\n(\n)\n;\nwhile\n(\ncursor.\nhasNext\n(\n))\n{\nconsole\n.\nlog\n(\nawait\ncursor.\nnext\n(\n))\n;\n}\n}\nWhile\nhasNext()\nis not called until after the result of\nnext()\nreturns,\nthe call to\nhasNext()\nreturns a Promise which evaluates to\ntrue\nrather\nthan the value it resolves to, similar to the prior example. The code\nattempts to call\nnext()\non a Cursor that has already returned its results\nand closed as a result.\nIf we alter the code to only\nawait\nthe call to\nhasNext()\nas shown in\nthe following example, the console prints Promise objects rather than the\ndocument objects.\nasync\nfunction\nrun\n(\n)\n{\n...\n// WARNING:\nthis snippet prints\nPromises instead of\nthe objects they\nresolve to\nconst\ncursor\n=\nmyColl.\nfind\n(\n)\n;\nwhile\n(\nawait\ncursor.\nhasNext\n(\n))\n{\nconsole\n.\nlog\n(\ncursor.\nnext\n(\n))\n;\n}\n}\nUse\nawait\nbefore both the\nhasNext()\nand\nnext()\nmethod calls to\nensure that you are operating on the correct return values as demonstrated\nin the following code:\nasync\nfunction\nrun\n(\n)\n{\n...\nconst\ncursor\n=\nmyColl.\nfind\n(\n)\n;\nwhile\n(\nawait\ncursor.\nhasNext\n(\n))\n{\nconsole\n.\nlog\n(\nawait\ncursor.\nnext\n(\n))\n;\n}\n}\nBack\nStore Large Files\nNext\nAggregation",
    "url": "https://www.mongodb.com/docs/drivers/node/current/fundamentals/promises/",
    "source": "mongodb",
    "doc_type": "driver",
    "scraped_at": 31811.9425938
  }
]




